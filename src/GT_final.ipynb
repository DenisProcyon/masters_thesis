{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Trends Analysis for Multidimensional Poverty Classification in Mexico\n",
    "\n",
    "This notebook analyzes Google search trends data to define a dimension-specific index that will be used to quantify multidimensional poverty across Mexican states. \n",
    "\n",
    "## Methodology Implemented\n",
    "\n",
    "The analysis leverages Google Trends data to understand public search behavior related to poverty dimensions:\n",
    "\n",
    "1. **Keyword Selection**: Carefully selected Spanish keywords that can serve as indicator of poverty \n",
    "2. **Geographic Scope**: Analysis across all 32 Mexican states using state-specific Google Trends data\n",
    "3. **Aggregation Strategy**: Compute individual keyword averages and then dimension averages\n",
    "\n",
    "##  Technical Approach\n",
    "\n",
    "We retrieved Google Trends timeseries data using SerpApi and then we saved them as separate collections in MongoDB. Since in SerpApi each request includes 5 words, each MongoDB collection contains search volume data for 5 specific keyword combinations by state. Data goes from 2004 to present, but analysis can focus only on target years (2020 and 2022 in our case).\n",
    "\n",
    "From each keyword, we extract the search volume for years of interest and we alculate annual averages. Afterwards, to create a unique and more informative dimension-specific measure, we aggregate individual keywords into our 7 poverty dimensions. \n",
    "\n",
    "In our analysis, we consider the dimensions of poverty identified by CONEVAL (Consejo Nacional de Evaluación de la Política de Desarrollo Social), with some minor modifications:\n",
    "\n",
    "Precisely, we consider:\n",
    "   - **Income**: Employment, wages, economic instability\n",
    "   - **Access to Health Services**: Healthcare availability, medical infrastructure\n",
    "   - **Educational Lag**: School dropout, educational access, academic delays\n",
    "   - **Access to Social Security**: Labor protection, social benefits, pension systems\n",
    "   - **Housing**: Living conditions, basic services, housing quality\n",
    "   - **Access to Food**: Food security, nutrition, food prices\n",
    "   - **Social Cohesion**: Discrimination, social exclusion, community tensions\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "The analysis generates CSV files for each target year containing:\n",
    "- Individual keyword search volumes by state\n",
    "- Aggregated dimension averages by state\n",
    "\n",
    "\n",
    "This measure is used as a component that will be combined with components retrieved from other data sources in order to quantify each dimension of poverty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from mongo_wrapper.mongo_wrapper import MongoWrapper\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define Mexican states using Google Trends API-compatible codes and list the keywords we want to retrieve from MongoDB as they were retrieved from SerpApi. These keyword combinations were optimized for API efficiency, so sometimes they might be mixed; therefore, we subsequently map them to poverty dimensions.\n",
    "\n",
    "Some dimensions have fewer keywords due to difficulties in identifying informative words. Indeed, some dimensions were found to be easy to capture and quantify while others were harder to detect. In this specific case, for example, it was challenging to find words indicating the 'access to food' dimension and the 'housing' dimension.\n",
    "\n",
    "Lastly, the comprehensive keyword list ensures we process all relevant search terms while avoiding duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mexican states using ISO 3166-2 codes for Google Trends API compatibility\n",
    "STATES = [\n",
    "    \"MX-AGU\", \"MX-BCN\", \"MX-BCS\", \"MX-CAM\", \"MX-CHP\", \"MX-CHH\",\n",
    "    \"MX-COA\", \"MX-COL\", \"MX-DUR\", \"MX-GUA\", \"MX-GRO\", \"MX-HID\",\n",
    "    \"MX-JAL\", \"MX-DIF\", \"MX-MIC\", \"MX-MOR\", \"MX-NAY\", \"MX-NLE\",\n",
    "    \"MX-OAX\", \"MX-PUE\", \"MX-QUE\", \"MX-ROO\", \"MX-SLP\", \"MX-SIN\",\n",
    "    \"MX-SON\", \"MX-MEX\", \"MX-TAB\", \"MX-TAM\", \"MX-TLA\", \"MX-VER\",\n",
    "    \"MX-YUC\", \"MX-ZAC\"]\n",
    "\n",
    "# keyword combinations used in Google Trends searches - grouped to maximize API efficiency \n",
    "keywords = [\n",
    "    \"crisis,desempleo,pobreza\",\n",
    "    \"conflictos,discriminación\", \n",
    "    \"violencia,becas,escuela secundaria,enfermedad,centro de salud\",\n",
    "    \"pensiones,seguro social,ayuda alimentaria,banco de alimentos,comedor comunitario\",\n",
    "    \"comida barata,receta pobre,apoyo Infonavit,ayuda renta,renta barata\",\n",
    "    \"servicios en la vivienda,vivienda del gobierno\", \n",
    "    \"agua potable, FOVISSSTE\",\n",
    "    \"tianguis, tiendeo, PromoDescuentos\"]\n",
    "\n",
    "# map individual keywords to CONEVAL's seven poverty dimensions\n",
    "POVERTY_DIMENSIONS = {\n",
    "    'income': ['crisis', 'desempleo', 'pobreza'],\n",
    "    'access_to_health_services': ['enfermedad', 'centro de salud'],\n",
    "    'educational_lag': ['becas', 'escuela secundaria'],\n",
    "    'access_to_social_security': ['pensiones', 'seguro social'],\n",
    "    'access_to_food': ['banco de alimentos', 'tianguis', 'tiendeo', 'PromoDescuentos'],\n",
    "    'housing': ['apoyo Infonavit', 'agua potable', 'FOVISSSTE'],\n",
    "    'social_cohesion': ['violencia', 'conflictos', 'discriminación']}\n",
    "\n",
    "\n",
    "# create a list of all unique keywords for processing\n",
    "ALL_KEYWORDS = []\n",
    "for dimension_words in POVERTY_DIMENSIONS.values():\n",
    "    ALL_KEYWORDS.extend(dimension_words)\n",
    "ALL_KEYWORDS = list(set(ALL_KEYWORDS))  # remove duplicates\n",
    "ALL_KEYWORDS = list(set(ALL_KEYWORDS))  # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the timeseries - which go from 2004 onwards - extract only the year so that we can filter the data by year of interest\n",
    "def extract_year_from_timestamp(timestamp):\n",
    "    return datetime.fromtimestamp(int(timestamp)).year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `extract_individual_word_averages` processes Google Trends timeseries data to calculate annual averages for each keyword. This is just an intermediate step, as our goal is to extract just one component per dimension, so this initial averaging serves only to compute the dimension-specific average later on.\n",
    "\n",
    "Afterwards, the function `calculate_dimension_averages` aggregates the individual keyword averages to create dimension averages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word, compute the average value for each year\n",
    "def extract_individual_word_averages(raw_data, target_years=[2020, 2022]):\n",
    "    if 'interest_over_time' not in raw_data:\n",
    "        return {}\n",
    "    \n",
    "    timeline_data = raw_data['interest_over_time']['timeline_data']\n",
    "    \n",
    "    # define a dictionary: {year: {word: [values]}}\n",
    "    yearly_word_data = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    for entry in timeline_data:\n",
    "        timestamp = entry['timestamp']\n",
    "        year = extract_year_from_timestamp(timestamp)\n",
    "        \n",
    "        # keep only the years we are interested in \n",
    "        if year in target_years:\n",
    "            for value_entry in entry['values']:\n",
    "                word = value_entry['query']\n",
    "                extracted_value = value_entry['extracted_value']\n",
    "                yearly_word_data[year][word].append(extracted_value)\n",
    "    \n",
    "    # compute the average for each word in each year\n",
    "    result = {}\n",
    "    for year in yearly_word_data:\n",
    "        result[year] = {}\n",
    "        for word in yearly_word_data[year]:\n",
    "            values = yearly_word_data[year][word]\n",
    "            if values:\n",
    "                result[year][word] = round(np.mean(values), 2)\n",
    "            else:\n",
    "                result[year][word] = 0\n",
    "    \n",
    "    return result\n",
    "\n",
    "# after computing the averages for each word, we aggregate words that belong to the same dimension and calculate its average \n",
    "def calculate_dimension_averages(word_averages):\n",
    "    dimension_averages = {}\n",
    "    \n",
    "    for year, words_data in word_averages.items():\n",
    "        dimension_averages[year] = {}\n",
    "        \n",
    "        for dimension, dimension_words in POVERTY_DIMENSIONS.items():\n",
    "            available_values = []\n",
    "            for word in dimension_words:\n",
    "                if word in words_data and words_data[word] > 0:\n",
    "                    available_values.append(words_data[word])\n",
    "            if available_values:\n",
    "                dimension_averages[year][dimension] = round(np.mean(available_values), 2)\n",
    "            else:\n",
    "                dimension_averages[year][dimension] = 0\n",
    "    \n",
    "    return dimension_averages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Data Processing and Output Generation\n",
    "\n",
    "The function `process_all_states` implements the complete data processing pipeline for all Mexican states:\n",
    "- Establishes MongoDB connections to retrieve Google Trends data \n",
    "- Aggregates data from different collections that contain overlapping keywords\n",
    "- Handles missing data by skipping unavailable collections,\n",
    "- Calculates final averages for keywords that appear in multiple collections\n",
    "- Returns a dataset with state-year-keyword structure ready for dimension aggregation.\n",
    "\n",
    "The function `create_output_files` then transforms this processed data into structured CSV files:\n",
    "- Generates separate output files for each target year (2020 and 2022 in our case), building columns for individual keyword search volumes and aggregated dimension averages. \n",
    "- Calculates dimension-specific averages for each state, and handles missing data by assigning zero values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process all states to extract individual words data and dimension averages\n",
    "def process_all_states():\n",
    "    mongo_client = MongoWrapper(\n",
    "        db=\"serpapi_5y\",\n",
    "        user=os.getenv(\"MONGO_USERNAME\"),\n",
    "        password=os.getenv(\"MONGO_PASSWORD\"),\n",
    "        ip=os.getenv(\"MONGO_IP\"),\n",
    "        port=os.getenv(\"MONGO_PORT\"))\n",
    "    \n",
    "    all_states_data = {}\n",
    "    \n",
    "    for state in STATES:\n",
    "        state_word_data = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "        for keyword_set in keywords:\n",
    "            collection_name = f'serpapi_timeseries_{state}_{keyword_set}'\n",
    "            try:\n",
    "                gt_data = mongo_client.get_collection_entries(collection=collection_name)\n",
    "                if gt_data:\n",
    "                    raw_data = gt_data[0]\n",
    "                    word_averages = extract_individual_word_averages(raw_data)\n",
    "                    for year in word_averages:\n",
    "                        for word, avg_value in word_averages[year].items():\n",
    "                            if word in ALL_KEYWORDS:\n",
    "                                state_word_data[year][word].append(avg_value)\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if state_word_data:\n",
    "            final_state_data = {}\n",
    "            for year in [2020, 2022]:\n",
    "                final_state_data[year] = {}\n",
    "                for word in ALL_KEYWORDS:\n",
    "                    if word in state_word_data[year] and state_word_data[year][word]:\n",
    "                        final_state_data[year][word] = round(np.mean(state_word_data[year][word]), 2)\n",
    "                    else:\n",
    "                        final_state_data[year][word] = 0\n",
    "            all_states_data[state] = final_state_data\n",
    "    return all_states_data\n",
    "\n",
    "# create output files for each year with individual words and dimension averages\n",
    "def create_output_files(all_states_data):\n",
    "    target_years = [2020, 2022]\n",
    "    \n",
    "    # create columns for indivual averages and dimensions averages \n",
    "    for year in target_years:\n",
    "        rows = []\n",
    "        for state, state_data in all_states_data.items():\n",
    "            if year in state_data:\n",
    "                row = {'state': state}\n",
    "                for word in ALL_KEYWORDS:\n",
    "                    row[word] = state_data[year].get(word, 0)\n",
    "                \n",
    "                dimension_averages = calculate_dimension_averages({year: state_data[year]})\n",
    "                if year in dimension_averages:\n",
    "                    for dimension, avg_value in dimension_averages[year].items():\n",
    "                        row[dimension] = avg_value\n",
    "                rows.append(row)\n",
    "        \n",
    "        if rows:\n",
    "            df = pd.DataFrame(rows)\n",
    "            word_cols = [col for col in df.columns if col in ALL_KEYWORDS]\n",
    "            dimension_cols = list(POVERTY_DIMENSIONS.keys())\n",
    "            ordered_cols = ['state'] + sorted(word_cols) + sorted(dimension_cols)\n",
    "            df = df[ordered_cols]\n",
    "            filename = f'gt_{year}.csv'\n",
    "            df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-09 11:35:55,859 WARNING Logger Mongo was configured with True console stream\n",
      "2025-06-09 11:35:56,180 INFO Connected to serpapi_5y database on 206.81.16.39\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    all_states_data = process_all_states()\n",
    "    if all_states_data:\n",
    "        create_output_files(all_states_data)\n",
    "        return all_states_data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
