{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d6576f",
   "metadata": {},
   "source": [
    "## PCA instead of Linear Regression or PLS Reression to extract weights (2020 only and validate on 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58203815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary libraries\n",
    "import pandas as pd \n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d31b87e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tg_2020 = pd.read_csv('clean_data/tg_2020.csv')\n",
    "tg_2022 = pd.read_csv('clean_data/tg_2022.csv')\n",
    "gt_2020 = pd.read_csv('clean_data/gt_2020.csv')\n",
    "gt_2022 = pd.read_csv('clean_data/gt_2022.csv')\n",
    "yt_2020 = pd.read_csv('clean_data/yt_2020.csv')\n",
    "yt_2022 = pd.read_csv('clean_data/yt_2022.csv')\n",
    "news_2020 = pd.read_csv('clean_data/news_2020.csv')\n",
    "news_2022 = pd.read_csv('clean_data/news_2022.csv')\n",
    "off_2020 = pd.read_csv('clean_data/off_2020.csv')\n",
    "off_2022 = pd.read_csv('clean_data/off_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893a170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there were inconsistencies in the state names, so this mapping standardizes the state names across all datasets\n",
    "state_name_map = {\n",
    "    \"México\": \"Estado de México\",\n",
    "    \"Mexico\": \"Estado de México\",\n",
    "    \"Estados Unidos Mexicanos\": \"Estado de México\",\n",
    "    \"Michoacán de Ocampo\": \"Michoacán\",\n",
    "    \"Veracruz de Ignacio de la Llave\": \"Veracruz\",\n",
    "    \"Coahuila de Zaragoza\": \"Coahuila\",\n",
    "    \"Yucatan\": \"Yucatán\",\n",
    "    \"Queretaro\": \"Querétaro\",\n",
    "    \"San Luis Potosi\": \"San Luis Potosí\",\n",
    "    \"Nuevo Leon\": \"Nuevo León\",\n",
    "    \"Michoacan\": \"Michoacán\",\n",
    "    \"Michoacán de Ocampo\": \"Michoacán\"}\n",
    "\n",
    "for df in [off_2020, off_2022, gt_2020, gt_2022, yt_2020, yt_2022, tg_2020, tg_2022, news_2020, news_2022]:\n",
    "    df['state'] = df['state'].astype(str).str.strip()\n",
    "    df['state'] = df['state'].replace(state_name_map)\n",
    "    df['state'] = df['state'].replace(\"nan\", None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca5164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'state' columns as strings in all dataframes\n",
    "for df in [off_2020, off_2022, gt_2020, gt_2022, yt_2020, yt_2022, tg_2020, tg_2022, news_2020, news_2022]:\n",
    "    df['state'] = df['state'].astype(str)\n",
    "\n",
    "# create dataset for 2020\n",
    "data_2020 = off_2020.copy()\n",
    "data_2020['year'] = 2020\n",
    "\n",
    "# merge Google Trends\n",
    "data_2020 = data_2020.merge(gt_2020, on='state', how='inner')\n",
    "\n",
    "# merge YouTube\n",
    "data_2020 = data_2020.merge(yt_2020, on='state', how='inner')\n",
    "\n",
    "# merge Telegram\n",
    "data_2020 = data_2020.merge(tg_2020, on='state', how='inner')\n",
    "\n",
    "# merge News (=LDA topics)\n",
    "data_2020 = data_2020.merge(news_2020, on='state', how='inner')\n",
    "\n",
    "\n",
    "# create dataset for 2022\n",
    "data_2022 = off_2022.copy()\n",
    "data_2022['year'] = 2022\n",
    "\n",
    "# merge Google Trends\n",
    "data_2022 = data_2022.merge(gt_2022, on='state', how='inner')\n",
    "\n",
    "# merge YouTube\n",
    "data_2022 = data_2022.merge(yt_2022, on='state', how='inner')\n",
    "\n",
    "# merge Telegram\n",
    "data_2022 = data_2022.merge(tg_2022, on='state', how='inner')\n",
    "\n",
    "# merge News (=LDA topics)\n",
    "data_2022 = data_2022.merge(news_2022, on='state', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56f511ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for educational lag: ['educational_lag', 'educational_lag_avg_sentiment', 'educational_lag_pct_yt', 'educational_lag_pct_tg', 'women_rights']\n",
      "Education - Explained variance: 0.326\n",
      "PCA Weights:\n",
      "  educational_lag: 0.6539\n",
      "  educational_lag_avg_sentiment: -0.4569\n",
      "  educational_lag_pct_yt: -0.4116\n",
      "  educational_lag_pct_tg: -0.3028\n",
      "  women_rights: -0.3202\n",
      "Artificial Intercept: 12.3767\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# REGRESSION 1: EDUCATIONAL LAG\n",
    "# =============================================\n",
    "\n",
    "# Features for educational lag\n",
    "educ_features = []\n",
    "\n",
    "# Google Trends\n",
    "if 'educational_lag' in data_2020.columns:\n",
    "    educ_features.append('educational_lag')\n",
    "\n",
    "# YouTube\n",
    "if 'educational_lag_avg_sentiment' in data_2020.columns:\n",
    "    educ_features.append('educational_lag_avg_sentiment')\n",
    "if 'educational_lag_pct_yt' in data_2020.columns:\n",
    "    educ_features.append('educational_lag_pct_yt')\n",
    "\n",
    "# Telegram\n",
    "if 'educational_lag_pct_tg' in data_2020.columns:\n",
    "    educ_features.append('educational_lag_pct_tg')\n",
    "\n",
    "# News topics\n",
    "if 'women_rights' in data_2020.columns:\n",
    "    educ_features.append('women_rights')\n",
    "\n",
    "print(f\"Features for educational lag: {educ_features}\")\n",
    "\n",
    "if len(educ_features) > 0:\n",
    "    # Prepare data\n",
    "    X_education = data_2020[educ_features].fillna(0)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler_education = StandardScaler()\n",
    "    X_education_scaled = scaler_education.fit_transform(X_education)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca_education = PCA(n_components=1)\n",
    "    pca_result_education = pca_education.fit_transform(X_education_scaled)\n",
    "    \n",
    "    pca_scores_education = pca_result_education.flatten()\n",
    "    \n",
    "    # create an artificial intercept to have only positive weights - need to justify the arbitrary value chosen \n",
    "    education_intercept = abs(pca_scores_education.min()) + 10  \n",
    "    pca_scores_positive_education = pca_scores_education + education_intercept\n",
    "    \n",
    "    # use loadings as weights and save scaling parameters\n",
    "    educ_weights = dict(zip(educ_features, pca_education.components_[0]))\n",
    "    educ_scaler_mean = dict(zip(educ_features, scaler_education.mean_))\n",
    "    educ_scaler_scale = dict(zip(educ_features, scaler_education.scale_))\n",
    "    \n",
    "    print(f\"Education - Explained variance: {pca_education.explained_variance_ratio_[0]:.3f}\")\n",
    "    print(\"PCA Weights:\")\n",
    "    for feature, weight in educ_weights.items():\n",
    "        print(f\"  {feature}: {weight:.4f}\")\n",
    "    print(f\"Artificial Intercept: {education_intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ec24290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for health: ['health_gt', 'access_to_health_services_avg_sentiment', 'access_to_health_services_pct_yt', 'access_to_health_services_pct_tg', 'health']\n",
      "Health - Explained variance: 0.391\n",
      "PCA Weights:\n",
      "  health_gt: 0.4521\n",
      "  access_to_health_services_avg_sentiment: -0.5435\n",
      "  access_to_health_services_pct_yt: 0.6253\n",
      "  access_to_health_services_pct_tg: 0.2229\n",
      "  health: 0.2439\n",
      "Artificial Intercept: 12.3376\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# REGRESSION 2: HEALTH\n",
    "# =============================================\n",
    "\n",
    "# Features for health\n",
    "health_features = []\n",
    "\n",
    "# Google Trends\n",
    "if 'health_gt' in data_2020.columns:\n",
    "    health_features.append('health_gt')\n",
    "\n",
    "# YouTube\n",
    "if 'access_to_health_services_avg_sentiment' in data_2020.columns:\n",
    "    health_features.append('access_to_health_services_avg_sentiment')\n",
    "if 'access_to_health_services_pct_yt' in data_2020.columns:\n",
    "    health_features.append('access_to_health_services_pct_yt')\n",
    "\n",
    "# Telegram\n",
    "if 'access_to_health_services_pct_tg' in data_2020.columns:\n",
    "    health_features.append('access_to_health_services_pct_tg')\n",
    "\n",
    "# News topics\n",
    "if 'health' in data_2020.columns:\n",
    "    health_features.append('health')\n",
    "\n",
    "print(f\"Features for health: {health_features}\")\n",
    "\n",
    "if len(health_features) > 0:\n",
    "    # Prepare data\n",
    "    X_health = data_2020[health_features].fillna(0)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler_health = StandardScaler()\n",
    "    X_health_scaled = scaler_health.fit_transform(X_health)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca_health = PCA(n_components=1)\n",
    "    pca_result_health = pca_health.fit_transform(X_health_scaled)\n",
    "    \n",
    "    pca_scores_health = pca_result_health.flatten()\n",
    "    \n",
    "    # create an artificial intercept to have only positive weights - need to justify the arbitrary value chosen \n",
    "    health_intercept = abs(pca_scores_health.min()) + 10  \n",
    "    pca_scores_positive_health = pca_scores_health + health_intercept\n",
    "    \n",
    "    # use loadings as weights and save scaling parameters\n",
    "    health_weights = dict(zip(health_features, pca_health.components_[0]))\n",
    "    health_scaler_mean = dict(zip(health_features, scaler_health.mean_))\n",
    "    health_scaler_scale = dict(zip(health_features, scaler_health.scale_))\n",
    "    \n",
    "    print(f\"Health - Explained variance: {pca_health.explained_variance_ratio_[0]:.3f}\")\n",
    "    print(\"PCA Weights:\")\n",
    "    for feature, weight in health_weights.items():\n",
    "        print(f\"  {feature}: {weight:.4f}\")\n",
    "    print(f\"Artificial Intercept: {health_intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "428ea766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for social security: ['social_gt', 'access_to_social_security_avg_sentiment', 'access_to_social_security_pct_yt', 'access_to_social_security_pct_tg', 'public_services', 'institutions']\n",
      "Social Security - Explained variance: 0.268\n",
      "PCA Weights:\n",
      "  social_gt: 0.0718\n",
      "  access_to_social_security_avg_sentiment: 0.2506\n",
      "  access_to_social_security_pct_yt: 0.6952\n",
      "  access_to_social_security_pct_tg: 0.3628\n",
      "  public_services: 0.2436\n",
      "  institutions: -0.5078\n",
      "Artificial Intercept: 12.5542\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# REGRESSION 3: SOCIAL SECURITY\n",
    "# =============================================\n",
    "\n",
    "# Features for social security\n",
    "social_features = []\n",
    "\n",
    "# Google Trends\n",
    "if 'social_gt' in data_2020.columns:\n",
    "    social_features.append('social_gt')\n",
    "\n",
    "# YouTube\n",
    "if 'access_to_social_security_avg_sentiment' in data_2020.columns:\n",
    "    social_features.append('access_to_social_security_avg_sentiment')\n",
    "if 'access_to_social_security_pct_yt' in data_2020.columns:\n",
    "    social_features.append('access_to_social_security_pct_yt')\n",
    "\n",
    "# Telegram\n",
    "if 'access_to_social_security_pct_tg' in data_2020.columns:\n",
    "    social_features.append('access_to_social_security_pct_tg')\n",
    "\n",
    "# News topics\n",
    "if 'public_services' in data_2020.columns:\n",
    "    social_features.append('public_services')\n",
    "if 'institutions' in data_2020.columns:\n",
    "    social_features.append('institutions')\n",
    "\n",
    "print(f\"Features for social security: {social_features}\")\n",
    "\n",
    "if len(social_features) > 0:\n",
    "    # Prepare data\n",
    "    X_social = data_2020[social_features].fillna(0)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler_social = StandardScaler()\n",
    "    X_social_scaled = scaler_social.fit_transform(X_social)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca_social = PCA(n_components=1)\n",
    "    pca_result_social = pca_social.fit_transform(X_social_scaled)\n",
    "    \n",
    "    pca_scores_social = pca_result_social.flatten()\n",
    "    \n",
    "    # create an artificial intercept to have only positive weights - need to justify the arbitrary value chosen \n",
    "    social_intercept = abs(pca_scores_social.min()) + 10  \n",
    "    pca_scores_positive_social = pca_scores_social + social_intercept\n",
    "    \n",
    "    # use loadings as weights and save scaling parameters\n",
    "    social_weights = dict(zip(social_features, pca_social.components_[0]))\n",
    "    social_scaler_mean = dict(zip(social_features, scaler_social.mean_))\n",
    "    social_scaler_scale = dict(zip(social_features, scaler_social.scale_))\n",
    "    \n",
    "    print(f\"Social Security - Explained variance: {pca_social.explained_variance_ratio_[0]:.3f}\")\n",
    "    print(\"PCA Weights:\")\n",
    "    for feature, weight in social_weights.items():\n",
    "        print(f\"  {feature}: {weight:.4f}\")\n",
    "    print(f\"Artificial Intercept: {social_intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "892fc95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for income: ['income_gt', 'income_avg_sentiment', 'income_pct_yt', 'income_pct_tg', 'economy', 'work']\n",
      "Income - Explained variance: 0.273\n",
      "PCA Weights:\n",
      "  income_gt: -0.3186\n",
      "  income_avg_sentiment: -0.3716\n",
      "  income_pct_yt: 0.5657\n",
      "  income_pct_tg: -0.5089\n",
      "  economy: -0.3996\n",
      "  work: 0.1476\n",
      "Artificial Intercept: 13.1896\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# REGRESSION 4: INCOME\n",
    "# =============================================\n",
    "\n",
    "# Features for income\n",
    "income_features = []\n",
    "\n",
    "# Google Trends\n",
    "if 'income_gt' in data_2020.columns:\n",
    "    income_features.append('income_gt')\n",
    "\n",
    "# YouTube\n",
    "if 'income_avg_sentiment' in data_2020.columns:\n",
    "    income_features.append('income_avg_sentiment')\n",
    "if 'income_pct_yt' in data_2020.columns:\n",
    "    income_features.append('income_pct_yt')\n",
    "\n",
    "# Telegram\n",
    "if 'income_pct_tg' in data_2020.columns:\n",
    "    income_features.append('income_pct_tg')\n",
    "\n",
    "# News topics\n",
    "if 'economy' in data_2020.columns:\n",
    "    income_features.append('economy')\n",
    "if 'work' in data_2020.columns:\n",
    "    income_features.append('work')\n",
    "\n",
    "print(f\"Features for income: {income_features}\")\n",
    "\n",
    "if len(income_features) > 0:\n",
    "    # Prepare data\n",
    "    X_income = data_2020[income_features].fillna(0)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler_income = StandardScaler()\n",
    "    X_income_scaled = scaler_income.fit_transform(X_income)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca_income = PCA(n_components=1)\n",
    "    pca_result_income = pca_income.fit_transform(X_income_scaled)\n",
    "    \n",
    "    pca_scores_income = pca_result_income.flatten()\n",
    "    \n",
    "    # create an artificial intercept to have only positive weights - need to justify the arbitrary value chosen \n",
    "    income_intercept = abs(pca_scores_income.min()) + 10  \n",
    "    pca_scores_positive_income = pca_scores_income + income_intercept\n",
    "    \n",
    "    # use loadings as weights and save scaling parameters\n",
    "    income_weights = dict(zip(income_features, pca_income.components_[0]))\n",
    "    income_scaler_mean = dict(zip(income_features, scaler_income.mean_))\n",
    "    income_scaler_scale = dict(zip(income_features, scaler_income.scale_))\n",
    "    \n",
    "    print(f\"Income - Explained variance: {pca_income.explained_variance_ratio_[0]:.3f}\")\n",
    "    print(\"PCA Weights:\")\n",
    "    for feature, weight in income_weights.items():\n",
    "        print(f\"  {feature}: {weight:.4f}\")\n",
    "    print(f\"Artificial Intercept: {income_intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "926d6539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for food: ['food_gt', 'access_to_food_avg_sentiment', 'access_to_food_pct_yt', 'access_to_food_pct_tg', 'women_rights']\n",
      "Food - Explained variance: 0.291\n",
      "PCA Weights:\n",
      "  food_gt: -0.1261\n",
      "  access_to_food_avg_sentiment: -0.5725\n",
      "  access_to_food_pct_yt: 0.6697\n",
      "  access_to_food_pct_tg: -0.3031\n",
      "  women_rights: -0.3405\n",
      "Artificial Intercept: 12.2616\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# REGRESSION 5: FOOD\n",
    "# =============================================\n",
    "\n",
    "# Features for food\n",
    "food_features = []\n",
    "\n",
    "# Google Trends\n",
    "if 'food_gt' in data_2020.columns:\n",
    "    food_features.append('food_gt')\n",
    "\n",
    "# YouTube\n",
    "if 'access_to_food_avg_sentiment' in data_2020.columns:\n",
    "    food_features.append('access_to_food_avg_sentiment')\n",
    "if 'access_to_food_pct_yt' in data_2020.columns:\n",
    "    food_features.append('access_to_food_pct_yt')\n",
    "\n",
    "# Telegram\n",
    "if 'access_to_food_pct_tg' in data_2020.columns:\n",
    "    food_features.append('access_to_food_pct_tg')\n",
    "\n",
    "# News topics\n",
    "if 'women_rights' in data_2020.columns:\n",
    "    food_features.append('women_rights')\n",
    "\n",
    "print(f\"Features for food: {food_features}\")\n",
    "\n",
    "if len(food_features) > 0:\n",
    "    # Prepare data\n",
    "    X_food = data_2020[food_features].fillna(0)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler_food = StandardScaler()\n",
    "    X_food_scaled = scaler_food.fit_transform(X_food)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca_food = PCA(n_components=1)\n",
    "    pca_result_food = pca_food.fit_transform(X_food_scaled)\n",
    "    \n",
    "    pca_scores_food = pca_result_food.flatten()\n",
    "    \n",
    "    # create an artificial intercept to have only positive weights - need to justify the arbitrary value chosen \n",
    "    food_intercept = abs(pca_scores_food.min()) + 10  \n",
    "    pca_scores_positive_food = pca_scores_food + food_intercept\n",
    "    \n",
    "    # use loadings as weights and save scaling parameters\n",
    "    food_weights = dict(zip(food_features, pca_food.components_[0]))\n",
    "    food_scaler_mean = dict(zip(food_features, scaler_food.mean_))\n",
    "    food_scaler_scale = dict(zip(food_features, scaler_food.scale_))\n",
    "    \n",
    "    print(f\"Food - Explained variance: {pca_food.explained_variance_ratio_[0]:.3f}\")\n",
    "    print(\"PCA Weights:\")\n",
    "    for feature, weight in food_weights.items():\n",
    "        print(f\"  {feature}: {weight:.4f}\")\n",
    "    print(f\"Artificial Intercept: {food_intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52daeaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for housing: ['housing_gt', 'housing_avg_sentiment', 'housing_pct_yt', 'housing_pct_tg', 'public_services']\n",
      "Social Cohesion - Explained variance: 0.369\n",
      "PCA Weights:\n",
      "  housing_gt: 0.5485\n",
      "  housing_avg_sentiment: -0.4288\n",
      "  housing_pct_yt: 0.0696\n",
      "  housing_pct_tg: 0.5107\n",
      "  public_services: 0.4995\n",
      "Artificial Intercept: 12.5463\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# REGRESSION 6: HOUSING\n",
    "# =============================================\n",
    "\n",
    "# Features for housing\n",
    "housing_features = []\n",
    "\n",
    "# Google Trends\n",
    "if 'housing_gt' in data_2020.columns:\n",
    "    housing_features.append('housing_gt')\n",
    "\n",
    "# YouTube\n",
    "if 'housing_avg_sentiment' in data_2020.columns:\n",
    "    housing_features.append('housing_avg_sentiment')\n",
    "if 'housing_pct_yt' in data_2020.columns:\n",
    "    housing_features.append('housing_pct_yt')\n",
    "\n",
    "# Telegram\n",
    "if 'housing_pct_tg' in data_2020.columns:\n",
    "    housing_features.append('housing_pct_tg')\n",
    "\n",
    "# News topics\n",
    "if 'public_services' in data_2020.columns:\n",
    "    housing_features.append('public_services')\n",
    "\n",
    "print(f\"Features for housing: {housing_features}\")\n",
    "\n",
    "if len(housing_features) > 0:\n",
    "    # Prepare data\n",
    "    X_housing = data_2020[housing_features].fillna(0)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler_housing = StandardScaler()\n",
    "    X_housing_scaled = scaler_housing.fit_transform(X_housing)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca_housing = PCA(n_components=1)\n",
    "    pca_result_housing = pca_housing.fit_transform(X_housing_scaled)\n",
    "    \n",
    "    pca_scores_housing = pca_result_housing.flatten()\n",
    "    \n",
    "    # create an artifical intercept to have only positive weights - need to justify the arbitrary value chosen \n",
    "    housing_intercept = abs(pca_scores_housing.min()) + 10  \n",
    "    pca_scores_positive_housing = pca_scores_housing + housing_intercept\n",
    "    \n",
    "    # use loadings as weights and save scaling parameters\n",
    "    housing_weights = dict(zip(housing_features, pca_housing.components_[0]))\n",
    "    housing_scaler_mean = dict(zip(housing_features, scaler_housing.mean_))\n",
    "    housing_scaler_scale = dict(zip(housing_features, scaler_housing.scale_))\n",
    "    \n",
    "    print(f\"Social Cohesion - Explained variance: {pca_housing.explained_variance_ratio_[0]:.3f}\")\n",
    "    print(\"PCA Weights:\")\n",
    "    for feature, weight in housing_weights.items():\n",
    "        print(f\"  {feature}: {weight:.4f}\")\n",
    "    print(f\"Artificial Intercept: {housing_intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc68e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features for social cohesion: ['cohesion_gt', 'social_cohesion_avg_sentiment', 'social_cohesion_pct_yt', 'social_cohesion_pct_tg', 'safety']\n",
      "Social Cohesion - Explained variance: 0.335\n",
      "PCA Weights:\n",
      "  cohesion_gt: 0.6497\n",
      "  social_cohesion_avg_sentiment: -0.5884\n",
      "  social_cohesion_pct_yt: -0.2718\n",
      "  social_cohesion_pct_tg: 0.2196\n",
      "  safety: 0.3311\n",
      "Artificial Intercept: 13.0030\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# PCA FOR SOCIAL COHESION (PCA loadings as weights as we don't have a target variable) - artificial intercept\n",
    "# =============================================\n",
    "\n",
    "# Features for social cohesion\n",
    "cohesion_features = []\n",
    "\n",
    "# Google Trends\n",
    "if 'cohesion_gt' in data_2020.columns:\n",
    "    cohesion_features.append('cohesion_gt')\n",
    "\n",
    "# YouTube\n",
    "if 'social_cohesion_avg_sentiment' in data_2020.columns:\n",
    "    cohesion_features.append('social_cohesion_avg_sentiment')\n",
    "if 'social_cohesion_pct_yt' in data_2020.columns:\n",
    "    cohesion_features.append('social_cohesion_pct_yt')\n",
    "\n",
    "# Telegram\n",
    "if 'social_cohesion_pct_tg' in data_2020.columns:\n",
    "    cohesion_features.append('social_cohesion_pct_tg')\n",
    "\n",
    "# News topics\n",
    "if 'safety' in data_2020.columns:\n",
    "    cohesion_features.append('safety')\n",
    "\n",
    "print(f\"Features for social cohesion: {cohesion_features}\")\n",
    "\n",
    "if len(cohesion_features) > 0:\n",
    "    # Prepare data\n",
    "    X_cohesion = data_2020[cohesion_features].fillna(0)\n",
    "    \n",
    "    # Standardize\n",
    "    scaler_cohesion = StandardScaler()\n",
    "    X_cohesion_scaled = scaler_cohesion.fit_transform(X_cohesion)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca_cohesion = PCA(n_components=1)\n",
    "    pca_result_cohesion = pca_cohesion.fit_transform(X_cohesion_scaled)\n",
    "    \n",
    "    pca_scores_cohesion = pca_result_cohesion.flatten()\n",
    "    \n",
    "    # create an artifical intercept to have only positive weights - need to justify the arbitrary value chosen \n",
    "    cohesion_intercept = abs(pca_scores_cohesion.min()) + 10  \n",
    "    pca_scores_positive_cohesion = pca_scores_cohesion + cohesion_intercept\n",
    "    \n",
    "    # use loadings as weights and save scaling parameters\n",
    "    cohesion_weights = dict(zip(cohesion_features, pca_cohesion.components_[0]))\n",
    "    cohesion_scaler_mean = dict(zip(cohesion_features, scaler_cohesion.mean_))\n",
    "    cohesion_scaler_scale = dict(zip(cohesion_features, scaler_cohesion.scale_))\n",
    "    \n",
    "    print(f\"Social Cohesion - Explained variance: {pca_cohesion.explained_variance_ratio_[0]:.3f}\")\n",
    "    print(\"PCA Weights:\")\n",
    "    for feature, weight in cohesion_weights.items():\n",
    "        print(f\"  {feature}: {weight:.4f}\")\n",
    "    print(f\"Artificial Intercept: {cohesion_intercept:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21045cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights, intercepts, and scaling parameters\n",
    "\n",
    "# Collect all weights with intercepts and scaling parameters\n",
    "all_weights = []\n",
    "all_scaling_params = []\n",
    "\n",
    "# Add regression weights for each dimension\n",
    "dimensions_data = [\n",
    "    ('educational_lag', educ_weights if 'educ_weights' in locals() else {}, education_intercept if 'educ_intercept' in locals() else 0, \n",
    "     educ_scaler_mean if 'educ_scaler_mean' in locals() else {}, educ_scaler_scale if 'educ_scaler_scale' in locals() else {}),\n",
    "    ('health', health_weights if 'health_weights' in locals() else {}, health_intercept if 'health_intercept' in locals() else 0,\n",
    "     health_scaler_mean if 'health_scaler_mean' in locals() else {}, health_scaler_scale if 'health_scaler_scale' in locals() else {}),\n",
    "    ('social_security', social_weights if 'social_weights' in locals() else {}, social_intercept if 'social_intercept' in locals() else 0,\n",
    "     social_scaler_mean if 'social_scaler_mean' in locals() else {}, social_scaler_scale if 'social_scaler_scale' in locals() else {}),\n",
    "    ('income', income_weights if 'income_weights' in locals() else {}, income_intercept if 'income_intercept' in locals() else 0,\n",
    "     income_scaler_mean if 'income_scaler_mean' in locals() else {}, income_scaler_scale if 'income_scaler_scale' in locals() else {}),\n",
    "    ('food', food_weights if 'food_weights' in locals() else {}, food_intercept if 'food_intercept' in locals() else 0,\n",
    "     food_scaler_mean if 'food_scaler_mean' in locals() else {}, food_scaler_scale if 'food_scaler_scale' in locals() else {}),\n",
    "    ('housing', housing_weights if 'housing_weights' in locals() else {}, housing_intercept if 'housing_intercept' in locals() else 0,\n",
    "     housing_scaler_mean if 'housing_scaler_mean' in locals() else {}, housing_scaler_scale if 'housing_scaler_scale' in locals() else {}),\n",
    "    ('social_cohesion', cohesion_weights if 'cohesion_weights' in locals() else {}, cohesion_intercept if 'cohesion_intercept' in locals() else 0,\n",
    "     cohesion_scaler_mean if 'cohesion_scaler_mean' in locals() else {}, cohesion_scaler_scale if 'cohesion_scaler_scale' in locals() else {})\n",
    "]\n",
    "\n",
    "for dimension, weights_dict, intercept, scaler_mean_dict, scaler_scale_dict in dimensions_data:\n",
    "    for feature, weight in weights_dict.items():\n",
    "        all_weights.append({\n",
    "            'dimension': dimension,\n",
    "            'feature': feature,\n",
    "            'weight': weight,\n",
    "            'method': 'linear_regression' if dimension != 'social_cohesion' else 'pca_with_artificial_intercept',\n",
    "            'intercept': intercept\n",
    "        })\n",
    "        \n",
    "        # Save scaling parameters for each feature\n",
    "        if feature in scaler_mean_dict and feature in scaler_scale_dict:\n",
    "            all_scaling_params.append({\n",
    "                'dimension': dimension,\n",
    "                'feature': feature,\n",
    "                'scaler_mean': scaler_mean_dict[feature],\n",
    "                'scaler_scale': scaler_scale_dict[feature]\n",
    "            })\n",
    "\n",
    "# Create dataframes\n",
    "weights_df = pd.DataFrame(all_weights)\n",
    "scaling_df = pd.DataFrame(all_scaling_params)\n",
    "\n",
    "# Create pivot table for weights\n",
    "if len(weights_df) > 0:\n",
    "    weights_pivot = weights_df.pivot(index='feature', columns='dimension', values='weight').fillna(0)\n",
    "\n",
    "# Save intercepts separately\n",
    "intercepts_data = []\n",
    "for dimension, weights_dict, intercept, scaler_mean_dict, scaler_scale_dict in dimensions_data:\n",
    "    if weights_dict:  # Only if we have weights for this dimension\n",
    "        intercepts_data.append({\n",
    "            'dimension': dimension,\n",
    "            'intercept': intercept\n",
    "        })\n",
    "\n",
    "intercepts_df = pd.DataFrame(intercepts_data)\n",
    "\n",
    "# prepare data \n",
    "\n",
    "# Create dictionaries for intercepts \n",
    "intercepts_dict = dict(zip(intercepts_df['dimension'], intercepts_df['intercept']))\n",
    "\n",
    "# Create scaling dictionaries by dimension\n",
    "scaling_means = {}\n",
    "scaling_scales = {}\n",
    "\n",
    "for dimension in scaling_df['dimension'].unique():\n",
    "    dim_scaling = scaling_df[scaling_df['dimension'] == dimension]\n",
    "    scaling_means[dimension] = dict(zip(dim_scaling['feature'], dim_scaling['scaler_mean']))\n",
    "    scaling_scales[dimension] = dict(zip(dim_scaling['feature'], dim_scaling['scaler_scale']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a83a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_dimension(data, dimension, weights_pivot, intercepts_dict, scaling_means, scaling_scales):\n",
    "    \n",
    "    # Get weights and intercept (PCA loadings + shift)\n",
    "    dimension_weights = weights_pivot[dimension]\n",
    "    dimension_weights = dimension_weights[dimension_weights != 0]\n",
    "    intercept = intercepts_dict.get(dimension, 0)\n",
    "    \n",
    "    # Prepare containers\n",
    "    feature_values = []\n",
    "    feature_names = []\n",
    "    weights_used = []\n",
    "    \n",
    "    for feature, weight in dimension_weights.items():\n",
    "        if feature in data.columns:\n",
    "            raw_values = data[feature].fillna(0).values\n",
    "            print(f\"  Raw values range: {raw_values.min():.4f} to {raw_values.max():.4f}\")\n",
    "            \n",
    "            if dimension in scaling_means and feature in scaling_means[dimension]:\n",
    "                mean = scaling_means[dimension][feature]\n",
    "                scale = scaling_scales[dimension][feature]\n",
    "                scaled = (raw_values - mean) / scale\n",
    "                \n",
    "                feature_values.append(scaled)\n",
    "                feature_names.append(feature)\n",
    "                weights_used.append(weight)\n",
    "                print(f\"   Feature added successfully\")\n",
    "            else:\n",
    "                print(f\"  No scaling params for {feature} in {dimension}\")\n",
    "        else:\n",
    "            print(f\"  Feature {feature} not found in data\")\n",
    "    \n",
    "    # Compute nowcast\n",
    "    if len(feature_values) > 0:\n",
    "        X_scaled = np.column_stack(feature_values)\n",
    "        weights_array = np.array(weights_used)\n",
    "        \n",
    "        print(f\"  X_scaled shape: {X_scaled.shape}\")\n",
    "        print(f\"  Weights shape: {weights_array.shape}\")\n",
    "        \n",
    "        projections = np.dot(X_scaled, weights_array) + intercept\n",
    "        return projections, feature_names\n",
    "    else:\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a26ee777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NOWCASTING: EDUCATIONAL_LAG ===\n",
      "  Raw values range: 3.5400 to 23.4200\n",
      "   Feature added successfully\n",
      "  Raw values range: -0.2623 to 0.0998\n",
      "   Feature added successfully\n",
      "  Raw values range: 1.4368 to 15.7447\n",
      "   Feature added successfully\n",
      "  Raw values range: 13.2446 to 23.0723\n",
      "   Feature added successfully\n",
      "  Raw values range: 0.0063 to 0.4924\n",
      "   Feature added successfully\n",
      "  X_scaled shape: (32, 5)\n",
      "  Weights shape: (5,)\n",
      "   R² = -14.690, RMSE = 19.692, MAE = 18.843\n",
      "\n",
      "=== NOWCASTING: HEALTH ===\n",
      "  Raw values range: -0.5596 to -0.1941\n",
      "   Feature added successfully\n",
      "  Raw values range: 9.6154 to 37.6623\n",
      "   Feature added successfully\n",
      "  Raw values range: 4.0375 to 9.7040\n",
      "   Feature added successfully\n",
      "  Raw values range: 0.0055 to 0.3892\n",
      "   Feature added successfully\n",
      "  Raw values range: 3.1200 to 23.3400\n",
      "   Feature added successfully\n",
      "  X_scaled shape: (32, 5)\n",
      "  Weights shape: (5,)\n",
      "   R² = -4.295, RMSE = 28.622, MAE = 25.585\n",
      "\n",
      "=== NOWCASTING: SOCIAL_SECURITY ===\n",
      "  Raw values range: -0.5338 to 0.0704\n",
      "   Feature added successfully\n",
      "  Raw values range: 12.1495 to 48.5849\n",
      "   Feature added successfully\n",
      "  Raw values range: 1.4807 to 3.6705\n",
      "   Feature added successfully\n",
      "  Raw values range: 0.0074 to 0.3144\n",
      "   Feature added successfully\n",
      "  Raw values range: 0.0194 to 0.4308\n",
      "   Feature added successfully\n",
      "  Raw values range: 2.3800 to 65.0000\n",
      "   Feature added successfully\n",
      "  X_scaled shape: (32, 6)\n",
      "  Weights shape: (6,)\n",
      "   R² = -6.648, RMSE = 38.953, MAE = 36.302\n",
      "\n",
      "=== NOWCASTING: INCOME ===\n",
      "  Raw values range: 0.0184 to 0.3373\n",
      "   Feature added successfully\n",
      "  Raw values range: -0.5201 to -0.2125\n",
      "   Feature added successfully\n",
      "  Raw values range: 1.1900 to 16.1100\n",
      "   Feature added successfully\n",
      "  Raw values range: 11.9149 to 46.2500\n",
      "   Feature added successfully\n",
      "  Raw values range: 28.6174 to 48.2174\n",
      "   Feature added successfully\n",
      "  Raw values range: 0.0040 to 0.2906\n",
      "   Feature added successfully\n",
      "  X_scaled shape: (32, 6)\n",
      "  Weights shape: (6,)\n",
      "   R² = -4.188, RMSE = 31.750, MAE = 28.574\n",
      "\n",
      "=== NOWCASTING: FOOD ===\n",
      "  Raw values range: -0.4521 to -0.1927\n",
      "   Feature added successfully\n",
      "  Raw values range: 0.4926 to 12.5000\n",
      "   Feature added successfully\n",
      "  Raw values range: 4.6954 to 10.1970\n",
      "   Feature added successfully\n",
      "  Raw values range: 0.5000 to 51.2500\n",
      "   Feature added successfully\n",
      "  Raw values range: 0.0063 to 0.4924\n",
      "   Feature added successfully\n",
      "  X_scaled shape: (32, 5)\n",
      "  Weights shape: (5,)\n",
      "   R² = -1.233, RMSE = 9.265, MAE = 7.167\n",
      "\n",
      "=== NOWCASTING: HOUSING ===\n",
      "  Raw values range: -0.6350 to -0.3349\n",
      "   Feature added successfully\n",
      "  Raw values range: 1.1200 to 32.1700\n",
      "   Feature added successfully\n",
      "  Raw values range: 0.0000 to 10.2804\n",
      "   Feature added successfully\n",
      "  Raw values range: 3.1491 to 12.3024\n",
      "   Feature added successfully\n",
      "  Raw values range: 0.0194 to 0.4308\n",
      "   Feature added successfully\n",
      "  X_scaled shape: (32, 5)\n",
      "  Weights shape: (5,)\n",
      "   R² = -0.010, RMSE = 10.566, MAE = 8.360\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "results_df['state'] = data_2022['state']\n",
    "\n",
    "dimension_mapping = {\n",
    "    'educational_lag': 'educ_target',\n",
    "    'health': 'health_target', \n",
    "    'social_security': 'social_target',\n",
    "    'income': 'income_target',\n",
    "    'food': 'food_target',\n",
    "    'housing': 'housing_target'}\n",
    "\n",
    "\n",
    "\n",
    "prediction_results = {}\n",
    "\n",
    "for dimension, target_col in dimension_mapping.items():\n",
    "    if target_col in data_2022.columns:\n",
    "        print(f\"\\n=== NOWCASTING: {dimension.upper()} ===\")\n",
    "        \n",
    "        predictions, features_used = predict_dimension(\n",
    "            data_2022, dimension, weights_pivot, intercepts_dict, scaling_means, scaling_scales)\n",
    "        \n",
    "        if predictions is not None:\n",
    "            results_df[f'{dimension}_actual'] = data_2022[target_col]\n",
    "            results_df[f'{dimension}_predicted'] = predictions\n",
    "            \n",
    "            actual = data_2022[target_col].values\n",
    "            r2 = r2_score(actual, predictions)\n",
    "            rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
    "            mae = mean_absolute_error(actual, predictions)\n",
    "            \n",
    "            prediction_results[dimension] = {\n",
    "                'r2': r2,\n",
    "                'rmse': rmse,\n",
    "                'mae': mae,\n",
    "                'features_used': features_used,\n",
    "                'n_features': len(features_used)}\n",
    "            \n",
    "            print(f\"   R² = {r2:.3f}, RMSE = {rmse:.3f}, MAE = {mae:.3f}\")\n",
    "        else:\n",
    "            print(f\"   ⚠️ Failed to predict {dimension}\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ Target {target_col} not found in data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
