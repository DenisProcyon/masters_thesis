{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef996967",
   "metadata": {},
   "source": [
    "## Search for the name of the State + 'news' / 'economy'\n",
    "Then use the advance nlp approach to do embedding on a neutral set of words to capture comments that talk about each poverty dimension, and do the sentiment on these comments. \n",
    "Get the count of the total comments and videos analyzed, the counts of comments that belong to each dimension and the sentiment score condition on each dimension. \n",
    "\n",
    "Here there should not be bias since we are:\n",
    "- doing a generic research (state + 'news' and 'noticias' - state + 'economy')\n",
    "- doing the embedding basing on words associated to the different dimensions of poverty but in a neutral way ('work', 'salary' ..). In this way we are able to identfy comments that talk about these issues, but we are not necessarily filtering for those that already talk about them negatively. The sentiment is not necessarily negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "YT_API_KEY = os.getenv(\"YT_API_KEY\")\n",
    "\n",
    "# Define states and search terms\n",
    "STATES_SEARCH_TERMS = {\n",
    "    \"Aguascalientes\": [\"Aguascalientes noticias\", \"Aguascalientes news\", \"Aguascalientes economía\"],\n",
    "    \"Baja California\": [\"Baja California noticias\", \"Baja California news\", \"Baja California economía\"],\n",
    "    \"Baja California Sur\": [\"Baja California Sur noticias\", \"Baja California Sur news\", \"Baja California Sur economía\"],\n",
    "    \"Campeche\": [\"Campeche noticias\", \"Campeche news\", \"Campeche economía\"],\n",
    "    \"Chiapas\": [\"Chiapas noticias\", \"Chiapas news\", \"Chiapas economía\"],\n",
    "    \"Chihuahua\": [\"Chihuahua noticias\", \"Chihuahua news\", \"Chihuahua economía\"],\n",
    "    \"Ciudad de México\": [\"Ciudad de México noticias\", \"Ciudad de México news\", \"Ciudad de México economía\"],\n",
    "    \"Coahuila\": [\"Coahuila noticias\", \"Coahuila news\", \"Coahuila economía\"],\n",
    "    \"Colima\": [\"Colima noticias\", \"Colima news\", \"Colima economía\"],\n",
    "    \"Durango\": [\"Durango noticias\", \"Durango news\", \"Durango economía\"],\n",
    "    \"Estado de México\": [\"Estado de México noticias\", \"Estado de México news\", \"Estado de México economía\"],\n",
    "    \"Guanajuato\": [\"Guanajuato noticias\", \"Guanajuato news\", \"Guanajuato economía\"],\n",
    "    \"Guerrero\": [\"Guerrero noticias\", \"Guerrero news\", \"Guerrero economía\"],\n",
    "    \"Hidalgo\": [\"Hidalgo noticias\", \"Hidalgo news\", \"Hidalgo economía\"],\n",
    "    \"Jalisco\": [\"Jalisco noticias\", \"Jalisco news\", \"Jalisco economía\"],\n",
    "    \"Michoacán\": [\"Michoacán noticias\", \"Michoacán news\", \"Michoacán economía\"],\n",
    "    \"Morelos\": [\"Morelos noticias\", \"Morelos news\", \"Morelos economía\"],\n",
    "    \"Nayarit\": [\"Nayarit noticias\", \"Nayarit news\", \"Nayarit economía\"],\n",
    "    \"Nuevo León\": [\"Nuevo León noticias\", \"Nuevo León news\", \"Nuevo León economía\"],\n",
    "    \"Oaxaca\": [\"Oaxaca noticias\", \"Oaxaca news\", \"Oaxaca economía\"],\n",
    "    \"Puebla\": [\"Puebla noticias\", \"Puebla news\", \"Puebla economía\"],\n",
    "    \"Querétaro\": [\"Querétaro noticias\", \"Querétaro news\", \"Querétaro economía\"],\n",
    "    \"Quintana Roo\": [\"Quintana Roo noticias\", \"Quintana Roo news\", \"Quintana Roo economía\"],\n",
    "    \"San Luis Potosí\": [\"San Luis Potosí noticias\", \"San Luis Potosí news\", \"San Luis Potosí economía\"],\n",
    "    \"Sinaloa\": [\"Sinaloa noticias\", \"Sinaloa news\", \"Sinaloa economía\"],\n",
    "    \"Sonora\": [\"Sonora noticias\", \"Sonora news\", \"Sonora economía\"],\n",
    "    \"Tabasco\": [\"Tabasco noticias\", \"Tabasco news\", \"Tabasco economía\"],\n",
    "    \"Tamaulipas\": [\"Tamaulipas noticias\", \"Tamaulipas news\", \"Tamaulipas economía\"],\n",
    "    \"Tlaxcala\": [\"Tlaxcala noticias\", \"Tlaxcala news\", \"Tlaxcala economía\"],\n",
    "    \"Veracruz\": [\"Veracruz noticias\", \"Veracruz news\", \"Veracruz economía\"],\n",
    "    \"Yucatán\": [\"Yucatán noticias\", \"Yucatán news\", \"Yucatán economía\"],\n",
    "    \"Zacatecas\": [\"Zacatecas noticias\", \"Zacatecas news\", \"Zacatecas economía\"]}\n",
    "\n",
    "\n",
    "# Neutral keyword-based descriptions for poverty dimensions: around 30 words per dimension \n",
    "# (60% standard spanish, 30% mexican/spanish slang and 10% english)\n",
    "POVERTY_DIMENSIONS = {\n",
    "    \"INCOME\": \"\"\"\n",
    "    empleo trabajo salario ingresos dinero economía sueldo ahorro impuestos\n",
    "    chamba lana nómina billete jale job salary income money\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO HEALTH SERVICES\": \"\"\"\n",
    "    salud médico hospital medicina tratamiento atención clínica seguro\n",
    "    sistema de salud servicios médicos doctor cuidado ir al doctor health insurance\n",
    "    seguro médico doctor particular ir a consulta healthcare medical treatment \n",
    "    \"\"\",\n",
    "    \n",
    "    \"EDUCATIONAL LAG\": \"\"\"\n",
    "    educación escuela universidad maestro estudiante aprendizaje escuela pública\n",
    "    clases formación conocimiento título bachillerato preparatoria escuela secundaria\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO SOCIAL SECURITY\": \"\"\"\n",
    "    seguridad social pensión jubilación contrato derechos laborales\n",
    "    prestaciones protección IMSS ISSSTE afore finiquito ahorro para retiro\n",
    "    cotizar retirement benefits social security worker rights informal job\n",
    "    \"\"\",\n",
    "    \n",
    "    \"HOUSING\": \"\"\"\n",
    "    vivienda casa habitación hogar alquiler renta depa housing utilities\n",
    "    servicios agua luz gas electricidad construcción propiedad rent \n",
    "    techo colonia vecindario urbanización asentamiento cuartito mortgage\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO FOOD\": \"\"\"\n",
    "    alimentación comida nutrición alimentos dieta cocinar recetas\n",
    "    canasta básica food security nutrition meal groceries\n",
    "    comida saludable dieta balanceada comida rápida comida chatarra\n",
    "    \"\"\",\n",
    "    \n",
    "    \"SOCIAL COHESION\": \"\"\"\n",
    "    comunidad sociedad integración participación convivencia barrio raza community\n",
    "    respeto diversidad solidaridad inclusión pertenencia \n",
    "    vecinos apoyo redes sociales confianza belonging inclusion\n",
    "    \"\"\"}\n",
    "\n",
    "# limits for scraping\n",
    "MAX_VIDEOS_PER_SEARCH = 100  \n",
    "MAX_COMMENTS_PER_VIDEO = 300  \n",
    "API_SLEEP_TIME = 0.5  \n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.dimension_names = list(POVERTY_DIMENSIONS.keys())\n",
    "        self.dimension_texts = []\n",
    "        for keywords in POVERTY_DIMENSIONS.values():\n",
    "            word_list = keywords.strip().split()\n",
    "            phrase = \" \".join(word_list)\n",
    "            self.dimension_texts.append(phrase)\n",
    "        self.dimension_embeddings = self.embedder.encode(self.dimension_texts, convert_to_tensor=True)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]', ' ', text)\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def classify_dimension(self, text):\n",
    "        if not text:\n",
    "            return None, 0.0\n",
    "        embedding = self.embedder.encode(text, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embedding, self.dimension_embeddings)[0]\n",
    "        max_idx = torch.argmax(cosine_scores).item()\n",
    "        return self.dimension_names[max_idx], cosine_scores[max_idx].item()\n",
    "\n",
    "    def get_sentiment_score(self, text):\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        stars = torch.argmax(outputs.logits, dim=1).item() + 1\n",
    "        return (stars - 3) / 2  # Normalize to [-1, 1]\n",
    "\n",
    "class YouTubeAnalyzer:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        self.processor = TextProcessor()\n",
    "\n",
    "    def search_videos(self, query, published_after, published_before, max_results=MAX_VIDEOS_PER_SEARCH):\n",
    "        \"\"\"Search for videos using a keyword query.\"\"\"\n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        try:\n",
    "            while len(videos) < max_results:\n",
    "                response = self.youtube.search().list(\n",
    "                    q=query,\n",
    "                    part=\"snippet\",\n",
    "                    maxResults=min(50, max_results - len(videos)),  # YouTube API allows max 50 per request\n",
    "                    pageToken=next_page_token,\n",
    "                    type=\"video\",\n",
    "                    order=\"relevance\",\n",
    "                    publishedAfter=published_after,\n",
    "                    publishedBefore=published_before,\n",
    "                    relevanceLanguage=\"es\"\n",
    "                ).execute()\n",
    "                \n",
    "                for item in response.get(\"items\", []):\n",
    "                    if item[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "                        videos.append({\n",
    "                            \"id\": item[\"id\"][\"videoId\"],\n",
    "                            \"title\": item[\"snippet\"][\"title\"],\n",
    "                            \"description\": item[\"snippet\"].get(\"description\", \"\"),\n",
    "                            \"published_at\": item[\"snippet\"][\"publishedAt\"]\n",
    "                        })\n",
    "                \n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or len(videos) >= max_results:\n",
    "                    break\n",
    "                \n",
    "                sleep(API_SLEEP_TIME)  # Avoid quota exceeded errors\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{query}': {e}\")\n",
    "        \n",
    "        print(f\"Found {len(videos)} videos for query '{query}'\")\n",
    "        return videos\n",
    "\n",
    "    def get_video_comments(self, video_id, max_comments=MAX_COMMENTS_PER_VIDEO):\n",
    "        \"\"\"Get comments for a specific video.\"\"\"\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        try:\n",
    "            while len(comments) < max_comments:\n",
    "                response = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=min(100, max_comments - len(comments)),  # YouTube API allows max 100 per request\n",
    "                    pageToken=next_page_token\n",
    "                ).execute()\n",
    "                \n",
    "                for item in response.get(\"items\", []):\n",
    "                    comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                    comments.append(comment_text)\n",
    "                \n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or len(comments) >= max_comments:\n",
    "                    break\n",
    "                \n",
    "                sleep(API_SLEEP_TIME)  # Avoid quota exceeded errors\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Many videos have comments disabled, so we'll just pass silently\n",
    "            pass\n",
    "        \n",
    "        return comments\n",
    "\n",
    "    def analyze_state_by_keywords(self, state_name, search_terms, date_range):\n",
    "        \"\"\"Analyze a state by searching for videos using specified search terms.\"\"\"\n",
    "        print(f\"\\nAnalyzing {state_name}...\")\n",
    "        dimension_stats = {dim: {\"sentiment_sum\": 0.0, \"count\": 0} for dim in POVERTY_DIMENSIONS}\n",
    "        total_videos = 0\n",
    "        total_comments = 0\n",
    "        \n",
    "        # Search for videos with each search term\n",
    "        for search_term in search_terms:\n",
    "            print(f\"  Searching for '{search_term}'...\")\n",
    "            videos = self.search_videos(\n",
    "                query=search_term,\n",
    "                published_after=date_range[\"published_after\"],\n",
    "                published_before=date_range[\"published_before\"],\n",
    "                max_results=MAX_VIDEOS_PER_SEARCH\n",
    "            )\n",
    "            \n",
    "            if not videos:\n",
    "                continue\n",
    "                \n",
    "            total_videos += len(videos)\n",
    "            \n",
    "            # Process videos\n",
    "            for video in tqdm(videos, desc=f\"Processing videos for '{search_term}'\"):\n",
    "                # Get video comments\n",
    "                comments = self.get_video_comments(video[\"id\"], MAX_COMMENTS_PER_VIDEO)\n",
    "                total_comments += len(comments)\n",
    "                \n",
    "                # Concatenate title, description and comments for analysis\n",
    "                all_texts = [video[\"title\"] + \". \" + video[\"description\"]] + comments\n",
    "                \n",
    "                # Analyze each text\n",
    "                for text in all_texts:\n",
    "                    clean = self.processor.clean_text(text)\n",
    "                    if len(clean) < 10:  # Skip very short texts\n",
    "                        continue\n",
    "                        \n",
    "                    dimension, confidence = self.processor.classify_dimension(clean)\n",
    "                    if confidence > 0.1:  # Only count if confidence is high enough\n",
    "                        sentiment = self.processor.get_sentiment_score(clean)\n",
    "                        dimension_stats[dimension][\"sentiment_sum\"] += sentiment\n",
    "                        dimension_stats[dimension][\"count\"] += 1\n",
    "        \n",
    "        print(f\"  Analyzed {total_videos} videos and {total_comments} comments for {state_name}\")\n",
    "        return dimension_stats, total_videos, total_comments\n",
    "\n",
    "def analyze_all_states():\n",
    "    analyzer = YouTubeAnalyzer(YT_API_KEY)\n",
    "    date_range = {\n",
    "        \"published_after\": \"2020-01-01T00:00:00Z\",\n",
    "        \"published_before\": \"2020-12-31T23:59:59Z\"\n",
    "    }\n",
    "    \n",
    "    # Create directories for results\n",
    "    os.makedirs(\"yt_data_2020\", exist_ok=True)\n",
    "    \n",
    "    # Store overall stats for summary\n",
    "    all_results = []\n",
    "    \n",
    "    for state, search_terms in STATES_SEARCH_TERMS.items():\n",
    "        stats, total_videos, total_comments = analyzer.analyze_state_by_keywords(\n",
    "            state_name=state,\n",
    "            search_terms=search_terms,\n",
    "            date_range=date_range\n",
    "        )\n",
    "        \n",
    "        # Create dataframe for this state\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                \"state\": state,\n",
    "                \"dimension\": dim.replace(\"_\", \" \").title(),\n",
    "                \"avg_sentiment\": v[\"sentiment_sum\"] / v[\"count\"] if v[\"count\"] else 0,\n",
    "                \"mentions_count\": v[\"count\"],\n",
    "                \"videos_analyzed\": total_videos,\n",
    "                \"comments_analyzed\": total_comments\n",
    "            }\n",
    "            for dim, v in stats.items()\n",
    "        ])\n",
    "        \n",
    "        # Save state-specific results\n",
    "        output_file = f\"yt_data_2020/{state.replace(' ', '_').lower()}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved results to {output_file}\")\n",
    "        \n",
    "        # Add to overall results\n",
    "        all_results.append(df)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    analyze_all_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
