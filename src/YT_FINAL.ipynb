{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef996967",
   "metadata": {},
   "source": [
    "# YouTube Analysis for Multidimensional Poverty Classification in Mexico\n",
    "\n",
    "This implementation creates a text classification system to analyze YouTube comments and categorize content according to multidimensional poverty dimensions. The analysis follows CONEVAL's (Consejo Nacional de Evaluación de la Política de Desarrollo Social) framework with adaptations for real-time social media data.\n",
    "\n",
    "We examine seven key dimensions of multidimensional poverty:\n",
    "\n",
    "- **Income**: Employment status, wages, economic instability, unemployment\n",
    "- **Access to Health Services**: Healthcare availability, medical infrastructure, health insurance\n",
    "- **Educational Lag**: School dropout rates, educational access, academic delays\n",
    "- **Access to Social Security**: Labor protection, social benefits, pension systems\n",
    "- **Housing**: Living conditions, basic utilities (water, electricity), housing quality\n",
    "- **Access to Food**: Food security, nutrition, food prices, hunger\n",
    "- **Social Cohesion**: Community integration, discrimination, social exclusion, belonging\n",
    "\n",
    "## Technical Methodology\n",
    "\n",
    "### 1. Data Collection\n",
    "\n",
    "**Search Parameters:**\n",
    "- **Temporal Scope**: Full year analysis (2022: January 1 - December 31)\n",
    "- **Geographic Coverage**: All 32 Mexican states\n",
    "- **Search Terms**: State name + [\"noticias\", \"news\", \"economía\"] (3 queries per state)\n",
    "- **Volume Limits**: 100 videos per query, 300 comments per video\n",
    "- **Language Priority**: Spanish content prioritized via `relevanceLanguage=\"es\"`\n",
    "\n",
    "### 2. Text Preprocessing\n",
    "\n",
    "**Preprocessing Steps:**\n",
    "1. **HTML/Markup Removal**: Strip HTML tags and web links\n",
    "2. **Character Normalization**: Preserve only alphanumeric + Spanish accented characters\n",
    "3. **Whitespace Normalization**: Remove extra spaces, convert to lowercase\n",
    "4. **Length Filtering**: Exclude texts shorter than 10 characters\n",
    "\n",
    "### 3. Embedding and Classification\n",
    "\n",
    "**Embedding Generation:**\n",
    "- **Model**: `paraphrase-multilingual-MiniLM-L12-v2` (768-dimensional embeddings)\n",
    "- **Language Support**: Optimized for Spanish, English, and mixed-language content\n",
    "- **Dimension Preprocessing**: Convert keyword lists to normalized text phrases for embedding\n",
    "\n",
    "**Classification Logic:**\n",
    "1. Generate embeddings for both input text and poverty dimension definitions\n",
    "2. Calculate cosine similarity between text and all dimension embeddings\n",
    "3. Assign text to highest-scoring dimension if score ≥ 0.10 threshold\n",
    "4. Classify as \"OTHER\" if below threshold (filters non-poverty content)\n",
    "\n",
    "### 4. Sentiment Analysis \n",
    "\n",
    "**Model**: `cardiffnlp/twitter-xlm-roberta-base-sentiment`\n",
    "- **Input**: Raw text\n",
    "- **Output**: The sentiment class with the highest predicted probability (`negative`, `neutral`, or `positive`)  \n",
    "- **Scoring**: The selected label is mapped to a fixed score:  \n",
    "  - `negative` = –1.0  \n",
    "  - `neutral` = 0.0  \n",
    "  - `positive` = +1.0  \n",
    "### 5. Extracted Components\n",
    "\n",
    "**State-Level Metrics:**\n",
    "- **Dimension Coverage**: Percentage of comments per poverty dimension\n",
    "- **Conditional Sentiment**: Average sentiment score per dimension per state\n",
    "- **General Statistics**: Total videos and comments analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c14129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load environment variables from .env file\n",
    "load_dotenv()\n",
    "YT_API_KEY = os.getenv(\"YT_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65254ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping of Mexican states with their corresponding search terms\n",
    "STATES_SEARCH_TERMS = {\n",
    "    \"Aguascalientes\": [\"Aguascalientes noticias\", \"Aguascalientes news\", \"Aguascalientes economía\"]}\n",
    "\n",
    "# Poverty dimension definitions with keywords. Each dimension contains a mix of formal Spanish terms, \n",
    "# Mexican slang, and English words to capture the diverse jargon used in YouTube comments\n",
    "POVERTY_DIMENSIONS = {\n",
    "    \"INCOME\": \"\"\"\n",
    "    empleo trabajo salario ingresos dinero economía sueldo ahorro impuestos\n",
    "    chamba lana nómina billete jale job salary income money\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO HEALTH SERVICES\": \"\"\"\n",
    "    salud médico hospital medicina tratamiento atención clínica seguro\n",
    "    sistema de salud servicios médicos doctor cuidado ir al doctor health insurance\n",
    "    seguro médico doctor particular ir a consulta healthcare medical treatment \n",
    "    \"\"\",\n",
    "    \n",
    "    \"EDUCATIONAL LAG\": \"\"\"\n",
    "    educación escuela universidad maestro estudiante aprendizaje escuela pública\n",
    "    clases formación conocimiento título bachillerato preparatoria escuela secundaria\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO SOCIAL SECURITY\": \"\"\"\n",
    "    seguridad social pensión jubilación contrato derechos laborales\n",
    "    prestaciones protección IMSS ISSSTE afore finiquito ahorro para retiro\n",
    "    cotizar retirement benefits social security worker rights informal job\n",
    "    \"\"\",\n",
    "    \n",
    "    \"HOUSING\": \"\"\"\n",
    "    vivienda casa habitación hogar alquiler renta depa housing utilities\n",
    "    servicios agua luz gas electricidad construcción propiedad rent \n",
    "    techo colonia vecindario urbanización asentamiento cuartito mortgage\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO FOOD\": \"\"\"\n",
    "    alimentación comida nutrición alimentos dieta cocinar recetas\n",
    "    canasta básica food security nutrition meal groceries\n",
    "    comida saludable dieta balanceada comida rápida comida chatarra\n",
    "    \"\"\",\n",
    "    \n",
    "    \"SOCIAL COHESION\": \"\"\"\n",
    "    comunidad sociedad integración participación convivencia barrio raza community\n",
    "    respeto diversidad solidaridad inclusión pertenencia \n",
    "    vecinos apoyo redes sociales confianza belonging inclusion\n",
    "    \"\"\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1255cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence threshold: comments with similarity scores below this threshold will be classified as 'OTHER'\n",
    "MIN_DIMENSION_CONFIDENCE = 0.10\n",
    "\n",
    "# define constants for YouTube API usage\n",
    "MAX_VIDEOS_PER_SEARCH = 100  \n",
    "MAX_COMMENTS_PER_VIDEO = 300  \n",
    "API_SLEEP_TIME = 0.5  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679289b8",
   "metadata": {},
   "source": [
    "### SimpleTextProcessor\n",
    "**Purpose**: Handles text preprocessing, dimension classification, and sentiment analysis\n",
    "\n",
    "**Key Methods:**\n",
    "- `clean_text()`: Normalizes and cleans input text\n",
    "- `classify_dimension()`: Assigns text to poverty dimensions using embeddings\n",
    "- `get_sentiment_score()`: Computes sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3ad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextProcessor:\n",
    "    def __init__(self):\n",
    "        # initialize multilingual sentence embedding model\n",
    "        # this model generates embeddings for both comments and poverty dimension definitions\n",
    "        self.embedder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "        # load multilingual sentiment model \n",
    "        self.sentiment_model = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.sentiment_model)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.sentiment_model)\n",
    "\n",
    "        # load poverty dimension names and definitions\n",
    "        self.dimension_names = list(POVERTY_DIMENSIONS.keys())\n",
    "        self.dimension_texts = []\n",
    "\n",
    "        # format and join keyword definitions into single phrases per dimension\n",
    "        for keywords in POVERTY_DIMENSIONS.values():\n",
    "            word_list = keywords.strip().split()\n",
    "            phrase = \" \".join(word_list)\n",
    "            self.dimension_texts.append(phrase)\n",
    "\n",
    "        # precompute embeddings for each poverty dimension definition\n",
    "        self.dimension_embeddings = self.embedder.encode(self.dimension_texts, convert_to_tensor=True)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # remove HTML tags\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        # remove URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        # remove punctuation and special characters (preserving accented Spanish characters)\n",
    "        text = re.sub(r'[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]', ' ', text)\n",
    "        # normalize whitespace and convert to lowercase\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def classify_dimension(self, text):\n",
    "        if not text:\n",
    "            return \"OTHER\", 0.0\n",
    "\n",
    "        # generate embedding for the input text\n",
    "        embedding = self.embedder.encode(text, convert_to_tensor=True)\n",
    "        # compute cosine similarity between input and all dimension definitions\n",
    "        cosine_scores = util.cos_sim(embedding, self.dimension_embeddings)[0]\n",
    "        # identify the most similar dimension\n",
    "        max_idx = torch.argmax(cosine_scores).item()\n",
    "        max_score = cosine_scores[max_idx].item()\n",
    "\n",
    "        # if similarity is below threshold, classify as \"OTHER\"\n",
    "        if max_score < MIN_DIMENSION_CONFIDENCE:\n",
    "            return \"OTHER\", max_score\n",
    "\n",
    "        # otherwise, return the best matching dimension and its similarity score\n",
    "        return self.dimension_names[max_idx], max_score\n",
    "    \n",
    "    def get_sentiment_score(self, text):\n",
    "        if not text:\n",
    "            return 0.0\n",
    "\n",
    "        # tokenize the input\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "\n",
    "        # run the model in inference mode: no gradient computation is needed (faster and more memory-efficient)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_class = torch.argmax(logits, dim=1).item()  # 0=negative, 1=neutral, 2=positive\n",
    "\n",
    "        # map class index to fixed score\n",
    "        class_to_score = {0: -1, 1: 0, 2: 1}\n",
    "        return class_to_score[predicted_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433a851e",
   "metadata": {},
   "source": [
    "### SimpleYouTubeAnalyzer  \n",
    "**Purpose**: Manages YouTube API interactions and orchestrates analysis pipeline\n",
    "\n",
    "**Key Methods:**\n",
    "- `search_videos()`: Retrieves videos based on search terms and date range\n",
    "- `get_video_comments()`: Extracts comments from individual videos\n",
    "- `analyze_state_by_keywords()`: Processes complete state analysis workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ff7424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle YouTube API interactions and content analysis. \n",
    "class SimpleYouTubeAnalyzer:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        self.processor = SimpleTextProcessor()\n",
    "\n",
    "    # search for videos on YouTube based on a query and date range\n",
    "    def search_videos(self, query, published_after, published_before, max_results=100):\n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        try:\n",
    "            while len(videos) < max_results:\n",
    "                # request videos from YouTube API with pagination\n",
    "                response = self.youtube.search().list(\n",
    "                    q=query,\n",
    "                    part=\"snippet\",\n",
    "                    maxResults=min(50, max_results - len(videos)),  # API limit is 50 per request\n",
    "                    pageToken=next_page_token,\n",
    "                    type=\"video\",\n",
    "                    order=\"relevance\",\n",
    "                    publishedAfter=published_after,\n",
    "                    publishedBefore=published_before,\n",
    "                    relevanceLanguage=\"es\"  # prioritize Spanish content\n",
    "                ).execute()\n",
    "                \n",
    "                # extract video information from API response\n",
    "                for item in response.get(\"items\", []):\n",
    "                    if item[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "                        videos.append({\n",
    "                            \"id\": item[\"id\"][\"videoId\"],\n",
    "                            \"title\": item[\"snippet\"][\"title\"],\n",
    "                            \"description\": item[\"snippet\"].get(\"description\", \"\"),\n",
    "                            \"published_at\": item[\"snippet\"][\"publishedAt\"]\n",
    "                        })\n",
    "                \n",
    "                # check if more pages are available\n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or len(videos) >= max_results:\n",
    "                    break\n",
    "                \n",
    "                # small pause to avoid quota issues\n",
    "                sleep(API_SLEEP_TIME)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{query}': {e}\")\n",
    "        \n",
    "        print(f\"Found {len(videos)} videos for query '{query}'\")\n",
    "        return videos\n",
    "\n",
    "    # retrieve comments for a specific video\n",
    "    def get_video_comments(self, video_id, max_comments=300):\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        try:\n",
    "            while len(comments) < max_comments:\n",
    "                # request comments with pagination support\n",
    "                response = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=min(100, max_comments - len(comments)),  # API limit is 100 per request\n",
    "                    pageToken=next_page_token\n",
    "                ).execute()\n",
    "                \n",
    "                # extract comment text from API response\n",
    "                for item in response.get(\"items\", []):\n",
    "                    comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                    comments.append(comment_text)\n",
    "                \n",
    "                # check for additional pages\n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or len(comments) >= max_comments:\n",
    "                    break\n",
    "                \n",
    "                # pause to avoid quota issues \n",
    "                sleep(API_SLEEP_TIME)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # hanlde videos that have disabled comments\n",
    "            pass\n",
    "        \n",
    "        return comments\n",
    "\n",
    "    def analyze_state_by_keywords(self, state_name, search_terms, date_range):\n",
    "        print(f\"\\nAnalyzing {state_name}...\")\n",
    "        \n",
    "        # initialize statistics tracking for all categories and 'OTHER'\n",
    "        all_categories = list(POVERTY_DIMENSIONS.keys()) + [\"OTHER\"]\n",
    "        dimension_stats = {cat: {\"sentiment_sum\": 0.0, \"count\": 0} for cat in all_categories}\n",
    "        \n",
    "        total_videos = 0\n",
    "        total_comments = 0\n",
    "        classification_stats = {cat: 0 for cat in all_categories}\n",
    "        \n",
    "        # process each search term for the current state\n",
    "        for search_term in search_terms:\n",
    "            print(f\"  Searching for '{search_term}'...\")\n",
    "            \n",
    "            # get relevant videos for this search term\n",
    "            videos = self.search_videos(\n",
    "                query=search_term,\n",
    "                published_after=date_range[\"published_after\"],\n",
    "                published_before=date_range[\"published_before\"],\n",
    "                max_results=MAX_VIDEOS_PER_SEARCH\n",
    "            )\n",
    "            \n",
    "            if not videos:\n",
    "                continue\n",
    "                \n",
    "            total_videos += len(videos)\n",
    "            \n",
    "            # process each video and its comments\n",
    "            for video in tqdm(videos, desc=f\"Processing videos for '{search_term}'\"):\n",
    "                # extract comments from the current video\n",
    "                comments = self.get_video_comments(video[\"id\"], MAX_COMMENTS_PER_VIDEO)\n",
    "                total_comments += len(comments)\n",
    "                \n",
    "                # combine video metadata with comments\n",
    "                all_texts = [video[\"title\"] + \". \" + video[\"description\"]] + comments\n",
    "                \n",
    "                # analyze each piece of text individually\n",
    "                for text in all_texts:\n",
    "                    clean = self.processor.clean_text(text)\n",
    "                    \n",
    "                    # skip very short texts \n",
    "                    if len(clean) < 10:\n",
    "                        continue\n",
    "                    \n",
    "                    # classify text into poverty dimensions or 'OTHER'\n",
    "                    category, confidence = self.processor.classify_dimension(clean)\n",
    "                    \n",
    "                    # update classification statistics for reporting\n",
    "                    classification_stats[category] += 1\n",
    "                    \n",
    "                    # calculate sentiment score for all classified texts\n",
    "                    sentiment = self.processor.get_sentiment_score(clean)\n",
    "                    dimension_stats[category][\"sentiment_sum\"] += sentiment\n",
    "                    dimension_stats[category][\"count\"] += 1\n",
    "        \n",
    "        # print classification statistics for this state\n",
    "        total_texts = sum(classification_stats.values())\n",
    "        print(f\"  Classification statistics for {state_name}:\")\n",
    "        for category, count in classification_stats.items():\n",
    "            percentage = (count / total_texts * 100) if total_texts > 0 else 0\n",
    "            print(f\"    {category}: {count} texts ({percentage:.1f}%)\")\n",
    "        \n",
    "        print(f\"  Analyzed {total_videos} videos and {total_comments} comments for {state_name}\")\n",
    "        return dimension_stats, total_videos, total_comments, classification_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2dc361",
   "metadata": {},
   "source": [
    "## Output Files\n",
    "\n",
    "Results are saved as CSV files in `yt_data_2022/` directory:\n",
    "- One file per state: `{state_name}.csv`\n",
    "- Aggregated statistics across all dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1da711bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noemilucchi/miniforge3/envs/new/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Aguascalientes...\n",
      "  Searching for 'Aguascalientes noticias'...\n",
      "Found 100 videos for query 'Aguascalientes noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Aguascalientes noticias': 100%|██████████| 100/100 [04:02<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Aguascalientes news'...\n",
      "Found 100 videos for query 'Aguascalientes news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Aguascalientes news': 100%|██████████| 100/100 [07:14<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Aguascalientes economía'...\n",
      "Found 100 videos for query 'Aguascalientes economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Aguascalientes economía': 100%|██████████| 100/100 [02:42<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classification statistics for Aguascalientes:\n",
      "    INCOME: 4819 texts (35.6%)\n",
      "    ACCESS TO HEALTH SERVICES: 967 texts (7.1%)\n",
      "    EDUCATIONAL LAG: 2072 texts (15.3%)\n",
      "    ACCESS TO SOCIAL SECURITY: 325 texts (2.4%)\n",
      "    HOUSING: 1001 texts (7.4%)\n",
      "    ACCESS TO FOOD: 796 texts (5.9%)\n",
      "    SOCIAL COHESION: 1338 texts (9.9%)\n",
      "    OTHER: 2234 texts (16.5%)\n",
      "  Analyzed 300 videos and 13767 comments for Aguascalientes\n",
      "Saved results to TRIAL2/aguascalientes.csv\n"
     ]
    }
   ],
   "source": [
    "#  main execution function that processes all Mexican states\n",
    "def analyze_all_states_simple():\n",
    "    # initialize the YouTube analyzer with API credentials\n",
    "    analyzer = SimpleYouTubeAnalyzer(YT_API_KEY)\n",
    "    \n",
    "    # define the analysis time period (2022 full year in this case)\n",
    "    date_range = {\n",
    "        \"published_after\": \"2022-01-01T00:00:00Z\",\n",
    "        \"published_before\": \"2022-12-31T23:59:59Z\"}\n",
    "    \n",
    "    # create output directory for results\n",
    "    os.makedirs(\"TRIAL2\", exist_ok=True)\n",
    "    \n",
    "    # initialize lists for aggregated results\n",
    "    all_results = []\n",
    "    overall_classification_stats = {}\n",
    "    \n",
    "    # process each Mexican state individually\n",
    "    for state, search_terms in STATES_SEARCH_TERMS.items():\n",
    "        # analyze the current state using its specific search terms\n",
    "        stats, total_videos, total_comments, classification_stats = analyzer.analyze_state_by_keywords(\n",
    "            state_name=state,\n",
    "            search_terms=search_terms,\n",
    "            date_range=date_range)\n",
    "        \n",
    "        # accumulate classification statistics across all states\n",
    "        for category, count in classification_stats.items():\n",
    "            overall_classification_stats[category] = overall_classification_stats.get(category, 0) + count\n",
    "        \n",
    "        # prepare structured data for this state\n",
    "        df_rows = []\n",
    "        for category, v in stats.items():\n",
    "            df_rows.append({\n",
    "                \"state\": state,\n",
    "                \"dimension\": category.replace(\"_\", \" \").title(),\n",
    "                \"avg_sentiment\": v[\"sentiment_sum\"] / v[\"count\"] if v[\"count\"] > 0 else 0,\n",
    "                \"mentions_count\": v[\"count\"],\n",
    "                \"percentage_of_total\": (v[\"count\"] / sum([s[\"count\"] for s in stats.values()]) * 100) if sum([s[\"count\"] for s in stats.values()]) > 0 else 0,\n",
    "                \"videos_analyzed\": total_videos,\n",
    "                \"comments_analyzed\": total_comments})\n",
    "        \n",
    "        # create DataFrame for this state's results\n",
    "        df = pd.DataFrame(df_rows)\n",
    "        \n",
    "        # save state-specific results to CSV file\n",
    "        output_file = f\"TRIAL2/{state.replace(' ', '_').lower()}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved results to {output_file}\")\n",
    "        \n",
    "        # add to aggregated results collection\n",
    "        all_results.append(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_all_states_simple()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
