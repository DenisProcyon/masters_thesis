{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from yt_client.yt_client import YouTubeClient\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from time import sleep\n",
    "from pprint import pprint\n",
    "import re\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3751d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "YT_API_KEY = os.getenv(\"YT_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of states to scrape \n",
    "STATES = [\n",
    "    \"Tamaulipas\",\n",
    "    \"Baja California\", \n",
    "    \"Zacatecas\", \n",
    "    \"Colima\", \n",
    "    \"Jalisco\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb27248",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBLISHED_AFTER = \"2022-01-01T00:00:00Z\"\n",
    "PUBLISHED_BEFORE = \"2022-12-31T23:59:59Z\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d996a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output folder \n",
    "os.makedirs(\"yt_data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models \n",
    "nlp_es = spacy.load(\"es_core_news_sm\")\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac08113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom stop words\n",
    "custom_stopwords = {\n",
    "    \"que\", \"yo\", \"eh\", \"si\", \"pa\", \"x\", \"xd\", \"el\", \"y\", \"la\",\n",
    "    \"the\", \"i\", \"and\", \"you\", \"this\", \"to\", \"is\", \"it\", \"of\", \"in\", \"on\", \"for\", \"me\", \"my\", \"do\", \"at\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ae02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the poverty dimensions and their keywords with balanced expressions\n",
    "poverty_dimensions = {\n",
    "    \"INCOME\": [\n",
    "        \"desempleo\", \"sueldo mínimo\", \"salario bajo\", \"inflación\", \"deudas\",\n",
    "        \"crisis\", \"préstamos\", \"despidos\", \"recortes\", \"quiebra\", \"pobreza\",\n",
    "        \"falta de chamba\", \"pérdida de empleo\", \"no hay trabajo\", \"sin chamba\",\n",
    "        \"unemployment\", \"low wages\", \"bankrupt\", \"jobless\", \"struggling\",\n",
    "        \"salario miserable\", \"jodido\", \"quedarse sin trabajo\",\n",
    "        \"inestabilidad económica\", \"dificultades financieras\", \"falta de recursos\",\n",
    "        \"sin ingresos suficientes\", \"incertidumbre laboral\", \"situación precaria\",\n",
    "        \"bajos ingresos\", \"falta de oportunidades\", \"dificultad para ahorrar\",\n",
    "        \"necesidades básicas insatisfechas\", \"problemas económicos\", \"vivir al día\",\n",
    "        \"inseguridad laboral\", \"trabajos temporales\", \"sin ingreso fijo\", \n",
    "        \"economía familiar reducida\", \"salario insuficiente\", \"búsqueda de empleo\",\n",
    "        \"sin un peso\", \"estar en la ruina\", \"no me alcanza\",\n",
    "        \"estar quebrado\", \"ganar una miseria\", \"no hay chamba\",\n",
    "        \"sueldo de hambre\", \"vivir con lo que cae\", \"quedé sin trabajo\",\n",
    "        \"buscando trabajo\"],\n",
    "    \"ACCESS TO HEALTH SERVICES\": [\n",
    "        \"sin medicinas\", \"hospital lejano\", \"esperas\", \"sin seguro\", \"mala atención\",\n",
    "        \"enfermedad\", \"rechazado\", \"sin tratamiento\", \"medicinas caras\", \"sin doctores\",\n",
    "        \"no hay medicinas\", \"no hay doctores\", \"centro de salud cerrado\", \"hospital saturado\",\n",
    "        \"no hay atención médica\", \"healthcare crisis\", \"expensive medicine\", \"medical neglect\",\n",
    "        \"mal servicio médico\", \"broncas de salud\", \"te dejan morir\",\n",
    "        \"deficiencia en el sistema de salud\", \"falta de personal médico\", \n",
    "        \"servicios médicos inaccesibles\", \"centros médicos distantes\", \n",
    "        \"tratamientos costosos\", \"falta de atención preventiva\", \n",
    "        \"servicios de urgencias insuficientes\", \"espera excesiva para atención\",\n",
    "        \"falta de especialistas\", \"medicamentos inaccesibles\", \"atención médica deficiente\",\n",
    "        \"personas sin cobertura médica\", \"salud pública colapsada\", \"sistemas de salud precarios\",\n",
    "        \"personal médico insuficiente\", \"instalaciones sanitarias inadecuadas\",\n",
    "        \"horas esperando\", \"no hay ni aspirinas\", \"no hay ni paracetamol\", \n",
    "        \"hospitales lejanos\", \"pura negligencia\", \"no hay citas\",\n",
    "        \"citas médicas muy tardías\", \"salud pública deficiente\",  \n",
    "        \"automedicación\", \"no hay médicos especialistas\"],\n",
    "    \"EDUCATIONAL LAG\": [\n",
    "        \"sin escuela\", \"analfabetismo\", \"deserción\", \"acceso a la educación\",\n",
    "        \"calidad educativa\", \"recursos didácticos\", \"infraestructura escolar\",\n",
    "        \"escuela lejana\", \"escuela lejos\", \"sin útiles\", \"ausentismo\",\n",
    "        \"sin maestros\", \"rezago escolar\", \"niños sin clases\", \"poca educación\",\n",
    "        \"school dropout\", \"no teachers\", \"poor education\", \"no school supplies\",\n",
    "        \"ni estudian ni trabajan\", \"escuela en ruinas\",\n",
    "        \"falta de acceso a educación\", \"baja tasa de escolaridad\", \"educación de baja calidad\",\n",
    "        \"jóvenes sin preparación\", \"falta de oportunidades educativas\", \"formación académica limitada\",\n",
    "        \"problemas de aprendizaje\", \"abandono escolar temprano\", \"educación incompleta\",\n",
    "        \"infraestructura educativa deficiente\", \"falta de material didáctico\", \n",
    "        \"educación discontinua\", \"escuelas en mal estado\", \"escolaridad interrumpida\",\n",
    "        \"recursos pedagógicos insuficientes\", \"desigualdad educativa\",\n",
    "        \"no hay profes\", \"maestros fantasma\", \"se caen los techos\", \n",
    "        \"escuelas sin recursos\", \"no hay clases\", \"jóvenes sin estudiar\", \n",
    "        \"escuelas en mal estado\", \"faltan materiales escolares\",\n",
    "        \"escuelas sin agua\", \"difícil acceso a la escuela\"],\n",
    "    \"ACCESS TO SOCIAL SECURITY\": [\n",
    "        \"sin contrato\", \"economía informal\", \"informal\", \"sin pensión\",\n",
    "        \"sin derechos\", \"explotación\", \"sin ahorro\", \"sin prestaciones\",\n",
    "        \"desprotección\", \"trabajo ilegal\", \"sin seguro\", \"sin IMSS\",\n",
    "        \"chamba sin contrato\", \"trabajo mal pagado\", \"explotado\", \n",
    "        \"no benefits\", \"no retirement\", \"informal jobs\", \"unprotected workers\",\n",
    "        \"sin aguinaldo\", \"trabajo en negro\",\n",
    "        \"empleo sin protección social\", \"falta de seguridad laboral\", \"sin beneficios laborales\",\n",
    "        \"carencia de servicios sociales\", \"trabajadores desprotegidos\", \"jubilación insuficiente\",\n",
    "        \"falta de apoyo institucional\", \"vulnerabilidad social\", \"falta de cobertura social\",\n",
    "        \"desprotección laboral\", \"servicios sociales inaccesibles\", \"sin acceso a ayudas sociales\",\n",
    "        \"empleo precario\", \"trabajadores marginados\", \"condiciones laborales precarias\",\n",
    "        \"empleos de subsistencia\", \"sin acceso a beneficios sociales\",\n",
    "        \"sin papeles\", \"trabajo por fuera\", \"trabajo no registrado\", \n",
    "        \"te dan de alta con menos\", \"trabajo informal\", \"sin jubilación\",\n",
    "        \"condiciones abusivas\", \"sin beneficios laborales\", \"sin finiquito\",\n",
    "        \"sin protección laboral\"],\n",
    "    \"HOUSING\": [\n",
    "        \"sin agua\", \"sin luz\", \"hacinamiento\", \"desalojo\", \"vivienda precaria\",\n",
    "        \"sin techo\", \"goteras\", \"renta cara\", \"casa insegura\", \"sin baño\",\n",
    "        \"techos de lámina\", \"cuartos de cartón\", \"casas abandonadas\", \"inundaciones\",\n",
    "        \"bad housing\", \"slum\", \"no electricity\", \"unsafe home\", \"eviction notice\",\n",
    "        \"vivir entre ratas\", \"se les cae la casa\",\n",
    "        \"vivienda inadecuada\", \"condiciones habitacionales deficientes\", \"falta de servicios básicos\",\n",
    "        \"asentamientos irregulares\", \"déficit habitacional\", \"falta de acceso a vivienda digna\",\n",
    "        \"viviendas en zonas de riesgo\", \"condiciones insalubres\", \"viviendas sin servicios esenciales\",\n",
    "        \"problemas de habitabilidad\", \"viviendas improvisadas\", \"hogares sin infraestructura básica\",\n",
    "        \"viviendas en mal estado\", \"construcciones vulnerables\", \"viviendas en zonas marginales\",\n",
    "        \"imposibilidad de acceder a vivienda\", \"viviendas no seguras\",\n",
    "        \"casa de lámina\", \"viviendo en la calle\", \"casas de cartón\", \"barrio marginal\",\n",
    "        \"casa sin drenaje\", \"colonia sin pavimentar\", \"renta muy cara\",\n",
    "        \"sin agua potable\", \"se mete el agua cuando llueve\", \"apagones frecuentes\",\n",
    "        \"casa sin ventilación\", \"techo que gotea\"],\n",
    "    \"ACCESS TO FOOD\": [\n",
    "        \"hambre\", \"desnutrición\", \"comida escasa\", \"sin alimentos\", \"comida cara\",\n",
    "        \"ayuda alimentaria\", \"escasez\", \"comida mala\", \"dieta pobre\", \"inseguridad alimentaria\",\n",
    "        \"no hay comida\", \"ni pa' frijoles\", \"colas para comida\", \"falta de comida\",\n",
    "        \"food insecurity\", \"starving\", \"malnutrition\", \"no food on table\",\n",
    "        \"tragando aire\", \"comer una vez al día\",\n",
    "        \"alimentación insuficiente\", \"nutrición inadecuada\", \"falta de acceso a alimentos\",\n",
    "        \"déficit nutricional\", \"alimentos de mala calidad\", \"dieta insuficiente\",\n",
    "        \"carencia alimentaria\", \"pobreza alimentaria\", \"falta de variedad en la dieta\",\n",
    "        \"aumento de precios alimentarios\", \"falta de alimentos básicos\", \"crisis alimentaria\",\n",
    "        \"problemas de desnutrición\", \"hambruna\", \"dependencia de asistencia alimentaria\",\n",
    "        \"alimentos inaccesibles\", \"problemas de alimentación\",\n",
    "        \"morirse de hambre\", \"ni tortillas hay\", \"comer cada tercer día\",\n",
    "        \"no alcanza para comida\", \"niños con hambre\", \"sin comida suficiente\",\n",
    "        \"no hay ni para huevos\", \"a puro arroz\", \"comiendo solo una vez al día\",\n",
    "        \"todo muy caro\", \"precios imposibles\"],\n",
    "    \"SOCIAL COHESION\": [\n",
    "        \"fragmentación\", \"polarización\", \"exclusión\", \"discriminación\", \"conflicto\",\n",
    "        \"desconfianza\", \"marginalización\", \"tensiones\", \"estigmatización\",\n",
    "        \"racismo\", \"odio de clase\", \"no hay comunidad\", \"violencia entre vecinos\",\n",
    "        \"division social\", \"hate speech\", \"segregation\", \"marginalized\", \"resentimiento social\",\n",
    "        \"pandillas\", \"se odian entre barrios\",\n",
    "        \"falta de integración social\", \"ruptura del tejido social\", \"división comunitaria\",\n",
    "        \"falta de solidaridad\", \"problemas de convivencia\", \"falta de inclusión\",\n",
    "        \"desigualdad social\", \"sectores sociales aislados\", \"segregación urbana\",\n",
    "        \"comunidades divididas\", \"aislamiento social\", \"falta de pertenencia\",\n",
    "        \"grupos sociales antagónicos\", \"conflictos comunitarios\", \"desintegración social\",\n",
    "        \"falta de cohesión\", \"degradación de relaciones sociales\", \"fracturas sociales\",\n",
    "        \"nadie se habla\", \"comunidades divididas\", \"no confiar en nadie\", \n",
    "        \"barrios enemistados\", \"zonas peligrosas\", \"sociedades cerradas\",\n",
    "        \"discriminación por origen\", \"separación entre ricos y pobres\",\n",
    "        \"barrios conflictivos\", \"falta de unión\", \"intolerancia social\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to pre process comments\n",
    "def clean_comment(text):\n",
    "    # Remove links and special characters\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-ZáéíóúüñÁÉÍÓÚÜÑ\\s]\", \"\", text)\n",
    "    text = text.lower().strip()\n",
    "\n",
    "    # Detect language (Spanish as default)\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "    except:\n",
    "        lang = \"es\"\n",
    "\n",
    "    nlp = nlp_en if lang == \"en\" else nlp_es\n",
    "\n",
    "    # Remove stop words and lemmatize\n",
    "    doc = nlp(text)\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc\n",
    "        if not token.is_stop and not token.is_punct\n",
    "        and token.lemma_ not in custom_stopwords\n",
    "        and token.lemma_ != \"\"\n",
    "    ]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# function to get comments from a video\n",
    "def get_video_comments(api_key, video_id, max_comments=400):\n",
    "    youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_comments:\n",
    "        try:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            ).execute()\n",
    "\n",
    "            comments += [\n",
    "                item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                for item in response[\"items\"]\n",
    "            ]\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "            sleep(0.5)  \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in video {video_id}: {e}\")\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "\n",
    "# function to get comments from all videos\n",
    "def get_all_comments(video_ids, api_key, max_comments_per_video=400):\n",
    "    all_comments = {}\n",
    "    for video_id in tqdm(video_ids, desc=\"Fetching comments\"):\n",
    "        all_comments[video_id] = get_video_comments(api_key, video_id, max_comments_per_video)\n",
    "    return all_comments\n",
    "\n",
    "\n",
    "# function to calculate sentiment score \n",
    "def get_bert_sentiment(text, tokenizer, model):\n",
    "    # Return 0 if text is empty or too short\n",
    "    if not text or len(text) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Convert to string if input is a list\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Tokenize and encode the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Get outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # The multilingual BERT model outputs value from 1 to 5, we convert it to a scale -1 to 1\n",
    "    predicted_class = torch.argmax(outputs.logits, dim=1).item()\n",
    "    stars = predicted_class + 1\n",
    "    sentiment_score = (stars - 3) / 2\n",
    "    \n",
    "    return sentiment_score\n",
    "\n",
    "\n",
    "\n",
    "def get_videos_for_state(state_name, yt_client, published_after, published_before):\n",
    "    all_videos = []\n",
    "    \n",
    "    contexts = [\n",
    "        f\"{state_name} economía\",\n",
    "        f\"{state_name} trabajo\",\n",
    "        f\"{state_name} empleo\",\n",
    "        f\"{state_name} salud\",\n",
    "        f\"{state_name} hospital\",\n",
    "        f\"{state_name} servicios médicos\",\n",
    "        f\"{state_name} médico\", \n",
    "        f\"{state_name} educación\",\n",
    "        f\"{state_name} escuela\",\n",
    "        f\"{state_name} universidad\",\n",
    "        f\"{state_name} estudiantes\",\n",
    "        f\"{state_name} seguridad social\",\n",
    "        f\"{state_name} beneficios\",\n",
    "        f\"{state_name} protección social\",\n",
    "        f\"{state_name} seguro social\",\n",
    "        f\"{state_name} vivienda\",\n",
    "        f\"{state_name} hogar\",\n",
    "        f\"{state_name} habitaciónes\",\n",
    "        f\"{state_name} servicios básicos\",\n",
    "        f\"{state_name} alimentación\",\n",
    "        f\"{state_name} comida\",\n",
    "        f\"{state_name} alimentos\",\n",
    "        f\"{state_name} comunidad\",\n",
    "        f\"{state_name} sociedad\",\n",
    "        f\"{state_name} integración\"] \n",
    "    \n",
    "    for context in contexts:\n",
    "        context_videos = yt_client.get_videos_by_keyword(\n",
    "            keyword=context,\n",
    "            published_after=published_after,\n",
    "            published_before=published_before,\n",
    "            limit=100)\n",
    "        all_videos.extend(context_videos)\n",
    "    \n",
    "    # Remove duplicates by video ID\n",
    "    unique_videos = []\n",
    "    seen_ids = set()\n",
    "    for video in all_videos:\n",
    "        if '_id' in video and 'videoId' in video['_id']:\n",
    "            video_id = video['_id']['videoId']\n",
    "            if video_id not in seen_ids:\n",
    "                unique_videos.append(video)\n",
    "                seen_ids.add(video_id)\n",
    "    \n",
    "    return unique_videos\n",
    "\n",
    "\n",
    "def analyze_state(state_name, published_after, published_before, yt_client, tokenizer, model):\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Processing state: {state_name}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    \n",
    "    # Create directory for EDA results\n",
    "    eda_dir = os.path.join(\"yt_data\", f\"{state_name.lower().replace(' ', '_')}_eda\")\n",
    "    os.makedirs(eda_dir, exist_ok=True)\n",
    "    \n",
    "    # Get videos for the state\n",
    "    print(f\"Fetching videos for {state_name}...\")\n",
    "    keyword_videos = get_videos_for_state(\n",
    "        state_name=state_name,\n",
    "        yt_client=yt_client, \n",
    "        published_after=published_after, \n",
    "        published_before=published_before)\n",
    "    print(f\"Found {len(keyword_videos)} videos for {state_name}\")\n",
    "    \n",
    "    if not keyword_videos:\n",
    "        print(f\"No videos found for {state_name}. Skipping analysis.\")\n",
    "        return None\n",
    "    \n",
    "    # Extract video IDs\n",
    "    video_ids = []\n",
    "    for video in keyword_videos:\n",
    "        # Check if the video has the required structure\n",
    "        if '_id' in video and 'kind' in video['_id'] and video['_id']['kind'] == 'youtube#video':\n",
    "            if 'videoId' in video['_id']:\n",
    "                video_ids.append(video['_id']['videoId'])\n",
    "    \n",
    "    # Save video IDs for later use\n",
    "    with open(os.path.join(eda_dir, \"video_ids.txt\"), \"w\") as f:\n",
    "        for video_id in video_ids:\n",
    "            f.write(f\"{video_id}\\n\")\n",
    "    \n",
    "    if not video_ids:\n",
    "        print(f\"No valid video IDs found for {state_name}. Skipping analysis.\")\n",
    "        return None\n",
    "    \n",
    "    # Get comments for all videos\n",
    "    print(f\"Fetching comments for {len(video_ids)} videos...\")\n",
    "    state_comments = get_all_comments(video_ids, YT_API_KEY, max_comments_per_video=400)\n",
    "    \n",
    "    # Create a list of all comments\n",
    "    all_comments_raw = []\n",
    "    for video_id, comments in state_comments.items():\n",
    "        all_comments_raw.extend(comments)\n",
    "    \n",
    "    print(f\"Total comments collected: {len(all_comments_raw)}\")\n",
    "    \n",
    "    # Process the comments for each poverty dimension\n",
    "    results = {\n",
    "        \"state\": [],\n",
    "        \"dimension\": [],\n",
    "        \"word_count\": [],\n",
    "        \"comments_count\": [],\n",
    "        \"avg_sentiment\": []}\n",
    "    \n",
    "    # Initialize dictionary to store comments by dimension\n",
    "    comments_by_dimension = {dim: [] for dim in poverty_dimensions.keys()}\n",
    "    \n",
    "    print(f\"Analyzing comments for each poverty dimension...\")\n",
    "    for dimension, keywords in tqdm(poverty_dimensions.items(), desc=\"Dimensions\"):\n",
    "        dimension_word_count = 0\n",
    "        comments_with_dimension = []\n",
    "        \n",
    "        # Convert keywords to lowercase for matching\n",
    "        keywords_lower = [kw.lower() for kw in keywords]\n",
    "        \n",
    "        # Analyze each comment\n",
    "        for comment in all_comments_raw:\n",
    "            comment_lower = comment.lower()\n",
    "            \n",
    "            # Count the number of keywords in the comment\n",
    "            dimension_keywords_in_comment = 0\n",
    "            for keyword in keywords_lower:\n",
    "                count = len(re.findall(r'\\b' + re.escape(keyword) + r'\\b', comment_lower))\n",
    "                dimension_word_count += count\n",
    "                dimension_keywords_in_comment += count\n",
    "                \n",
    "            # If the comment contains at least 1 keyword, add it to the list\n",
    "            if dimension_keywords_in_comment > 0:\n",
    "                comments_with_dimension.append(comment)\n",
    "                comments_by_dimension[dimension].append(comment)\n",
    "        \n",
    "        # Compute average sentiment for the comments within the dimension\n",
    "        sentiment_scores = []\n",
    "        for comment in comments_with_dimension:\n",
    "            try:\n",
    "                sentiment = get_bert_sentiment(comment, tokenizer, model)\n",
    "                sentiment_scores.append(sentiment)\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing sentiment for dimension {dimension}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        avg_sentiment = np.mean(sentiment_scores) if sentiment_scores else 0.0\n",
    "        \n",
    "        results[\"state\"].append(state_name)\n",
    "        results[\"dimension\"].append(dimension)\n",
    "        results[\"word_count\"].append(dimension_word_count)\n",
    "        results[\"comments_count\"].append(len(comments_with_dimension))\n",
    "        results[\"avg_sentiment\"].append(avg_sentiment)\n",
    "    \n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save the results to CSV\n",
    "    results_df.to_csv(os.path.join(\"yt_data\", f\"{state_name.lower().replace(' ', '_')}_new.csv\"), index=False)\n",
    "    \n",
    "    # Now perform additional EDA\n",
    "    print(\"Performing EDA on collected comments...\")\n",
    "    \n",
    "    # 1. Keyword Distribution by Poverty Dimension\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Sort dimensions by word count\n",
    "    sorted_df = results_df.sort_values('word_count', ascending=False)\n",
    "    plt.bar(sorted_df['dimension'], sorted_df['word_count'], color='skyblue')\n",
    "    plt.title(f'Keyword Frequency by Poverty Dimension in {state_name}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Keyword Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(eda_dir, \"dimension_keyword_frequency.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Comments Count by Dimension\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Sort dimensions by comments count\n",
    "    sorted_df = results_df.sort_values('comments_count', ascending=False)\n",
    "    plt.bar(sorted_df['dimension'], sorted_df['comments_count'], color='lightgreen')\n",
    "    plt.title(f'Number of Comments Mentioning Each Poverty Dimension in {state_name}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Comment Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(eda_dir, \"dimension_comment_count.png\"))\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Sentiment Analysis by Dimension\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Sort dimensions by sentiment\n",
    "    sorted_df = results_df.sort_values('avg_sentiment')\n",
    "    colors = ['red' if s < 0 else 'green' for s in sorted_df['avg_sentiment']]\n",
    "    plt.bar(sorted_df['dimension'], sorted_df['avg_sentiment'], color=colors)\n",
    "    plt.title(f'Average Sentiment by Poverty Dimension in {state_name}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Sentiment Score (-1 to 1)')\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(eda_dir, \"dimension_sentiment.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Word Frequency Analysis by Dimension\n",
    "    dimension_top_words = {}\n",
    "    spanish_stopwords = set(nlp_es.Defaults.stop_words)\n",
    "    english_stopwords = set(nlp_en.Defaults.stop_words)\n",
    "    all_stopwords = spanish_stopwords.union(english_stopwords).union(custom_stopwords)\n",
    "    \n",
    "    for dimension, comments_list in comments_by_dimension.items():\n",
    "        if not comments_list:\n",
    "            dimension_top_words[dimension] = []\n",
    "            continue\n",
    "            \n",
    "        dimension_text = \" \".join(comments_list).lower()\n",
    "        words = re.findall(r'\\b[a-zA-ZáéíóúüñÁÉÍÓÚÜÑ]+\\b', dimension_text)\n",
    "        filtered_words = [word for word in words if word not in all_stopwords and len(word) > 2]\n",
    "        \n",
    "        word_freq = Counter(filtered_words)\n",
    "        dimension_top_words[dimension] = word_freq.most_common(10)\n",
    "    \n",
    "    # Save word frequency by dimension to a file\n",
    "    with open(os.path.join(eda_dir, \"dimension_word_freq.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for dimension, word_freqs in dimension_top_words.items():\n",
    "            f.write(f\"Top 10 words for {dimension}:\\n\")\n",
    "            for word, freq in word_freqs:\n",
    "                f.write(f\"  {word}: {freq}\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"EDA completed for {state_name}. Results saved to {eda_dir}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed9d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    \n",
    "    # Initialize the YouTube client\n",
    "    yt_client = YouTubeClient(api_key=YT_API_KEY)\n",
    "    \n",
    "    # Process each state\n",
    "    for state in STATES:\n",
    "        result_df = analyze_state(\n",
    "            state_name=state,\n",
    "            published_after=PUBLISHED_AFTER,\n",
    "            published_before=PUBLISHED_BEFORE,\n",
    "            yt_client=yt_client,\n",
    "            tokenizer=tokenizer,\n",
    "            model=model)\n",
    "        \n",
    "        if result_df is not None:\n",
    "            # Save individual state results\n",
    "            output_path = os.path.join(\"yt_data\", f\"{state.lower()}_new.csv\")\n",
    "            result_df.to_csv(output_path, index=False)\n",
    "            print(f\"Saved results for {state} to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
