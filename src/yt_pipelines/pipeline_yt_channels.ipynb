{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6084b34e",
   "metadata": {},
   "source": [
    "## 1) sentiment conditional on the dimension - define dimensions just in a descrptive/neutral way \n",
    "Define the 7 dimensions of poverty using neutral words - for instance for income use words such as employment, work, income, money, salary, financial stability, opportunities - and then do the word embedding for them to find comments that talk about the specific dimension. Once all comments are categorized into the corresponding dimension, compute the average sentiment per dimension. \n",
    "\n",
    "In this way we avoid any bias as we are just categorizing by dimension of poverty and then computing the sentiment score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf111d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "YT_API_KEY = os.getenv(\"YT_API_KEY\")\n",
    "\n",
    "# Define states and channel names to search for\n",
    "STATES_CHANNELS_NAMES = {\n",
    "    \"Guanajuato\": [\n",
    "        {\"name\": \"TV4 Guanajuato\"},\n",
    "        {\"name\": \"Periódico Correo\"},\n",
    "        {\"name\": \"Gobierno de Guanajuato\"}\n",
    "    ],\n",
    "    \"Michoacán\": [\n",
    "        {\"name\": \"CB Televisión\"},\n",
    "        {\"name\": \"Noticias Michoacán\"},\n",
    "        {\"name\": \"Gobierno de Michoacán\"}\n",
    "    ],\n",
    "    \"Sinaloa\": [\n",
    "        {\"name\": \"Noticiero Altavoz\"},\n",
    "        {\"name\": \"TVP Culiacán\"},\n",
    "        {\"name\": \"Gobierno de Sinaloa\"}\n",
    "    ],\n",
    "    \"Chihuahua\": [\n",
    "        {\"name\": \"Canal 28 Chihuahua\"},\n",
    "        {\"name\": \"Noticias de Chihuahua\"},\n",
    "        {\"name\": \"Gobierno de Chihuahua\"}\n",
    "    ],\n",
    "    \"Guerrero\": [\n",
    "        {\"name\": \"Noticiero Acapulco\"},\n",
    "        {\"name\": \"Televisa Acapulco\"},\n",
    "        {\"name\": \"Gobierno de Guerrero\"}\n",
    "    ],\n",
    "    \"Tamaulipas\": [\n",
    "        {\"name\": \"Noticias Tamaulipas\"},\n",
    "        {\"name\": \"Televisa Tamaulipas\"},\n",
    "        {\"name\": \"Gobierno de Tamaulipas\"}\n",
    "    ],\n",
    "    \"Baja California\": [\n",
    "        {\"name\": \"Síntesis TV\"},\n",
    "        {\"name\": \"PSN Televisión\"},\n",
    "        {\"name\": \"Gobierno de Baja California\"}\n",
    "    ],\n",
    "    \"Zacatecas\": [\n",
    "        {\"name\": \"NTR Zacatecas\"},\n",
    "        {\"name\": \"Zacatecas Online\"},\n",
    "        {\"name\": \"Gobierno de Zacatecas\"}\n",
    "    ],\n",
    "    \"Colima\": [\n",
    "        {\"name\": \"AF Medios\"},\n",
    "        {\"name\": \"Colima Noticias\"},\n",
    "        {\"name\": \"Gobierno de Colima\"}\n",
    "    ],\n",
    "    \"Jalisco\": [\n",
    "        {\"name\": \"Canal 44\"},\n",
    "        {\"name\": \"Televisa Guadalajara\"},\n",
    "        {\"name\": \"Gobierno de Jalisco\"}]}\n",
    "\n",
    "# Neutral keyword-based descriptions for poverty dimensions\n",
    "POVERTY_DIMENSIONS = {\n",
    "    \"INCOME\": \"empleo trabajo ingreso dinero salario estabilidad ocupación oportunidades\",\n",
    "    \"ACCESS TO HEALTH SERVICES\": \"salud hospital médico medicina tratamiento atención clínica seguro\",\n",
    "    \"EDUCATIONAL LAG\": \"educación escuela maestro estudiante aprendizaje clases universidad formación\",\n",
    "    \"ACCESS TO SOCIAL SECURITY\": \"seguridad social pensión jubilación contrato derechos prestaciones protección laboral\",\n",
    "    \"HOUSING\": \"vivienda casa habitación servicios básicos infraestructura hogar alquiler agua luz\",\n",
    "    \"ACCESS TO FOOD\": \"alimentación comida nutrición alimentos dieta mercado hambre acceso\",\n",
    "    \"SOCIAL COHESION\": \"comunidad inclusión integración participación convivencia respeto diversidad solidaridad\"\n",
    "}\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.dimension_names = list(POVERTY_DIMENSIONS.keys())\n",
    "        self.dimension_embeddings = self.embedder.encode(list(POVERTY_DIMENSIONS.values()), convert_to_tensor=True)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]', ' ', text)\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def classify_dimension(self, text):\n",
    "        if not text:\n",
    "            return None, 0.0\n",
    "        embedding = self.embedder.encode(text, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embedding, self.dimension_embeddings)[0]\n",
    "        max_idx = torch.argmax(cosine_scores).item()\n",
    "        return self.dimension_names[max_idx], cosine_scores[max_idx].item()\n",
    "\n",
    "    def get_sentiment_score(self, text):\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        stars = torch.argmax(outputs.logits, dim=1).item() + 1\n",
    "        return (stars - 3) / 2  # Normalize to [-1, 1]\n",
    "\n",
    "class YouTubeAnalyzer:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        self.processor = TextProcessor()\n",
    "\n",
    "    def get_channel_id_by_name(self, name, state):\n",
    "        query = f\"{name} {state}\"\n",
    "        response = self.youtube.search().list(q=query, part=\"id\", maxResults=1, type=\"channel\").execute()\n",
    "        if response['items']:\n",
    "            return response['items'][0]['id']['channelId']\n",
    "        return None\n",
    "\n",
    "    def get_channel_videos(self, channel_id, published_after, published_before):\n",
    "        videos = []\n",
    "        uploads_id = self.youtube.channels().list(part=\"contentDetails\", id=channel_id).execute()['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "        next_page_token = None\n",
    "        while True:\n",
    "            response = self.youtube.playlistItems().list(\n",
    "                playlistId=uploads_id, part=\"snippet\", maxResults=50, pageToken=next_page_token\n",
    "            ).execute()\n",
    "            for item in response['items']:\n",
    "                published = item['snippet']['publishedAt']\n",
    "                if published_after <= published <= published_before:\n",
    "                    videos.append({\n",
    "                        \"id\": item['snippet']['resourceId']['videoId'],\n",
    "                        \"title\": item['snippet']['title'],\n",
    "                        \"description\": item['snippet'].get('description', '')\n",
    "                    })\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "            sleep(0.5)\n",
    "        return videos\n",
    "\n",
    "    def get_video_comments(self, video_id):\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "        while True:\n",
    "            try:\n",
    "                response = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\", videoId=video_id, maxResults=100, pageToken=next_page_token\n",
    "                ).execute()\n",
    "                for item in response.get(\"items\", []):\n",
    "                    comments.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token:\n",
    "                    break\n",
    "                sleep(0.5)\n",
    "            except Exception:\n",
    "                break\n",
    "        return comments\n",
    "\n",
    "    def analyze_state(self, state_name, channel_infos, date_range):\n",
    "        print(f\"Analyzing {state_name}...\")\n",
    "        dimension_stats = {dim: {\"sentiment_sum\": 0.0, \"count\": 0} for dim in POVERTY_DIMENSIONS}\n",
    "        for channel in channel_infos:\n",
    "            channel_id = self.get_channel_id_by_name(channel[\"name\"], state_name)\n",
    "            if not channel_id:\n",
    "                continue\n",
    "            videos = self.get_channel_videos(channel_id, date_range[\"published_after\"], date_range[\"published_before\"])\n",
    "            for video in videos:\n",
    "                all_texts = [video[\"title\"] + \". \" + video[\"description\"]] + self.get_video_comments(video[\"id\"])\n",
    "                for text in all_texts:\n",
    "                    clean = self.processor.clean_text(text)\n",
    "                    dimension, confidence = self.processor.classify_dimension(clean)\n",
    "                    if confidence > 0.1:\n",
    "                        sentiment = self.processor.get_sentiment_score(clean)\n",
    "                        dimension_stats[dimension][\"sentiment_sum\"] += sentiment\n",
    "                        dimension_stats[dimension][\"count\"] += 1\n",
    "        return dimension_stats\n",
    "\n",
    "def analyze_all_states():\n",
    "    analyzer = YouTubeAnalyzer(YT_API_KEY)\n",
    "    date_range = {\n",
    "        \"published_after\": \"2022-01-01T00:00:00Z\",\n",
    "        \"published_before\": \"2022-12-31T23:59:59Z\"\n",
    "    }\n",
    "    os.makedirs(\"yt_channels_sentiment\", exist_ok=True)\n",
    "    for state, channels in STATES_CHANNELS_NAMES.items():\n",
    "        stats = analyzer.analyze_state(state, channels, date_range)\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                \"dimension\": dim.replace(\"_\", \" \").title(),\n",
    "                \"avg_sentiment\": v[\"sentiment_sum\"] / v[\"count\"] if v[\"count\"] else 0\n",
    "            }\n",
    "            for dim, v in stats.items()\n",
    "        ])\n",
    "        df.to_csv(f\"yt_channels_sentiment/{state.replace(' ', '_').lower()}.csv\", index=False)\n",
    "        print(f\"Saved yt_channels_sentiment/{state.replace(' ', '_').lower()}.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_all_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b26e5",
   "metadata": {},
   "source": [
    "## 2) only count of comments related to each dimension of poverty to avoid any bias \n",
    "\n",
    "Here we are categorizing by negative attributes related to each poverty dimension - for instance for income we are now using unemployment, economic crisis, low salary, unstable jobs - and then we are just counting the occurences of 'negative' words per dimension.\n",
    "\n",
    "We avoid the bias since we don't do the sentiment analysis - which would lean towards negative scores as we are filtering for negative things in the first place - but we just count how much each dimension of poverty is discussed. We could assume that, the more a dimension of poverty is discussed, the higher that 'type' of poverty is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac23be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "load_dotenv()\n",
    "YT_API_KEY = os.getenv(\"YT_API_KEY\")\n",
    "\n",
    "# Expanded poverty dimensions\n",
    "POVERTY_DIMENSIONS = {\n",
    "    \"INCOME\": \"Desempleo, salario bajo, crisis económica, sin ingresos suficientes, trabajos temporales, vivir al día, situación precaria, inflación, deuda, sueldo de hambre, precariedad laboral, no alcanza, buscar trabajo, sin chamba.\",\n",
    "    \"ACCESS TO HEALTH SERVICES\": \"Sin medicinas, hospital lejano, largas esperas, sin seguro médico, mala atención, falta de doctores, centros de salud cerrados, salud pública colapsada, servicios de urgencia deficientes, tratamientos caros, automedicación.\",\n",
    "    \"EDUCATIONAL LAG\": \"Rezago escolar, analfabetismo, sin maestros, abandono escolar, escuelas en mal estado, falta de útiles, deserción, educación de baja calidad, falta de acceso educativo, desigualdad educativa, jóvenes sin estudiar.\",\n",
    "    \"ACCESS TO SOCIAL SECURITY\": \"Trabajo informal, sin contrato, sin prestaciones, sin IMSS, falta de protección laboral, empleo sin derechos, sin jubilación, condiciones precarias, trabajadores explotados, empleo sin seguridad social.\",\n",
    "    \"HOUSING\": \"Vivienda precaria, sin agua o luz, hacinamiento, casa insegura, techos de lámina, casas de cartón, renta cara, falta de drenaje, zonas de riesgo, sin baño, construcciones vulnerables, viviendas abandonadas.\",\n",
    "    \"ACCESS TO FOOD\": \"Inseguridad alimentaria, hambre, comida escasa, sin alimentos básicos, malnutrición, dieta pobre, precios altos, ni para frijoles, dependencia alimentaria, comer una vez al día, alimentos inaccesibles.\",\n",
    "    \"SOCIAL COHESION\": \"Fragmentación social, discriminación, exclusión, desigualdad, tensiones comunitarias, racismo, violencia entre barrios, marginación, falta de integración, odio de clase, polarización social.\"\n",
    "}\n",
    "\n",
    "STATES_CHANNELS_NAMES = {\n",
    "    \"Guanajuato\": [\n",
    "        {\"name\": \"TV4 Guanajuato\"},\n",
    "        {\"name\": \"Periódico Correo\"},\n",
    "        {\"name\": \"Gobierno de Guanajuato\"}\n",
    "    ],\n",
    "    \"Michoacán\": [\n",
    "        {\"name\": \"CB Televisión\"},\n",
    "        {\"name\": \"Noticias Michoacán\"},\n",
    "        {\"name\": \"Gobierno de Michoacán\"}\n",
    "    ],\n",
    "    \"Sinaloa\": [\n",
    "        {\"name\": \"Noticiero Altavoz\"},\n",
    "        {\"name\": \"TVP Culiacán\"},\n",
    "        {\"name\": \"Gobierno de Sinaloa\"}\n",
    "    ],\n",
    "    \"Chihuahua\": [\n",
    "        {\"name\": \"Canal 28 Chihuahua\"},\n",
    "        {\"name\": \"Noticias de Chihuahua\"},\n",
    "        {\"name\": \"Gobierno de Chihuahua\"}\n",
    "    ],\n",
    "    \"Guerrero\": [\n",
    "        {\"name\": \"Noticiero Acapulco\"},\n",
    "        {\"name\": \"Televisa Acapulco\"},\n",
    "        {\"name\": \"Gobierno de Guerrero\"}\n",
    "    ],\n",
    "    \"Tamaulipas\": [\n",
    "        {\"name\": \"Noticias Tamaulipas\"},\n",
    "        {\"name\": \"Televisa Tamaulipas\"},\n",
    "        {\"name\": \"Gobierno de Tamaulipas\"}\n",
    "    ],\n",
    "    \"Baja California\": [\n",
    "        {\"name\": \"Síntesis TV\"},\n",
    "        {\"name\": \"PSN Televisión\"},\n",
    "        {\"name\": \"Gobierno de Baja California\"}\n",
    "    ],\n",
    "    \"Zacatecas\": [\n",
    "        {\"name\": \"NTR Zacatecas\"},\n",
    "        {\"name\": \"Zacatecas Online\"},\n",
    "        {\"name\": \"Gobierno de Zacatecas\"}\n",
    "    ],\n",
    "    \"Colima\": [\n",
    "        {\"name\": \"AF Medios\"},\n",
    "        {\"name\": \"Colima Noticias\"},\n",
    "        {\"name\": \"Gobierno de Colima\"}\n",
    "    ],\n",
    "    \"Jalisco\": [\n",
    "        {\"name\": \"Canal 44\"},\n",
    "        {\"name\": \"Televisa Guadalajara\"},\n",
    "        {\"name\": \"Gobierno de Jalisco\"}]}\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        self.dimensions = list(POVERTY_DIMENSIONS.keys())\n",
    "        self.embeddings = self.embedder.encode(list(POVERTY_DIMENSIONS.values()), convert_to_tensor=True)\n",
    "\n",
    "    def clean(self, text):\n",
    "        text = re.sub(r\"<.*?>\", \" \", text)\n",
    "        text = re.sub(r\"http\\\\S+\", \"\", text)\n",
    "        text = re.sub(r\"[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]\", \" \", text)\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
    "\n",
    "    def classify(self, text):\n",
    "        if not text:\n",
    "            return None, 0.0\n",
    "        emb = self.embedder.encode(text, convert_to_tensor=True)\n",
    "        scores = util.cos_sim(emb, self.embeddings)[0]\n",
    "        best_idx = torch.argmax(scores).item()\n",
    "        return self.dimensions[best_idx], scores[best_idx].item()\n",
    "\n",
    "class YouTubeAnalyzer:\n",
    "    def __init__(self, key):\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=key)\n",
    "        self.processor = TextProcessor()\n",
    "\n",
    "    def get_channel_id(self, name, state):\n",
    "        q = f\"{name} {state}\"\n",
    "        res = self.youtube.search().list(q=q, part=\"id\", maxResults=1, type=\"channel\").execute()\n",
    "        return res[\"items\"][0][\"id\"][\"channelId\"] if res[\"items\"] else None\n",
    "\n",
    "    def get_videos(self, channel_id, after, before):\n",
    "        vids = []\n",
    "        uploads_id = self.youtube.channels().list(part=\"contentDetails\", id=channel_id).execute()[\"items\"][0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "        token = None\n",
    "        while True:\n",
    "            res = self.youtube.playlistItems().list(playlistId=uploads_id, part=\"snippet\", maxResults=50, pageToken=token).execute()\n",
    "            for item in res[\"items\"]:\n",
    "                pub = item[\"snippet\"][\"publishedAt\"]\n",
    "                if after <= pub <= before:\n",
    "                    vids.append({\n",
    "                        \"id\": item[\"snippet\"][\"resourceId\"][\"videoId\"],\n",
    "                        \"title\": item[\"snippet\"][\"title\"],\n",
    "                        \"description\": item[\"snippet\"].get(\"description\", \"\")\n",
    "                    })\n",
    "            token = res.get(\"nextPageToken\")\n",
    "            if not token:\n",
    "                break\n",
    "            sleep(0.5)\n",
    "        return vids\n",
    "\n",
    "    def get_comments(self, video_id):\n",
    "        coms = []\n",
    "        token = None\n",
    "        while True:\n",
    "            try:\n",
    "                res = self.youtube.commentThreads().list(videoId=video_id, part=\"snippet\", maxResults=100, pageToken=token).execute()\n",
    "                for item in res.get(\"items\", []):\n",
    "                    coms.append(item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"])\n",
    "                token = res.get(\"nextPageToken\")\n",
    "                if not token:\n",
    "                    break\n",
    "                sleep(0.5)\n",
    "            except Exception:\n",
    "                break\n",
    "        return coms\n",
    "\n",
    "    def analyze(self, state, channels, drange):\n",
    "        print(f\"\\nAnalyzing {state}...\")\n",
    "        counts = {d: 0 for d in POVERTY_DIMENSIONS}\n",
    "        for ch in channels:\n",
    "            cid = self.get_channel_id(ch[\"name\"], state)\n",
    "            if not cid:\n",
    "                continue\n",
    "            videos = self.get_videos(cid, drange[\"after\"], drange[\"before\"])\n",
    "            for v in videos:\n",
    "                texts = [v[\"title\"] + \". \" + v[\"description\"]] + self.get_comments(v[\"id\"])\n",
    "                for t in texts:\n",
    "                    dim, conf = self.processor.classify(self.processor.clean(t))\n",
    "                    if conf > 0.1:\n",
    "                        counts[dim] += 1\n",
    "        return counts\n",
    "\n",
    "def run_analysis():\n",
    "    yt = YouTubeAnalyzer(YT_API_KEY)\n",
    "    drange = {\"after\": \"2022-01-01T00:00:00Z\", \"before\": \"2022-12-31T23:59:59Z\"}\n",
    "    os.makedirs(\"yt_channels\", exist_ok=True)\n",
    "    for state, chs in STATES_CHANNELS_NAMES.items():\n",
    "        results = yt.analyze(state, chs, drange)\n",
    "        df = pd.DataFrame([{\"dimension\": k, \"comment_count\": v} for k, v in results.items()])\n",
    "        df.to_csv(f\"yt_channels/{state.lower().replace(' ', '_')}.csv\", index=False)\n",
    "        print(f\"Saved yt_channels/{state.lower().replace(' ', '_')}.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abff550f",
   "metadata": {},
   "source": [
    "## 3) standard approach of filtering for negative words + sentiment score + counts of words \n",
    "This could potentially lead to bias results, although I compared results of this approach with results from approach 1 and they are more or less aligned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "YT_API_KEY = os.getenv(\"YT_API_KEY\")\n",
    "\n",
    "# Define states and channel names to search for\n",
    "STATES_CHANNELS_NAMES = {\n",
    "    \"Guanajuato\": [\n",
    "        {\"name\": \"TV4 Guanajuato\"},\n",
    "        {\"name\": \"Periódico Correo\"},\n",
    "        {\"name\": \"Gobierno de Guanajuato\"}\n",
    "    ],\n",
    "    \"Michoacán\": [\n",
    "        {\"name\": \"CB Televisión\"},\n",
    "        {\"name\": \"Noticias Michoacán\"},\n",
    "        {\"name\": \"Gobierno de Michoacán\"}\n",
    "    ],\n",
    "    \"Sinaloa\": [\n",
    "        {\"name\": \"Noticiero Altavoz\"},\n",
    "        {\"name\": \"TVP Culiacán\"},\n",
    "        {\"name\": \"Gobierno de Sinaloa\"}\n",
    "    ],\n",
    "    \"Chihuahua\": [\n",
    "        {\"name\": \"Canal 28 Chihuahua\"},\n",
    "        {\"name\": \"Noticias de Chihuahua\"},\n",
    "        {\"name\": \"Gobierno de Chihuahua\"}\n",
    "    ],\n",
    "    \"Guerrero\": [\n",
    "        {\"name\": \"Noticiero Acapulco\"},\n",
    "        {\"name\": \"Televisa Acapulco\"},\n",
    "        {\"name\": \"Gobierno de Guerrero\"}\n",
    "    ],\n",
    "    \"Tamaulipas\": [\n",
    "        {\"name\": \"Noticias Tamaulipas\"},\n",
    "        {\"name\": \"Televisa Tamaulipas\"},\n",
    "        {\"name\": \"Gobierno de Tamaulipas\"}\n",
    "    ],\n",
    "    \"Baja California\": [\n",
    "        {\"name\": \"Síntesis TV\"},\n",
    "        {\"name\": \"PSN Televisión\"},\n",
    "        {\"name\": \"Gobierno de Baja California\"}\n",
    "    ],\n",
    "    \"Zacatecas\": [\n",
    "        {\"name\": \"NTR Zacatecas\"},\n",
    "        {\"name\": \"Zacatecas Online\"},\n",
    "        {\"name\": \"Gobierno de Zacatecas\"}\n",
    "    ],\n",
    "    \"Colima\": [\n",
    "        {\"name\": \"AF Medios\"},\n",
    "        {\"name\": \"Colima Noticias\"},\n",
    "        {\"name\": \"Gobierno de Colima\"}\n",
    "    ],\n",
    "    \"Jalisco\": [\n",
    "        {\"name\": \"Canal 44\"},\n",
    "        {\"name\": \"Televisa Guadalajara\"},\n",
    "        {\"name\": \"Gobierno de Jalisco\"}]}\n",
    "\n",
    "# Spanish descriptions of poverty dimensions\n",
    "POVERTY_DIMENSIONS = {\n",
    "    \"INGRESOS\": \"Empleo, salarios, estabilidad financiera, desigualdad de ingresos, oportunidades económicas, seguridad laboral.\",\n",
    "    \"ACCESO A SALUD\": \"Acceso a servicios de salud, calidad médica, medicamentos, seguros, infraestructura hospitalaria.\",\n",
    "    \"REZAGO EDUCATIVO\": \"Acceso a la educación, calidad escolar, alfabetización, abandono escolar, recursos educativos.\",\n",
    "    \"SEGURIDAD SOCIAL\": \"Derechos laborales, protección social, empleo informal, prestaciones, jubilación, seguridad en el empleo.\",\n",
    "    \"VIVIENDA\": \"Calidad de vivienda, acceso a servicios, hacinamiento, asequibilidad, condiciones, desalojos, instalaciones.\",\n",
    "    \"ALIMENTACIÓN\": \"Seguridad alimentaria, hambre, asequibilidad de alimentos, calidad alimentaria, malnutrición, disponibilidad, asistencia.\",\n",
    "    \"COHESIÓN SOCIAL\": \"Integración social, exclusión, marginación, discriminación, confianza, apoyo comunitario.\"\n",
    "}\n",
    "\n",
    "SPANISH_STOPWORDS = [\"de\", \"la\", \"que\", \"el\", \"en\"]\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        self.dimension_names = list(POVERTY_DIMENSIONS.keys())\n",
    "        self.dimension_embeddings = self.embedder.encode(list(POVERTY_DIMENSIONS.values()), convert_to_tensor=True)\n",
    "        self.sentiment_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(self.sentiment_model_name)\n",
    "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(self.sentiment_model_name)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]', ' ', text)\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def get_sentiment_score(self, text):\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        inputs = self.sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.sentiment_model(**inputs)\n",
    "        stars = torch.argmax(outputs.logits, dim=1).item() + 1\n",
    "        return (stars - 3) / 2\n",
    "\n",
    "    def classify_dimension(self, text):\n",
    "        if not text:\n",
    "            return None, 0.0\n",
    "        embedding = self.embedder.encode(text, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embedding, self.dimension_embeddings)[0]\n",
    "        max_idx = torch.argmax(cosine_scores).item()\n",
    "        return self.dimension_names[max_idx], cosine_scores[max_idx].item()\n",
    "\n",
    "class YouTubeAnalyzer:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        self.processor = TextProcessor()\n",
    "\n",
    "    def get_channel_id_by_name(self, name, state):\n",
    "        query = f\"{name} {state}\"\n",
    "        response = self.youtube.search().list(q=query, part=\"id\", maxResults=1, type=\"channel\").execute()\n",
    "        if response['items']:\n",
    "            return response['items'][0]['id']['channelId']\n",
    "        return None\n",
    "\n",
    "    def get_channel_videos(self, channel_id, published_after, published_before):\n",
    "        videos = []\n",
    "        uploads_id = self.youtube.channels().list(part=\"contentDetails\", id=channel_id).execute()['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "        next_page_token = None\n",
    "        while True:\n",
    "            response = self.youtube.playlistItems().list(\n",
    "                playlistId=uploads_id, part=\"snippet\", maxResults=50, pageToken=next_page_token\n",
    "            ).execute()\n",
    "            for item in response['items']:\n",
    "                published = item['snippet']['publishedAt']\n",
    "                if published_after <= published <= published_before:\n",
    "                    videos.append({\n",
    "                        \"id\": item['snippet']['resourceId']['videoId'],\n",
    "                        \"title\": item['snippet']['title'],\n",
    "                        \"description\": item['snippet'].get('description', '')\n",
    "                    })\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "            sleep(0.5)\n",
    "        return videos\n",
    "\n",
    "    def get_video_comments(self, video_id):\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "        while True:\n",
    "            try:\n",
    "                response = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\", videoId=video_id, maxResults=100, pageToken=next_page_token\n",
    "                ).execute()\n",
    "                for item in response.get(\"items\", []):\n",
    "                    comments.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token:\n",
    "                    break\n",
    "                sleep(0.5)\n",
    "            except Exception:\n",
    "                break\n",
    "        return comments\n",
    "\n",
    "    def analyze_state(self, state_name, channel_infos, date_range):\n",
    "        print(f\"Analyzing {state_name}...\")\n",
    "        dimension_stats = {dim: {\"count\": 0, \"sentiment_sum\": 0.0} for dim in POVERTY_DIMENSIONS}\n",
    "        for channel in channel_infos:\n",
    "            channel_id = self.get_channel_id_by_name(channel[\"name\"], state_name)\n",
    "            if not channel_id:\n",
    "                continue\n",
    "            videos = self.get_channel_videos(channel_id, date_range[\"published_after\"], date_range[\"published_before\"])\n",
    "            for video in videos:\n",
    "                all_texts = [video[\"title\"] + \". \" + video[\"description\"]] + self.get_video_comments(video[\"id\"])\n",
    "                for text in all_texts:\n",
    "                    clean = self.processor.clean_text(text)\n",
    "                    sentiment = self.processor.get_sentiment_score(clean)\n",
    "                    dimension, confidence = self.processor.classify_dimension(clean)\n",
    "                    if confidence > 0.1:\n",
    "                        dimension_stats[dimension][\"count\"] += 1\n",
    "                        dimension_stats[dimension][\"sentiment_sum\"] += sentiment\n",
    "        return dimension_stats\n",
    "\n",
    "\n",
    "def analyze_all_states():\n",
    "    analyzer = YouTubeAnalyzer(YT_API_KEY)\n",
    "    date_range = {\n",
    "        \"published_after\": \"2022-01-01T00:00:00Z\",\n",
    "        \"published_before\": \"2022-12-31T23:59:59Z\"\n",
    "    }\n",
    "    os.makedirs(\"yt_channels\", exist_ok=True)\n",
    "    for state, channels in STATES_CHANNELS_NAMES.items():\n",
    "        stats = analyzer.analyze_state(state, channels, date_range)\n",
    "        df = pd.DataFrame([{ \"dimension\": dim, \"comment_count\": v[\"count\"], \"avg_sentiment\": v[\"sentiment_sum\"]/v[\"count\"] if v[\"count\"] else 0 } for dim, v in stats.items()])\n",
    "        df.to_csv(f\"yt_channels/{state.replace(' ', '_').lower()}.csv\", index=False)\n",
    "        print(f\"Saved yt_channels/{state.replace(' ', '_').lower()}.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_all_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9e41e2",
   "metadata": {},
   "source": [
    "## 4) same as above but with just more words to define the embedding \n",
    "Technically the embedding, since takes the context, should be able to generalize and so shouldn't be necessary to give too many words. But still this might improve the generalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8416bd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "YT_API_KEY = os.getenv(\"YT_API_KEY\")\n",
    "\n",
    "# Define states and channel names to search for\n",
    "STATES_CHANNELS_NAMES = {\n",
    "    \"Guanajuato\": [\n",
    "        {\"name\": \"TV4 Guanajuato\"},\n",
    "        {\"name\": \"Periódico Correo\"},\n",
    "        {\"name\": \"Gobierno de Guanajuato\"}\n",
    "    ],\n",
    "    \"Michoacán\": [\n",
    "        {\"name\": \"CB Televisión\"},\n",
    "        {\"name\": \"Noticias Michoacán\"},\n",
    "        {\"name\": \"Gobierno de Michoacán\"}\n",
    "    ],\n",
    "    \"Sinaloa\": [\n",
    "        {\"name\": \"Noticiero Altavoz\"},\n",
    "        {\"name\": \"TVP Culiacán\"},\n",
    "        {\"name\": \"Gobierno de Sinaloa\"}\n",
    "    ],\n",
    "    \"Chihuahua\": [\n",
    "        {\"name\": \"Canal 28 Chihuahua\"},\n",
    "        {\"name\": \"Noticias de Chihuahua\"},\n",
    "        {\"name\": \"Gobierno de Chihuahua\"}\n",
    "    ],\n",
    "    \"Guerrero\": [\n",
    "        {\"name\": \"Noticiero Acapulco\"},\n",
    "        {\"name\": \"Televisa Acapulco\"},\n",
    "        {\"name\": \"Gobierno de Guerrero\"}\n",
    "    ],\n",
    "    \"Tamaulipas\": [\n",
    "        {\"name\": \"Noticias Tamaulipas\"},\n",
    "        {\"name\": \"Televisa Tamaulipas\"},\n",
    "        {\"name\": \"Gobierno de Tamaulipas\"}\n",
    "    ],\n",
    "    \"Baja California\": [\n",
    "        {\"name\": \"Síntesis TV\"},\n",
    "        {\"name\": \"PSN Televisión\"},\n",
    "        {\"name\": \"Gobierno de Baja California\"}\n",
    "    ],\n",
    "    \"Zacatecas\": [\n",
    "        {\"name\": \"NTR Zacatecas\"},\n",
    "        {\"name\": \"Zacatecas Online\"},\n",
    "        {\"name\": \"Gobierno de Zacatecas\"}\n",
    "    ],\n",
    "    \"Colima\": [\n",
    "        {\"name\": \"AF Medios\"},\n",
    "        {\"name\": \"Colima Noticias\"},\n",
    "        {\"name\": \"Gobierno de Colima\"}\n",
    "    ],\n",
    "    \"Jalisco\": [\n",
    "        {\"name\": \"Canal 44\"},\n",
    "        {\"name\": \"Televisa Guadalajara\"},\n",
    "        {\"name\": \"Gobierno de Jalisco\"}]}\n",
    "\n",
    "# Expanded poverty dimensions \n",
    "POVERTY_DIMENSIONS = {\n",
    "    \"INCOME\": \"Desempleo, salario bajo, crisis económica, sin ingresos suficientes, trabajos temporales, vivir al día, situación precaria, inflación, deuda, sueldo de hambre, precariedad laboral, no alcanza, buscar trabajo, sin chamba.\",\n",
    "    \"ACCESS TO HEALTH SERVICES\": \"Sin medicinas, hospital lejano, largas esperas, sin seguro médico, mala atención, falta de doctores, centros de salud cerrados, salud pública colapsada, servicios de urgencia deficientes, tratamientos caros, automedicación.\",\n",
    "    \"EDUCATIONAL LAG\": \"Rezago escolar, analfabetismo, sin maestros, abandono escolar, escuelas en mal estado, falta de útiles, deserción, educación de baja calidad, falta de acceso educativo, desigualdad educativa, jóvenes sin estudiar.\",\n",
    "    \"ACCESS TO SOCIAL SECURITY\": \"Trabajo informal, sin contrato, sin prestaciones, sin IMSS, falta de protección laboral, empleo sin derechos, sin jubilación, condiciones precarias, trabajadores explotados, empleo sin seguridad social.\",\n",
    "    \"HOUSING\": \"Vivienda precaria, sin agua o luz, hacinamiento, casa insegura, techos de lámina, casas de cartón, renta cara, falta de drenaje, zonas de riesgo, sin baño, construcciones vulnerables, viviendas abandonadas.\",\n",
    "    \"ACCESS TO FOOD\": \"Inseguridad alimentaria, hambre, comida escasa, sin alimentos básicos, malnutrición, dieta pobre, precios altos, ni para frijoles, dependencia alimentaria, comer una vez al día, alimentos inaccesibles.\",\n",
    "    \"SOCIAL COHESION\": \"Fragmentación social, discriminación, exclusión, desigualdad, tensiones comunitarias, racismo, violencia entre barrios, marginación, falta de integración, odio de clase, polarización social.\"\n",
    "}\n",
    "\n",
    "SPANISH_STOPWORDS = [\"de\", \"la\", \"que\", \"el\", \"en\"]\n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        self.dimension_names = list(POVERTY_DIMENSIONS.keys())\n",
    "        self.dimension_embeddings = self.embedder.encode(list(POVERTY_DIMENSIONS.values()), convert_to_tensor=True)\n",
    "        self.sentiment_model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "        self.sentiment_tokenizer = AutoTokenizer.from_pretrained(self.sentiment_model_name)\n",
    "        self.sentiment_model = AutoModelForSequenceClassification.from_pretrained(self.sentiment_model_name)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]', ' ', text)\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def get_sentiment_score(self, text):\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        inputs = self.sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.sentiment_model(**inputs)\n",
    "        stars = torch.argmax(outputs.logits, dim=1).item() + 1\n",
    "        return (stars - 3) / 2\n",
    "\n",
    "    def classify_dimension(self, text):\n",
    "        if not text:\n",
    "            return None, 0.0\n",
    "        embedding = self.embedder.encode(text, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embedding, self.dimension_embeddings)[0]\n",
    "        max_idx = torch.argmax(cosine_scores).item()\n",
    "        return self.dimension_names[max_idx], cosine_scores[max_idx].item()\n",
    "\n",
    "class YouTubeAnalyzer:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        self.processor = TextProcessor()\n",
    "\n",
    "    def get_channel_id_by_name(self, name, state):\n",
    "        query = f\"{name} {state}\"\n",
    "        response = self.youtube.search().list(q=query, part=\"id\", maxResults=1, type=\"channel\").execute()\n",
    "        if response['items']:\n",
    "            return response['items'][0]['id']['channelId']\n",
    "        return None\n",
    "\n",
    "    def get_channel_videos(self, channel_id, published_after, published_before):\n",
    "        videos = []\n",
    "        uploads_id = self.youtube.channels().list(part=\"contentDetails\", id=channel_id).execute()['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "        next_page_token = None\n",
    "        while True:\n",
    "            response = self.youtube.playlistItems().list(\n",
    "                playlistId=uploads_id, part=\"snippet\", maxResults=50, pageToken=next_page_token\n",
    "            ).execute()\n",
    "            for item in response['items']:\n",
    "                published = item['snippet']['publishedAt']\n",
    "                if published_after <= published <= published_before:\n",
    "                    videos.append({\n",
    "                        \"id\": item['snippet']['resourceId']['videoId'],\n",
    "                        \"title\": item['snippet']['title'],\n",
    "                        \"description\": item['snippet'].get('description', '')\n",
    "                    })\n",
    "            next_page_token = response.get('nextPageToken')\n",
    "            if not next_page_token:\n",
    "                break\n",
    "            sleep(0.5)\n",
    "        return videos\n",
    "\n",
    "    def get_video_comments(self, video_id):\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "        while True:\n",
    "            try:\n",
    "                response = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\", videoId=video_id, maxResults=100, pageToken=next_page_token\n",
    "                ).execute()\n",
    "                for item in response.get(\"items\", []):\n",
    "                    comments.append(item['snippet']['topLevelComment']['snippet']['textDisplay'])\n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token:\n",
    "                    break\n",
    "                sleep(0.5)\n",
    "            except Exception:\n",
    "                break\n",
    "        return comments\n",
    "\n",
    "    def analyze_state(self, state_name, channel_infos, date_range):\n",
    "        print(f\"Analyzing {state_name}...\")\n",
    "        dimension_stats = {dim: {\"count\": 0, \"sentiment_sum\": 0.0} for dim in POVERTY_DIMENSIONS}\n",
    "        for channel in channel_infos:\n",
    "            channel_id = self.get_channel_id_by_name(channel[\"name\"], state_name)\n",
    "            if not channel_id:\n",
    "                continue\n",
    "            videos = self.get_channel_videos(channel_id, date_range[\"published_after\"], date_range[\"published_before\"])\n",
    "            for video in videos:\n",
    "                all_texts = [video[\"title\"] + \". \" + video[\"description\"]] + self.get_video_comments(video[\"id\"])\n",
    "                for text in all_texts:\n",
    "                    clean = self.processor.clean_text(text)\n",
    "                    sentiment = self.processor.get_sentiment_score(clean)\n",
    "                    dimension, confidence = self.processor.classify_dimension(clean)\n",
    "                    if confidence > 0.1:\n",
    "                        dimension_stats[dimension][\"count\"] += 1\n",
    "                        dimension_stats[dimension][\"sentiment_sum\"] += sentiment\n",
    "        return dimension_stats\n",
    "\n",
    "\n",
    "def analyze_all_states():\n",
    "    analyzer = YouTubeAnalyzer(YT_API_KEY)\n",
    "    date_range = {\n",
    "        \"published_after\": \"2022-01-01T00:00:00Z\",\n",
    "        \"published_before\": \"2022-12-31T23:59:59Z\"\n",
    "    }\n",
    "    os.makedirs(\"yt_channels\", exist_ok=True)\n",
    "    for state, channels in STATES_CHANNELS_NAMES.items():\n",
    "        stats = analyzer.analyze_state(state, channels, date_range)\n",
    "        df = pd.DataFrame([{ \"dimension\": dim.replace(\"_\", \" \").title(), \"comment_count\": v[\"count\"], \"avg_sentiment\": v[\"sentiment_sum\"]/v[\"count\"] if v[\"count\"] else 0 } for dim, v in stats.items()])\n",
    "        df.to_csv(f\"yt_channels/{state.replace(' ', '_').lower()}.csv\", index=False)\n",
    "        print(f\"Saved yt_channels/{state.replace(' ', '_').lower()}.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_all_states()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
