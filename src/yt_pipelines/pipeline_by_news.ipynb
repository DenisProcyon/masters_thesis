{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef996967",
   "metadata": {},
   "source": [
    "## Search for the name of the State + 'news' / 'economy'\n",
    "Then use the advance nlp approach to do embedding on a neutral set of words to capture comments that talk about each poverty dimension, and do the sentiment on these comments. \n",
    "Get the count of the total comments and videos analyzed, the counts of comments that belong to each dimension and the sentiment score condition on each dimension. \n",
    "\n",
    "Here there should not be bias since we are:\n",
    "- doing a generic research (state + 'news' and 'noticias' - state + 'economy')\n",
    "- doing the embedding basing on words associated to the different dimensions of poverty but in a neutral way ('work', 'salary' ..). In this way we are able to identfy comments that talk about these issues, but we are not necessarily filtering for those that already talk about them negatively. The sentiment is not necessarily negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90e16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "YT_API_KEY = os.getenv(\"YT_API_KEY\")\n",
    "\n",
    "# Define states and search terms\n",
    "STATES_SEARCH_TERMS = {\n",
    "    \"Tamaulipas\": [\n",
    "        \"Tamaulipas noticias\", \n",
    "        \"Tamaulipas news\", \n",
    "        \"Tamaulipas economía\"\n",
    "    ],\n",
    "    \"Baja California\": [\n",
    "        \"Baja California noticias\", \n",
    "        \"Baja California news\", \n",
    "        \"Baja California economía\"\n",
    "    ],\n",
    "    \"Zacatecas\": [\n",
    "        \"Zacatecas noticias\", \n",
    "        \"Zacatecas new\", \n",
    "        \"Zacatecas economía\"\n",
    "    ],\n",
    "    \"Colima\": [\n",
    "        \"Colima noticias\", \n",
    "        \"Colima news\", \n",
    "        \"Colima economía\"\n",
    "    ],\n",
    "    \"Jalisco\": [\n",
    "        \"Jalisco noticias\", \n",
    "        \"Jalisco news\", \n",
    "        \"Jalisco economía\"]}\n",
    "\n",
    "# Neutral keyword-based descriptions for poverty dimensions: around 30 words per dimension \n",
    "# (60% standard spanish, 30% mexican/spanish slang and 10% english)\n",
    "POVERTY_DIMENSIONS = {\n",
    "    \"INCOME\": \"\"\"\n",
    "    empleo, trabajo, salario, desempleo, ingresos, dinero, deudas, economía, inflación, \n",
    "    sueldo, crisis, préstamos, despidos, recortes, ahorro, impuestos, \n",
    "    chamba, lana, sin chamba, salario jodido, nómina, estar quebrado, billete, \n",
    "    jale, no alcanza, amarrar la quincena, estirar el gasto, \n",
    "    job, unemployment, salary, income, low wages\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO HEALTH SERVICES\": \"\"\"\n",
    "    salud, médico, hospital, medicina, tratamiento, atención, clínica, seguro, \n",
    "    enfermedad, sistema de salud, vacunas, servicios médicos, doctor, emergencia, cuidado, \n",
    "    ir al doctor, sin medicinas, hospital lejano, esperas largas, seguro médico, \n",
    "    doctor particular, medicina cara, remedios caseros, ir a consulta, \n",
    "    healthcare, medical treatment, doctor appointment, health insurance\n",
    "    \"\"\",\n",
    "    \n",
    "    \"EDUCATIONAL LAG\": \"\"\"\n",
    "    educación, escuela, universidad, maestro, estudiante, aprendizaje, \n",
    "    clases, formación, conocimiento, título, bachillerato, preparatoria, \n",
    "    primaria, analfabetismo, deserción escolar, materias, \n",
    "    escuela lejos, sin útiles, no estudiar, chambear en vez de estudiar, \n",
    "    maestros faltistas, sacar buenas notas, echar la flojera, \n",
    "    education, school, learning, degree, student loans\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO SOCIAL SECURITY\": \"\"\"\n",
    "    seguridad social, pensión, jubilación, contrato, derechos laborales, \n",
    "    prestaciones, protección, IMSS, ISSSTE, afore, finiquito, ahorro para retiro, \n",
    "    trabajar en negro, sin papeles, chamba informal, sin contrato, \n",
    "    trabajar por fuera, sin prestaciones, cotizar, sin derechos, \n",
    "    retirement, benefits, social security, worker rights, informal job\n",
    "    \"\"\",\n",
    "    \n",
    "    \"HOUSING\": \"\"\"\n",
    "    vivienda, casa, habitación, hogar, alquiler, renta, hipoteca, \n",
    "    servicios, agua, luz, gas, electricidad, construcción, propiedad, \n",
    "    techo, colonia, vecindario, urbanización, asentamiento, \n",
    "    goteras, casa de lámina, sin agua, renta cara, cuartito, \n",
    "    vecindad, depa, vivir apretados, inundaciones, \n",
    "    housing, rent, mortgage, utilities, housing crisis\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO FOOD\": \"\"\"\n",
    "    alimentación, comida, nutrición, alimentos, dieta, hambre, \n",
    "    mercado, productos, frutas, verduras, carne, leche, básicos, \n",
    "    despensa, supermercado, tienda, comer, cocinar, \n",
    "    canasta básica, tragar, sin comida, hambruna, comida escasa, \n",
    "    no alcanza para comer, frijoles y arroz, pura sopa, \n",
    "    food security, hunger, nutrition, meal, groceries\n",
    "    \"\"\",\n",
    "    \n",
    "    \"SOCIAL COHESION\": \"\"\"\n",
    "    comunidad, sociedad, integración, participación, convivencia, \n",
    "    respeto, diversidad, solidaridad, inclusión, pertenencia, \n",
    "    vecinos, familia, apoyo, redes sociales, confianza, \n",
    "    barrio, marginación, discriminación, racismo, clasismo, \n",
    "    gente divide, no hay unión, barrios peligrosos, \n",
    "    banda, raza, vivir aislados, desplazados, \n",
    "    community, belonging, discrimination, inclusion, social gap\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# limits for scraping\n",
    "MAX_VIDEOS_PER_SEARCH = 100  \n",
    "MAX_COMMENTS_PER_VIDEO = 300  \n",
    "API_SLEEP_TIME = 0.5  \n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.dimension_names = list(POVERTY_DIMENSIONS.keys())\n",
    "        self.dimension_embeddings = self.embedder.encode(list(POVERTY_DIMENSIONS.values()), convert_to_tensor=True)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]', ' ', text)\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def classify_dimension(self, text):\n",
    "        if not text:\n",
    "            return None, 0.0\n",
    "        embedding = self.embedder.encode(text, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embedding, self.dimension_embeddings)[0]\n",
    "        max_idx = torch.argmax(cosine_scores).item()\n",
    "        return self.dimension_names[max_idx], cosine_scores[max_idx].item()\n",
    "\n",
    "    def get_sentiment_score(self, text):\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        stars = torch.argmax(outputs.logits, dim=1).item() + 1\n",
    "        return (stars - 3) / 2  # Normalize to [-1, 1]\n",
    "\n",
    "class YouTubeAnalyzer:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        self.processor = TextProcessor()\n",
    "\n",
    "    def search_videos(self, query, published_after, published_before, max_results=MAX_VIDEOS_PER_SEARCH):\n",
    "        \"\"\"Search for videos using a keyword query.\"\"\"\n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        try:\n",
    "            while len(videos) < max_results:\n",
    "                response = self.youtube.search().list(\n",
    "                    q=query,\n",
    "                    part=\"snippet\",\n",
    "                    maxResults=min(50, max_results - len(videos)),  # YouTube API allows max 50 per request\n",
    "                    pageToken=next_page_token,\n",
    "                    type=\"video\",\n",
    "                    order=\"relevance\",\n",
    "                    publishedAfter=published_after,\n",
    "                    publishedBefore=published_before,\n",
    "                    relevanceLanguage=\"es\"\n",
    "                ).execute()\n",
    "                \n",
    "                for item in response.get(\"items\", []):\n",
    "                    if item[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "                        videos.append({\n",
    "                            \"id\": item[\"id\"][\"videoId\"],\n",
    "                            \"title\": item[\"snippet\"][\"title\"],\n",
    "                            \"description\": item[\"snippet\"].get(\"description\", \"\"),\n",
    "                            \"published_at\": item[\"snippet\"][\"publishedAt\"]\n",
    "                        })\n",
    "                \n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or len(videos) >= max_results:\n",
    "                    break\n",
    "                \n",
    "                sleep(API_SLEEP_TIME)  # Avoid quota exceeded errors\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{query}': {e}\")\n",
    "        \n",
    "        print(f\"Found {len(videos)} videos for query '{query}'\")\n",
    "        return videos\n",
    "\n",
    "    def get_video_comments(self, video_id, max_comments=MAX_COMMENTS_PER_VIDEO):\n",
    "        \"\"\"Get comments for a specific video.\"\"\"\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        try:\n",
    "            while len(comments) < max_comments:\n",
    "                response = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=min(100, max_comments - len(comments)),  # YouTube API allows max 100 per request\n",
    "                    pageToken=next_page_token\n",
    "                ).execute()\n",
    "                \n",
    "                for item in response.get(\"items\", []):\n",
    "                    comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                    comments.append(comment_text)\n",
    "                \n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or len(comments) >= max_comments:\n",
    "                    break\n",
    "                \n",
    "                sleep(API_SLEEP_TIME)  # Avoid quota exceeded errors\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Many videos have comments disabled, so we'll just pass silently\n",
    "            pass\n",
    "        \n",
    "        return comments\n",
    "\n",
    "    def analyze_state_by_keywords(self, state_name, search_terms, date_range):\n",
    "        \"\"\"Analyze a state by searching for videos using specified search terms.\"\"\"\n",
    "        print(f\"\\nAnalyzing {state_name}...\")\n",
    "        dimension_stats = {dim: {\"sentiment_sum\": 0.0, \"count\": 0} for dim in POVERTY_DIMENSIONS}\n",
    "        total_videos = 0\n",
    "        total_comments = 0\n",
    "        \n",
    "        # Search for videos with each search term\n",
    "        for search_term in search_terms:\n",
    "            print(f\"  Searching for '{search_term}'...\")\n",
    "            videos = self.search_videos(\n",
    "                query=search_term,\n",
    "                published_after=date_range[\"published_after\"],\n",
    "                published_before=date_range[\"published_before\"],\n",
    "                max_results=MAX_VIDEOS_PER_SEARCH\n",
    "            )\n",
    "            \n",
    "            if not videos:\n",
    "                continue\n",
    "                \n",
    "            total_videos += len(videos)\n",
    "            \n",
    "            # Process videos\n",
    "            for video in tqdm(videos, desc=f\"Processing videos for '{search_term}'\"):\n",
    "                # Get video comments\n",
    "                comments = self.get_video_comments(video[\"id\"], MAX_COMMENTS_PER_VIDEO)\n",
    "                total_comments += len(comments)\n",
    "                \n",
    "                # Concatenate title, description and comments for analysis\n",
    "                all_texts = [video[\"title\"] + \". \" + video[\"description\"]] + comments\n",
    "                \n",
    "                # Analyze each text\n",
    "                for text in all_texts:\n",
    "                    clean = self.processor.clean_text(text)\n",
    "                    if len(clean) < 10:  # Skip very short texts\n",
    "                        continue\n",
    "                        \n",
    "                    dimension, confidence = self.processor.classify_dimension(clean)\n",
    "                    if confidence > 0.1:  # Only count if confidence is high enough\n",
    "                        sentiment = self.processor.get_sentiment_score(clean)\n",
    "                        dimension_stats[dimension][\"sentiment_sum\"] += sentiment\n",
    "                        dimension_stats[dimension][\"count\"] += 1\n",
    "        \n",
    "        print(f\"  Analyzed {total_videos} videos and {total_comments} comments for {state_name}\")\n",
    "        return dimension_stats, total_videos, total_comments\n",
    "\n",
    "def analyze_all_states():\n",
    "    analyzer = YouTubeAnalyzer(YT_API_KEY)\n",
    "    date_range = {\n",
    "        \"published_after\": \"2022-01-01T00:00:00Z\",\n",
    "        \"published_before\": \"2022-12-31T23:59:59Z\"\n",
    "    }\n",
    "    \n",
    "    # Create directories for results\n",
    "    os.makedirs(\"yt_keyword_sentiment\", exist_ok=True)\n",
    "    \n",
    "    # Store overall stats for summary\n",
    "    all_results = []\n",
    "    \n",
    "    for state, search_terms in STATES_SEARCH_TERMS.items():\n",
    "        stats, total_videos, total_comments = analyzer.analyze_state_by_keywords(\n",
    "            state_name=state,\n",
    "            search_terms=search_terms,\n",
    "            date_range=date_range\n",
    "        )\n",
    "        \n",
    "        # Create dataframe for this state\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                \"state\": state,\n",
    "                \"dimension\": dim.replace(\"_\", \" \").title(),\n",
    "                \"avg_sentiment\": v[\"sentiment_sum\"] / v[\"count\"] if v[\"count\"] else 0,\n",
    "                \"mentions_count\": v[\"count\"],\n",
    "                \"videos_analyzed\": total_videos,\n",
    "                \"comments_analyzed\": total_comments\n",
    "            }\n",
    "            for dim, v in stats.items()\n",
    "        ])\n",
    "        \n",
    "        # Save state-specific results\n",
    "        output_file = f\"yt_keyword_sentiment/{state.replace(' ', '_').lower()}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved results to {output_file}\")\n",
    "        \n",
    "        # Add to overall results\n",
    "        all_results.append(df)\n",
    "    \n",
    "    # Combine all results into one dataframe\n",
    "    if all_results:\n",
    "        all_df = pd.concat(all_results)\n",
    "        all_df.to_csv(\"yt_keyword_sentiment/all_states_results.csv\", index=False)\n",
    "        print(\"Saved combined results to yt_keyword_sentiment/all_states_results.csv\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_all_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
