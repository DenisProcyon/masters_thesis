{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef996967",
   "metadata": {},
   "source": [
    "## Search for the name of the State + 'news' / 'economy'\n",
    "Then use the advance nlp approach to do embedding on a neutral set of words to capture comments that talk about each poverty dimension, and do the sentiment on these comments. \n",
    "Get the count of the total comments and videos analyzed, the counts of comments that belong to each dimension and the sentiment score condition on each dimension. \n",
    "\n",
    "Here there should not be bias since we are:\n",
    "- doing a generic research (state + 'news' and 'noticias' - state + 'economy')\n",
    "- doing the embedding basing on words associated to the different dimensions of poverty but in a neutral way ('work', 'salary' ..). In this way we are able to identfy comments that talk about these issues, but we are not necessarily filtering for those that already talk about them negatively. The sentiment is not necessarily negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da90e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noemilucchi/miniforge3/envs/new/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Guanajuato...\n",
      "  Searching for 'Guanajuato noticias'...\n",
      "Found 100 videos for query 'Guanajuato noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Guanajuato noticias': 100%|██████████| 100/100 [04:00<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Guanajuato news'...\n",
      "Found 100 videos for query 'Guanajuato news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Guanajuato news': 100%|██████████| 100/100 [04:56<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Guanajuato economía'...\n",
      "Found 100 videos for query 'Guanajuato economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Guanajuato economía': 100%|██████████| 100/100 [02:38<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 8987 comments for Guanajuato\n",
      "Saved results to yt_keyword_sentiment/guanajuato.csv\n",
      "\n",
      "Analyzing Michoacán...\n",
      "  Searching for 'Michoacán noticias'...\n",
      "Found 100 videos for query 'Michoacán noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Michoacán noticias': 100%|██████████| 100/100 [05:03<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Michoacán new'...\n",
      "Found 100 videos for query 'Michoacán new'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Michoacán new': 100%|██████████| 100/100 [10:41<00:00,  6.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Michoacán economía'...\n",
      "Found 100 videos for query 'Michoacán economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Michoacán economía': 100%|██████████| 100/100 [05:36<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 15046 comments for Michoacán\n",
      "Saved results to yt_keyword_sentiment/michoacán.csv\n",
      "\n",
      "Analyzing Sinaloa...\n",
      "  Searching for 'Sinaloa noticias'...\n",
      "Found 100 videos for query 'Sinaloa noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Sinaloa noticias': 100%|██████████| 100/100 [06:47<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Sinaloa news'...\n",
      "Found 100 videos for query 'Sinaloa news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Sinaloa news': 100%|██████████| 100/100 [11:12<00:00,  6.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Sinaloa economía'...\n",
      "Found 100 videos for query 'Sinaloa economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Sinaloa economía': 100%|██████████| 100/100 [01:07<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 13467 comments for Sinaloa\n",
      "Saved results to yt_keyword_sentiment/sinaloa.csv\n",
      "\n",
      "Analyzing Chihuahua...\n",
      "  Searching for 'Chihuahua noticias'...\n",
      "Found 100 videos for query 'Chihuahua noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Chihuahua noticias': 100%|██████████| 100/100 [02:22<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Chihuahua news'...\n",
      "Found 100 videos for query 'Chihuahua news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Chihuahua news': 100%|██████████| 100/100 [02:13<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Chihuahua economía'...\n",
      "Found 100 videos for query 'Chihuahua economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Chihuahua economía': 100%|██████████| 100/100 [04:44<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 10811 comments for Chihuahua\n",
      "Saved results to yt_keyword_sentiment/chihuahua.csv\n",
      "\n",
      "Analyzing Guerrero...\n",
      "  Searching for 'Guerrero noticias'...\n",
      "Found 100 videos for query 'Guerrero noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Guerrero noticias': 100%|██████████| 100/100 [02:36<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Guerrero news'...\n",
      "Found 100 videos for query 'Guerrero news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Guerrero news': 100%|██████████| 100/100 [03:29<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Guerrero economía'...\n",
      "Found 100 videos for query 'Guerrero economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Guerrero economía': 100%|██████████| 100/100 [03:08<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 10180 comments for Guerrero\n",
      "Saved results to yt_keyword_sentiment/guerrero.csv\n",
      "\n",
      "Analyzing Tamaulipas...\n",
      "  Searching for 'Tamaulipas noticias'...\n",
      "Found 100 videos for query 'Tamaulipas noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tamaulipas noticias': 100%|██████████| 100/100 [01:57<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Tamaulipas news'...\n",
      "Found 100 videos for query 'Tamaulipas news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tamaulipas news': 100%|██████████| 100/100 [08:06<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Tamaulipas economía'...\n",
      "Found 100 videos for query 'Tamaulipas economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tamaulipas economía': 100%|██████████| 100/100 [02:43<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 14120 comments for Tamaulipas\n",
      "Saved results to yt_keyword_sentiment/tamaulipas.csv\n",
      "\n",
      "Analyzing Baja California...\n",
      "  Searching for 'Baja California noticias'...\n",
      "Found 100 videos for query 'Baja California noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Baja California noticias': 100%|██████████| 100/100 [02:13<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Baja California news'...\n",
      "Found 100 videos for query 'Baja California news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Baja California news': 100%|██████████| 100/100 [03:38<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Baja California economía'...\n",
      "Found 100 videos for query 'Baja California economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Baja California economía': 100%|██████████| 100/100 [00:24<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 6178 comments for Baja California\n",
      "Saved results to yt_keyword_sentiment/baja_california.csv\n",
      "\n",
      "Analyzing Zacatecas...\n",
      "  Searching for 'Zacatecas noticias'...\n",
      "Found 100 videos for query 'Zacatecas noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Zacatecas noticias': 100%|██████████| 100/100 [03:58<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Zacatecas new'...\n",
      "Found 100 videos for query 'Zacatecas new'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Zacatecas new': 100%|██████████| 100/100 [06:25<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Zacatecas economía'...\n",
      "Found 100 videos for query 'Zacatecas economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Zacatecas economía': 100%|██████████| 100/100 [05:03<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 17707 comments for Zacatecas\n",
      "Saved results to yt_keyword_sentiment/zacatecas.csv\n",
      "\n",
      "Analyzing Colima...\n",
      "  Searching for 'Colima noticias'...\n",
      "Found 100 videos for query 'Colima noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Colima noticias': 100%|██████████| 100/100 [03:14<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Colima news'...\n",
      "Found 100 videos for query 'Colima news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Colima news': 100%|██████████| 100/100 [11:31<00:00,  6.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Colima economía'...\n",
      "Found 100 videos for query 'Colima economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Colima economía': 100%|██████████| 100/100 [02:59<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 21324 comments for Colima\n",
      "Saved results to yt_keyword_sentiment/colima.csv\n",
      "\n",
      "Analyzing Jalisco...\n",
      "  Searching for 'Jalisco noticias'...\n",
      "Found 100 videos for query 'Jalisco noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Jalisco noticias': 100%|██████████| 100/100 [00:49<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Jalisco news'...\n",
      "Found 100 videos for query 'Jalisco news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Jalisco news': 100%|██████████| 100/100 [01:19<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Jalisco economía'...\n",
      "Found 100 videos for query 'Jalisco economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Jalisco economía': 100%|██████████| 100/100 [00:21<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 2075 comments for Jalisco\n",
      "Saved results to yt_keyword_sentiment/jalisco.csv\n",
      "Saved combined results to yt_keyword_sentiment/all_states_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "YT_API_KEY = os.getenv(\"YT_API_KEY\")\n",
    "\n",
    "# Define states and search terms\n",
    "STATES_SEARCH_TERMS = {\n",
    "    \"Guanajuato\": [\n",
    "        \"Guanajuato noticias\", \n",
    "        \"Guanajuato news\", \n",
    "        \"Guanajuato economía\"\n",
    "    ],\n",
    "    \"Michoacán\": [\n",
    "        \"Michoacán noticias\", \n",
    "        \"Michoacán new\", \n",
    "        \"Michoacán economía\"\n",
    "    ],\n",
    "    \"Sinaloa\": [\n",
    "        \"Sinaloa noticias\", \n",
    "        \"Sinaloa news\", \n",
    "        \"Sinaloa economía\"\n",
    "    ],\n",
    "    \"Chihuahua\": [\n",
    "        \"Chihuahua noticias\", \n",
    "        \"Chihuahua news\", \n",
    "        \"Chihuahua economía\"\n",
    "    ],\n",
    "    \"Guerrero\": [\n",
    "        \"Guerrero noticias\", \n",
    "        \"Guerrero news\", \n",
    "        \"Guerrero economía\"\n",
    "    ],\n",
    "    \"Tamaulipas\": [\n",
    "        \"Tamaulipas noticias\", \n",
    "        \"Tamaulipas news\", \n",
    "        \"Tamaulipas economía\"\n",
    "    ],\n",
    "    \"Baja California\": [\n",
    "        \"Baja California noticias\", \n",
    "        \"Baja California news\", \n",
    "        \"Baja California economía\"\n",
    "    ],\n",
    "    \"Zacatecas\": [\n",
    "        \"Zacatecas noticias\", \n",
    "        \"Zacatecas new\", \n",
    "        \"Zacatecas economía\"\n",
    "    ],\n",
    "    \"Colima\": [\n",
    "        \"Colima noticias\", \n",
    "        \"Colima news\", \n",
    "        \"Colima economía\"\n",
    "    ],\n",
    "    \"Jalisco\": [\n",
    "        \"Jalisco noticias\", \n",
    "        \"Jalisco news\", \n",
    "        \"Jalisco economía\"]}\n",
    "\n",
    "# Neutral keyword-based descriptions for poverty dimensions: around 30 words per dimension \n",
    "# (60% standard spanish, 30% mexican/spanish slang and 10% english)\n",
    "POVERTY_DIMENSIONS = {\n",
    "    \"INCOME\": \"\"\"\n",
    "    empleo, trabajo, salario, ingresos, dinero, economía, sueldo, ahorro, impuestos, \n",
    "    chamba, lana, nómina, billete, jale, job, salary, income, money\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO HEALTH SERVICES\": \"\"\"\n",
    "    salud, médico, hospital, medicina, tratamiento, atención, clínica, seguro,\n",
    "    sistema de salud, vacunas, servicios médicos, doctor, cuidado, ir al doctor,\n",
    "    seguro médico, doctor particular, ir a consulta, healthcare, medical treatment, \n",
    "    doctor appointment, health insurance\n",
    "    \"\"\",\n",
    "    \n",
    "    \"EDUCATIONAL LAG\": \"\"\"\n",
    "    educación, escuela, universidad, maestro, estudiante, aprendizaje, \n",
    "    clases, formación, conocimiento, título, bachillerato, preparatoria, \n",
    "    primaria, materias, escuela lejos, sacar buenas notas,\n",
    "    education, school, learning, degree, student loans\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO SOCIAL SECURITY\": \"\"\"\n",
    "    seguridad social, pensión, jubilación, contrato, derechos laborales, \n",
    "    prestaciones, protección, IMSS, ISSSTE, afore, finiquito, ahorro para retiro, \n",
    "    cotizar, retirement, benefits, social security, worker rights, informal job\n",
    "    \"\"\",\n",
    "    \n",
    "    \"HOUSING\": \"\"\"\n",
    "    vivienda, casa, habitación, hogar, alquiler, renta,\n",
    "    servicios, agua, luz, gas, electricidad, construcción, propiedad, \n",
    "    techo, colonia, vecindario, urbanización, asentamiento, cuartito, \n",
    "    depa, housing, rent, mortgage, utilities\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO FOOD\": \"\"\"\n",
    "    alimentación, comida, nutrición, alimentos, dieta,\n",
    "    mercado, productos, frutas, verduras, carne, leche, básicos, \n",
    "    despensa, supermercado, tienda, comer, cocinar, \n",
    "    canasta básica, tragar, food security, nutrition, meal, groceries\n",
    "    \"\"\",\n",
    "    \n",
    "    \"SOCIAL COHESION\": \"\"\"\n",
    "    comunidad, sociedad, integración, participación, convivencia, \n",
    "    respeto, diversidad, solidaridad, inclusión, pertenencia, \n",
    "    vecinos, familia, apoyo, redes sociales, confianza, \n",
    "    barrio, raza, community, belonging, inclusion\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# limits for scraping\n",
    "MAX_VIDEOS_PER_SEARCH = 100  \n",
    "MAX_COMMENTS_PER_VIDEO = 300  \n",
    "API_SLEEP_TIME = 0.5  \n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.dimension_names = list(POVERTY_DIMENSIONS.keys())\n",
    "        self.dimension_embeddings = self.embedder.encode(list(POVERTY_DIMENSIONS.values()), convert_to_tensor=True)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]', ' ', text)\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def classify_dimension(self, text):\n",
    "        if not text:\n",
    "            return None, 0.0\n",
    "        embedding = self.embedder.encode(text, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embedding, self.dimension_embeddings)[0]\n",
    "        max_idx = torch.argmax(cosine_scores).item()\n",
    "        return self.dimension_names[max_idx], cosine_scores[max_idx].item()\n",
    "\n",
    "    def get_sentiment_score(self, text):\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        stars = torch.argmax(outputs.logits, dim=1).item() + 1\n",
    "        return (stars - 3) / 2  # Normalize to [-1, 1]\n",
    "\n",
    "class YouTubeAnalyzer:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        self.processor = TextProcessor()\n",
    "\n",
    "    def search_videos(self, query, published_after, published_before, max_results=MAX_VIDEOS_PER_SEARCH):\n",
    "        \"\"\"Search for videos using a keyword query.\"\"\"\n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        try:\n",
    "            while len(videos) < max_results:\n",
    "                response = self.youtube.search().list(\n",
    "                    q=query,\n",
    "                    part=\"snippet\",\n",
    "                    maxResults=min(50, max_results - len(videos)),  # YouTube API allows max 50 per request\n",
    "                    pageToken=next_page_token,\n",
    "                    type=\"video\",\n",
    "                    order=\"relevance\",\n",
    "                    publishedAfter=published_after,\n",
    "                    publishedBefore=published_before,\n",
    "                    relevanceLanguage=\"es\"\n",
    "                ).execute()\n",
    "                \n",
    "                for item in response.get(\"items\", []):\n",
    "                    if item[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "                        videos.append({\n",
    "                            \"id\": item[\"id\"][\"videoId\"],\n",
    "                            \"title\": item[\"snippet\"][\"title\"],\n",
    "                            \"description\": item[\"snippet\"].get(\"description\", \"\"),\n",
    "                            \"published_at\": item[\"snippet\"][\"publishedAt\"]\n",
    "                        })\n",
    "                \n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or len(videos) >= max_results:\n",
    "                    break\n",
    "                \n",
    "                sleep(API_SLEEP_TIME)  # Avoid quota exceeded errors\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{query}': {e}\")\n",
    "        \n",
    "        print(f\"Found {len(videos)} videos for query '{query}'\")\n",
    "        return videos\n",
    "\n",
    "    def get_video_comments(self, video_id, max_comments=MAX_COMMENTS_PER_VIDEO):\n",
    "        \"\"\"Get comments for a specific video.\"\"\"\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        try:\n",
    "            while len(comments) < max_comments:\n",
    "                response = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=min(100, max_comments - len(comments)),  # YouTube API allows max 100 per request\n",
    "                    pageToken=next_page_token\n",
    "                ).execute()\n",
    "                \n",
    "                for item in response.get(\"items\", []):\n",
    "                    comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                    comments.append(comment_text)\n",
    "                \n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or len(comments) >= max_comments:\n",
    "                    break\n",
    "                \n",
    "                sleep(API_SLEEP_TIME)  # Avoid quota exceeded errors\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Many videos have comments disabled, so we'll just pass silently\n",
    "            pass\n",
    "        \n",
    "        return comments\n",
    "\n",
    "    def analyze_state_by_keywords(self, state_name, search_terms, date_range):\n",
    "        \"\"\"Analyze a state by searching for videos using specified search terms.\"\"\"\n",
    "        print(f\"\\nAnalyzing {state_name}...\")\n",
    "        dimension_stats = {dim: {\"sentiment_sum\": 0.0, \"count\": 0} for dim in POVERTY_DIMENSIONS}\n",
    "        total_videos = 0\n",
    "        total_comments = 0\n",
    "        \n",
    "        # Search for videos with each search term\n",
    "        for search_term in search_terms:\n",
    "            print(f\"  Searching for '{search_term}'...\")\n",
    "            videos = self.search_videos(\n",
    "                query=search_term,\n",
    "                published_after=date_range[\"published_after\"],\n",
    "                published_before=date_range[\"published_before\"],\n",
    "                max_results=MAX_VIDEOS_PER_SEARCH\n",
    "            )\n",
    "            \n",
    "            if not videos:\n",
    "                continue\n",
    "                \n",
    "            total_videos += len(videos)\n",
    "            \n",
    "            # Process videos\n",
    "            for video in tqdm(videos, desc=f\"Processing videos for '{search_term}'\"):\n",
    "                # Get video comments\n",
    "                comments = self.get_video_comments(video[\"id\"], MAX_COMMENTS_PER_VIDEO)\n",
    "                total_comments += len(comments)\n",
    "                \n",
    "                # Concatenate title, description and comments for analysis\n",
    "                all_texts = [video[\"title\"] + \". \" + video[\"description\"]] + comments\n",
    "                \n",
    "                # Analyze each text\n",
    "                for text in all_texts:\n",
    "                    clean = self.processor.clean_text(text)\n",
    "                    if len(clean) < 10:  # Skip very short texts\n",
    "                        continue\n",
    "                        \n",
    "                    dimension, confidence = self.processor.classify_dimension(clean)\n",
    "                    if confidence > 0.1:  # Only count if confidence is high enough\n",
    "                        sentiment = self.processor.get_sentiment_score(clean)\n",
    "                        dimension_stats[dimension][\"sentiment_sum\"] += sentiment\n",
    "                        dimension_stats[dimension][\"count\"] += 1\n",
    "        \n",
    "        print(f\"  Analyzed {total_videos} videos and {total_comments} comments for {state_name}\")\n",
    "        return dimension_stats, total_videos, total_comments\n",
    "\n",
    "def analyze_all_states():\n",
    "    analyzer = YouTubeAnalyzer(YT_API_KEY)\n",
    "    date_range = {\n",
    "        \"published_after\": \"2022-01-01T00:00:00Z\",\n",
    "        \"published_before\": \"2022-12-31T23:59:59Z\"\n",
    "    }\n",
    "    \n",
    "    # Create directories for results\n",
    "    os.makedirs(\"yt_keyword_sentiment\", exist_ok=True)\n",
    "    \n",
    "    # Store overall stats for summary\n",
    "    all_results = []\n",
    "    \n",
    "    for state, search_terms in STATES_SEARCH_TERMS.items():\n",
    "        stats, total_videos, total_comments = analyzer.analyze_state_by_keywords(\n",
    "            state_name=state,\n",
    "            search_terms=search_terms,\n",
    "            date_range=date_range\n",
    "        )\n",
    "        \n",
    "        # Create dataframe for this state\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                \"state\": state,\n",
    "                \"dimension\": dim.replace(\"_\", \" \").title(),\n",
    "                \"avg_sentiment\": v[\"sentiment_sum\"] / v[\"count\"] if v[\"count\"] else 0,\n",
    "                \"mentions_count\": v[\"count\"],\n",
    "                \"videos_analyzed\": total_videos,\n",
    "                \"comments_analyzed\": total_comments\n",
    "            }\n",
    "            for dim, v in stats.items()\n",
    "        ])\n",
    "        \n",
    "        # Save state-specific results\n",
    "        output_file = f\"yt_keyword_sentiment/{state.replace(' ', '_').lower()}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved results to {output_file}\")\n",
    "        \n",
    "        # Add to overall results\n",
    "        all_results.append(df)\n",
    "    \n",
    "    # Combine all results into one dataframe\n",
    "    if all_results:\n",
    "        all_df = pd.concat(all_results)\n",
    "        all_df.to_csv(\"yt_keyword_sentiment/all_states_results.csv\", index=False)\n",
    "        print(\"Saved combined results to yt_keyword_sentiment/all_states_results.csv\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_all_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
