{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef996967",
   "metadata": {},
   "source": [
    "## Search for the name of the State + 'news' / 'economy'\n",
    "Then use the advance nlp approach to do embedding on a neutral set of words to capture comments that talk about each poverty dimension, and do the sentiment on these comments. \n",
    "Get the count of the total comments and videos analyzed, the counts of comments that belong to each dimension and the sentiment score condition on each dimension. \n",
    "\n",
    "Here there should not be bias since we are:\n",
    "- doing a generic research (state + 'news' and 'noticias' - state + 'economy')\n",
    "- doing the embedding basing on words associated to the different dimensions of poverty but in a neutral way ('work', 'salary' ..). In this way we are able to identfy comments that talk about these issues, but we are not necessarily filtering for those that already talk about them negatively. The sentiment is not necessarily negative "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da90e16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/noemilucchi/miniforge3/envs/new/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing Querétaro...\n",
      "  Searching for 'Querétaro noticias'...\n",
      "Found 100 videos for query 'Querétaro noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Querétaro noticias': 100%|██████████| 100/100 [01:24<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Querétaro news'...\n",
      "Found 100 videos for query 'Querétaro news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Querétaro news': 100%|██████████| 100/100 [05:01<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Querétaro economía'...\n",
      "Found 100 videos for query 'Querétaro economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Querétaro economía': 100%|██████████| 100/100 [01:36<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 8876 comments for Querétaro\n",
      "Saved results to yt_data_2020/querétaro.csv\n",
      "\n",
      "Analyzing Quintana Roo...\n",
      "  Searching for 'Quintana Roo noticias'...\n",
      "Found 100 videos for query 'Quintana Roo noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Quintana Roo noticias': 100%|██████████| 100/100 [00:52<00:00,  1.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Quintana Roo news'...\n",
      "Found 100 videos for query 'Quintana Roo news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Quintana Roo news': 100%|██████████| 100/100 [01:50<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Quintana Roo economía'...\n",
      "Found 100 videos for query 'Quintana Roo economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Quintana Roo economía': 100%|██████████| 100/100 [00:38<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 2968 comments for Quintana Roo\n",
      "Saved results to yt_data_2020/quintana_roo.csv\n",
      "\n",
      "Analyzing San Luis Potosí...\n",
      "  Searching for 'San Luis Potosí noticias'...\n",
      "Found 100 videos for query 'San Luis Potosí noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'San Luis Potosí noticias': 100%|██████████| 100/100 [01:44<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'San Luis Potosí news'...\n",
      "Found 100 videos for query 'San Luis Potosí news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'San Luis Potosí news': 100%|██████████| 100/100 [02:04<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'San Luis Potosí economía'...\n",
      "Found 100 videos for query 'San Luis Potosí economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'San Luis Potosí economía': 100%|██████████| 100/100 [00:23<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 3394 comments for San Luis Potosí\n",
      "Saved results to yt_data_2020/san_luis_potosí.csv\n",
      "\n",
      "Analyzing Sinaloa...\n",
      "  Searching for 'Sinaloa noticias'...\n",
      "Found 100 videos for query 'Sinaloa noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Sinaloa noticias': 100%|██████████| 100/100 [01:46<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Sinaloa news'...\n",
      "Found 100 videos for query 'Sinaloa news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Sinaloa news': 100%|██████████| 100/100 [09:26<00:00,  5.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Sinaloa economía'...\n",
      "Found 100 videos for query 'Sinaloa economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Sinaloa economía': 100%|██████████| 100/100 [02:03<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 13493 comments for Sinaloa\n",
      "Saved results to yt_data_2020/sinaloa.csv\n",
      "\n",
      "Analyzing Sonora...\n",
      "  Searching for 'Sonora noticias'...\n",
      "Found 100 videos for query 'Sonora noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Sonora noticias': 100%|██████████| 100/100 [01:48<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Sonora news'...\n",
      "Found 100 videos for query 'Sonora news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Sonora news': 100%|██████████| 100/100 [02:39<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Sonora economía'...\n",
      "Found 100 videos for query 'Sonora economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Sonora economía': 100%|██████████| 100/100 [00:47<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 4591 comments for Sonora\n",
      "Saved results to yt_data_2020/sonora.csv\n",
      "\n",
      "Analyzing Tabasco...\n",
      "  Searching for 'Tabasco noticias'...\n",
      "Found 100 videos for query 'Tabasco noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tabasco noticias': 100%|██████████| 100/100 [01:57<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Tabasco news'...\n",
      "Found 100 videos for query 'Tabasco news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tabasco news': 100%|██████████| 100/100 [06:33<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Tabasco economía'...\n",
      "Found 100 videos for query 'Tabasco economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tabasco economía': 100%|██████████| 100/100 [01:15<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 8958 comments for Tabasco\n",
      "Saved results to yt_data_2020/tabasco.csv\n",
      "\n",
      "Analyzing Tamaulipas...\n",
      "  Searching for 'Tamaulipas noticias'...\n",
      "Found 100 videos for query 'Tamaulipas noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tamaulipas noticias': 100%|██████████| 100/100 [03:02<00:00,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Tamaulipas news'...\n",
      "Found 100 videos for query 'Tamaulipas news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tamaulipas news': 100%|██████████| 100/100 [08:27<00:00,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Tamaulipas economía'...\n",
      "Found 100 videos for query 'Tamaulipas economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tamaulipas economía': 100%|██████████| 100/100 [01:19<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 13011 comments for Tamaulipas\n",
      "Saved results to yt_data_2020/tamaulipas.csv\n",
      "\n",
      "Analyzing Tlaxcala...\n",
      "  Searching for 'Tlaxcala noticias'...\n",
      "Found 100 videos for query 'Tlaxcala noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tlaxcala noticias': 100%|██████████| 100/100 [01:16<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Tlaxcala news'...\n",
      "Found 100 videos for query 'Tlaxcala news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tlaxcala news': 100%|██████████| 100/100 [03:31<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Tlaxcala economía'...\n",
      "Found 100 videos for query 'Tlaxcala economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Tlaxcala economía': 100%|██████████| 100/100 [01:55<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 5786 comments for Tlaxcala\n",
      "Saved results to yt_data_2020/tlaxcala.csv\n",
      "\n",
      "Analyzing Veracruz...\n",
      "  Searching for 'Veracruz noticias'...\n",
      "Found 100 videos for query 'Veracruz noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Veracruz noticias': 100%|██████████| 100/100 [02:56<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Veracruz news'...\n",
      "Found 100 videos for query 'Veracruz news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Veracruz news': 100%|██████████| 100/100 [09:00<00:00,  5.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Veracruz economía'...\n",
      "Found 100 videos for query 'Veracruz economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Veracruz economía': 100%|██████████| 100/100 [03:50<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 16241 comments for Veracruz\n",
      "Saved results to yt_data_2020/veracruz.csv\n",
      "\n",
      "Analyzing Yucatán...\n",
      "  Searching for 'Yucatán noticias'...\n",
      "Found 100 videos for query 'Yucatán noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Yucatán noticias': 100%|██████████| 100/100 [00:38<00:00,  2.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Yucatán news'...\n",
      "Found 100 videos for query 'Yucatán news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Yucatán news': 100%|██████████| 100/100 [02:11<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Yucatán economía'...\n",
      "Found 100 videos for query 'Yucatán economía'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Yucatán economía': 100%|██████████| 100/100 [00:43<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Analyzed 300 videos and 2971 comments for Yucatán\n",
      "Saved results to yt_data_2020/yucatán.csv\n",
      "\n",
      "Analyzing Zacatecas...\n",
      "  Searching for 'Zacatecas noticias'...\n",
      "Found 100 videos for query 'Zacatecas noticias'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Zacatecas noticias': 100%|██████████| 100/100 [05:21<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Zacatecas news'...\n",
      "Found 100 videos for query 'Zacatecas news'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos for 'Zacatecas news': 100%|██████████| 100/100 [04:47<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Searching for 'Zacatecas economía'...\n",
      "Error searching for 'Zacatecas economía': <HttpError 403 when requesting https://youtube.googleapis.com/youtube/v3/search?q=Zacatecas+econom%C3%ADa&part=snippet&maxResults=50&type=video&order=relevance&publishedAfter=2020-01-01T00%3A00%3A00Z&publishedBefore=2020-12-31T23%3A59%3A59Z&relevanceLanguage=es&key=AIzaSyALlbNSWF23xN2MS12rL3-cJEviyA0nPwU&alt=json returned \"The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.\". Details: \"[{'message': 'The request cannot be completed because you have exceeded your <a href=\"/youtube/v3/getting-started#quota\">quota</a>.', 'domain': 'youtube.quota', 'reason': 'quotaExceeded'}]\">\n",
      "Found 0 videos for query 'Zacatecas economía'\n",
      "  Analyzed 200 videos and 8858 comments for Zacatecas\n",
      "Saved results to yt_data_2020/zacatecas.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from googleapiclient.discovery import build\n",
    "from time import sleep\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "YT_API_KEY = os.getenv(\"YT_API_KEY\")\n",
    "\n",
    "# Define states and search terms\n",
    "STATES_SEARCH_TERMS = {\n",
    "    \"Aguascalientes\": [\"Aguascalientes noticias\", \"Aguascalientes news\", \"Aguascalientes economía\"],\n",
    "    \"Baja California\": [\"Baja California noticias\", \"Baja California news\", \"Baja California economía\"],\n",
    "    \"Baja California Sur\": [\"Baja California Sur noticias\", \"Baja California Sur news\", \"Baja California Sur economía\"],\n",
    "    \"Campeche\": [\"Campeche noticias\", \"Campeche news\", \"Campeche economía\"],\n",
    "    \"Chiapas\": [\"Chiapas noticias\", \"Chiapas news\", \"Chiapas economía\"],\n",
    "    \"Chihuahua\": [\"Chihuahua noticias\", \"Chihuahua news\", \"Chihuahua economía\"],\n",
    "    \"Ciudad de México\": [\"Ciudad de México noticias\", \"Ciudad de México news\", \"Ciudad de México economía\"],\n",
    "    \"Coahuila\": [\"Coahuila noticias\", \"Coahuila news\", \"Coahuila economía\"],\n",
    "    \"Colima\": [\"Colima noticias\", \"Colima news\", \"Colima economía\"],\n",
    "    \"Durango\": [\"Durango noticias\", \"Durango news\", \"Durango economía\"],\n",
    "    \"Estado de México\": [\"Estado de México noticias\", \"Estado de México news\", \"Estado de México economía\"],\n",
    "    \"Guanajuato\": [\"Guanajuato noticias\", \"Guanajuato news\", \"Guanajuato economía\"],\n",
    "    \"Guerrero\": [\"Guerrero noticias\", \"Guerrero news\", \"Guerrero economía\"],\n",
    "    \"Hidalgo\": [\"Hidalgo noticias\", \"Hidalgo news\", \"Hidalgo economía\"],\n",
    "    \"Jalisco\": [\"Jalisco noticias\", \"Jalisco news\", \"Jalisco economía\"],\n",
    "    \"Michoacán\": [\"Michoacán noticias\", \"Michoacán news\", \"Michoacán economía\"],\n",
    "    \"Morelos\": [\"Morelos noticias\", \"Morelos news\", \"Morelos economía\"],\n",
    "    \"Nayarit\": [\"Nayarit noticias\", \"Nayarit news\", \"Nayarit economía\"],\n",
    "    \"Nuevo León\": [\"Nuevo León noticias\", \"Nuevo León news\", \"Nuevo León economía\"],\n",
    "    \"Oaxaca\": [\"Oaxaca noticias\", \"Oaxaca news\", \"Oaxaca economía\"],\n",
    "    \"Puebla\": [\"Puebla noticias\", \"Puebla news\", \"Puebla economía\"],\n",
    "    \"Querétaro\": [\"Querétaro noticias\", \"Querétaro news\", \"Querétaro economía\"],\n",
    "    \"Quintana Roo\": [\"Quintana Roo noticias\", \"Quintana Roo news\", \"Quintana Roo economía\"],\n",
    "    \"San Luis Potosí\": [\"San Luis Potosí noticias\", \"San Luis Potosí news\", \"San Luis Potosí economía\"],\n",
    "    \"Sinaloa\": [\"Sinaloa noticias\", \"Sinaloa news\", \"Sinaloa economía\"],\n",
    "    \"Sonora\": [\"Sonora noticias\", \"Sonora news\", \"Sonora economía\"],\n",
    "    \"Tabasco\": [\"Tabasco noticias\", \"Tabasco news\", \"Tabasco economía\"],\n",
    "    \"Tamaulipas\": [\"Tamaulipas noticias\", \"Tamaulipas news\", \"Tamaulipas economía\"],\n",
    "    \"Tlaxcala\": [\"Tlaxcala noticias\", \"Tlaxcala news\", \"Tlaxcala economía\"],\n",
    "    \"Veracruz\": [\"Veracruz noticias\", \"Veracruz news\", \"Veracruz economía\"],\n",
    "    \"Yucatán\": [\"Yucatán noticias\", \"Yucatán news\", \"Yucatán economía\"],\n",
    "    \"Zacatecas\": [\"Zacatecas noticias\", \"Zacatecas news\", \"Zacatecas economía\"]}\n",
    "\n",
    "\n",
    "# Neutral keyword-based descriptions for poverty dimensions: around 30 words per dimension \n",
    "# (60% standard spanish, 30% mexican/spanish slang and 10% english)\n",
    "POVERTY_DIMENSIONS = {\n",
    "    \"INCOME\": \"\"\"\n",
    "    empleo trabajo salario ingresos dinero economía sueldo ahorro impuestos\n",
    "    chamba lana nómina billete jale job salary income money\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO HEALTH SERVICES\": \"\"\"\n",
    "    salud médico hospital medicina tratamiento atención clínica seguro\n",
    "    sistema de salud servicios médicos doctor cuidado ir al doctor health insurance\n",
    "    seguro médico doctor particular ir a consulta healthcare medical treatment \n",
    "    \"\"\",\n",
    "    \n",
    "    \"EDUCATIONAL LAG\": \"\"\"\n",
    "    educación escuela universidad maestro estudiante aprendizaje escuela pública\n",
    "    clases formación conocimiento título bachillerato preparatoria escuela secundaria\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO SOCIAL SECURITY\": \"\"\"\n",
    "    seguridad social pensión jubilación contrato derechos laborales\n",
    "    prestaciones protección IMSS ISSSTE afore finiquito ahorro para retiro\n",
    "    cotizar retirement benefits social security worker rights informal job\n",
    "    \"\"\",\n",
    "    \n",
    "    \"HOUSING\": \"\"\"\n",
    "    vivienda casa habitación hogar alquiler renta depa housing utilities\n",
    "    servicios agua luz gas electricidad construcción propiedad rent \n",
    "    techo colonia vecindario urbanización asentamiento cuartito mortgage\n",
    "    \"\"\",\n",
    "    \n",
    "    \"ACCESS TO FOOD\": \"\"\"\n",
    "    alimentación comida nutrición alimentos dieta cocinar recetas\n",
    "    canasta básica food security nutrition meal groceries\n",
    "    comida saludable dieta balanceada comida rápida comida chatarra\n",
    "    \"\"\",\n",
    "    \n",
    "    \"SOCIAL COHESION\": \"\"\"\n",
    "    comunidad sociedad integración participación convivencia barrio raza community\n",
    "    respeto diversidad solidaridad inclusión pertenencia \n",
    "    vecinos apoyo redes sociales confianza belonging inclusion\n",
    "    \"\"\"}\n",
    "\n",
    "# limits for scraping\n",
    "MAX_VIDEOS_PER_SEARCH = 100  \n",
    "MAX_COMMENTS_PER_VIDEO = 300  \n",
    "API_SLEEP_TIME = 0.5  \n",
    "\n",
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        self.embedder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.dimension_names = list(POVERTY_DIMENSIONS.keys())\n",
    "        self.dimension_texts = []\n",
    "        for keywords in POVERTY_DIMENSIONS.values():\n",
    "            word_list = keywords.strip().split()\n",
    "            phrase = \" \".join(word_list)\n",
    "            self.dimension_texts.append(phrase)\n",
    "        self.dimension_embeddings = self.embedder.encode(self.dimension_texts, convert_to_tensor=True)\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'<.*?>', ' ', text)\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'[^\\w\\sáéíóúüñÁÉÍÓÚÜÑ]', ' ', text)\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def classify_dimension(self, text):\n",
    "        if not text:\n",
    "            return None, 0.0\n",
    "        embedding = self.embedder.encode(text, convert_to_tensor=True)\n",
    "        cosine_scores = util.cos_sim(embedding, self.dimension_embeddings)[0]\n",
    "        max_idx = torch.argmax(cosine_scores).item()\n",
    "        return self.dimension_names[max_idx], cosine_scores[max_idx].item()\n",
    "\n",
    "    def get_sentiment_score(self, text):\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        stars = torch.argmax(outputs.logits, dim=1).item() + 1\n",
    "        return (stars - 3) / 2  # Normalize to [-1, 1]\n",
    "\n",
    "class YouTubeAnalyzer:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "        self.processor = TextProcessor()\n",
    "\n",
    "    def search_videos(self, query, published_after, published_before, max_results=MAX_VIDEOS_PER_SEARCH):\n",
    "        \"\"\"Search for videos using a keyword query.\"\"\"\n",
    "        videos = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        try:\n",
    "            while len(videos) < max_results:\n",
    "                response = self.youtube.search().list(\n",
    "                    q=query,\n",
    "                    part=\"snippet\",\n",
    "                    maxResults=min(50, max_results - len(videos)),  # YouTube API allows max 50 per request\n",
    "                    pageToken=next_page_token,\n",
    "                    type=\"video\",\n",
    "                    order=\"relevance\",\n",
    "                    publishedAfter=published_after,\n",
    "                    publishedBefore=published_before,\n",
    "                    relevanceLanguage=\"es\"\n",
    "                ).execute()\n",
    "                \n",
    "                for item in response.get(\"items\", []):\n",
    "                    if item[\"id\"][\"kind\"] == \"youtube#video\":\n",
    "                        videos.append({\n",
    "                            \"id\": item[\"id\"][\"videoId\"],\n",
    "                            \"title\": item[\"snippet\"][\"title\"],\n",
    "                            \"description\": item[\"snippet\"].get(\"description\", \"\"),\n",
    "                            \"published_at\": item[\"snippet\"][\"publishedAt\"]\n",
    "                        })\n",
    "                \n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or len(videos) >= max_results:\n",
    "                    break\n",
    "                \n",
    "                sleep(API_SLEEP_TIME)  # Avoid quota exceeded errors\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching for '{query}': {e}\")\n",
    "        \n",
    "        print(f\"Found {len(videos)} videos for query '{query}'\")\n",
    "        return videos\n",
    "\n",
    "    def get_video_comments(self, video_id, max_comments=MAX_COMMENTS_PER_VIDEO):\n",
    "        \"\"\"Get comments for a specific video.\"\"\"\n",
    "        comments = []\n",
    "        next_page_token = None\n",
    "        \n",
    "        try:\n",
    "            while len(comments) < max_comments:\n",
    "                response = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=min(100, max_comments - len(comments)),  # YouTube API allows max 100 per request\n",
    "                    pageToken=next_page_token\n",
    "                ).execute()\n",
    "                \n",
    "                for item in response.get(\"items\", []):\n",
    "                    comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                    comments.append(comment_text)\n",
    "                \n",
    "                next_page_token = response.get(\"nextPageToken\")\n",
    "                if not next_page_token or len(comments) >= max_comments:\n",
    "                    break\n",
    "                \n",
    "                sleep(API_SLEEP_TIME)  # Avoid quota exceeded errors\n",
    "                \n",
    "        except Exception as e:\n",
    "            # Many videos have comments disabled, so we'll just pass silently\n",
    "            pass\n",
    "        \n",
    "        return comments\n",
    "\n",
    "    def analyze_state_by_keywords(self, state_name, search_terms, date_range):\n",
    "        \"\"\"Analyze a state by searching for videos using specified search terms.\"\"\"\n",
    "        print(f\"\\nAnalyzing {state_name}...\")\n",
    "        dimension_stats = {dim: {\"sentiment_sum\": 0.0, \"count\": 0} for dim in POVERTY_DIMENSIONS}\n",
    "        total_videos = 0\n",
    "        total_comments = 0\n",
    "        \n",
    "        # Search for videos with each search term\n",
    "        for search_term in search_terms:\n",
    "            print(f\"  Searching for '{search_term}'...\")\n",
    "            videos = self.search_videos(\n",
    "                query=search_term,\n",
    "                published_after=date_range[\"published_after\"],\n",
    "                published_before=date_range[\"published_before\"],\n",
    "                max_results=MAX_VIDEOS_PER_SEARCH\n",
    "            )\n",
    "            \n",
    "            if not videos:\n",
    "                continue\n",
    "                \n",
    "            total_videos += len(videos)\n",
    "            \n",
    "            # Process videos\n",
    "            for video in tqdm(videos, desc=f\"Processing videos for '{search_term}'\"):\n",
    "                # Get video comments\n",
    "                comments = self.get_video_comments(video[\"id\"], MAX_COMMENTS_PER_VIDEO)\n",
    "                total_comments += len(comments)\n",
    "                \n",
    "                # Concatenate title, description and comments for analysis\n",
    "                all_texts = [video[\"title\"] + \". \" + video[\"description\"]] + comments\n",
    "                \n",
    "                # Analyze each text\n",
    "                for text in all_texts:\n",
    "                    clean = self.processor.clean_text(text)\n",
    "                    if len(clean) < 10:  # Skip very short texts\n",
    "                        continue\n",
    "                        \n",
    "                    dimension, confidence = self.processor.classify_dimension(clean)\n",
    "                    if confidence > 0.1:  # Only count if confidence is high enough\n",
    "                        sentiment = self.processor.get_sentiment_score(clean)\n",
    "                        dimension_stats[dimension][\"sentiment_sum\"] += sentiment\n",
    "                        dimension_stats[dimension][\"count\"] += 1\n",
    "        \n",
    "        print(f\"  Analyzed {total_videos} videos and {total_comments} comments for {state_name}\")\n",
    "        return dimension_stats, total_videos, total_comments\n",
    "\n",
    "def analyze_all_states():\n",
    "    analyzer = YouTubeAnalyzer(YT_API_KEY)\n",
    "    date_range = {\n",
    "        \"published_after\": \"2020-01-01T00:00:00Z\",\n",
    "        \"published_before\": \"2020-12-31T23:59:59Z\"\n",
    "    }\n",
    "    \n",
    "    # Create directories for results\n",
    "    os.makedirs(\"yt_data_2020\", exist_ok=True)\n",
    "    \n",
    "    # Store overall stats for summary\n",
    "    all_results = []\n",
    "    \n",
    "    for state, search_terms in STATES_SEARCH_TERMS.items():\n",
    "        stats, total_videos, total_comments = analyzer.analyze_state_by_keywords(\n",
    "            state_name=state,\n",
    "            search_terms=search_terms,\n",
    "            date_range=date_range\n",
    "        )\n",
    "        \n",
    "        # Create dataframe for this state\n",
    "        df = pd.DataFrame([\n",
    "            {\n",
    "                \"state\": state,\n",
    "                \"dimension\": dim.replace(\"_\", \" \").title(),\n",
    "                \"avg_sentiment\": v[\"sentiment_sum\"] / v[\"count\"] if v[\"count\"] else 0,\n",
    "                \"mentions_count\": v[\"count\"],\n",
    "                \"videos_analyzed\": total_videos,\n",
    "                \"comments_analyzed\": total_comments\n",
    "            }\n",
    "            for dim, v in stats.items()\n",
    "        ])\n",
    "        \n",
    "        # Save state-specific results\n",
    "        output_file = f\"yt_data_2020/{state.replace(' ', '_').lower()}.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved results to {output_file}\")\n",
    "        \n",
    "        # Add to overall results\n",
    "        all_results.append(df)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    analyze_all_states()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
