{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo_wrapper.mongo_wrapper import MongoWrapper\n",
    "import os\n",
    "import spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from collections import Counter, defaultdict\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data from different collections in MongoDB: articles from Google News and articles from MediaCloud\n",
    "def combine(mcloud: pd.DataFrame, gnews: pd.DataFrame, state: str) -> pd.DataFrame:\n",
    "    gnews['source'] = 'gnews'\n",
    "    mcloud['source'] = 'mcloud'\n",
    "    \n",
    "    # ensure same columns exist in both dataframes\n",
    "    all_columns = set(gnews.columns) | set(mcloud.columns)\n",
    "    for col in all_columns:\n",
    "        if col not in gnews.columns:\n",
    "            gnews[col] = None\n",
    "        if col not in mcloud.columns:\n",
    "            mcloud[col] = None\n",
    "    \n",
    "    # combine the dataframes\n",
    "    combined = pd.concat([gnews, mcloud], ignore_index=True)\n",
    "    combined['year'] = combined['date'].dt.year\n",
    "    \n",
    "    # keep only articles from 2020 and 2022\n",
    "    target_years = [2020, 2022]\n",
    "    combined_filtered = combined[combined['year'].isin(target_years)]\n",
    "    \n",
    "    # check how many articles are in each year-state combination\n",
    "    print(f\"\\\\n{state} - summary for 2020 and 2022:\")\n",
    "    \n",
    "    for year in target_years:\n",
    "        year_data = combined_filtered[combined_filtered['year'] == year]\n",
    "        \n",
    "        if not year_data.empty:\n",
    "            mcloud_count = len(year_data[year_data['source'] == 'mcloud'])\n",
    "            gnews_count = len(year_data[year_data['source'] == 'gnews'])\n",
    "            total_count = len(year_data)\n",
    "            \n",
    "            print(f\"  {year}:\")\n",
    "            print(f\"    MediaCloud: {mcloud_count} articles\")\n",
    "            print(f\"    Google News: {gnews_count} articles\")\n",
    "            print(f\"    Total: {total_count} articles\")\n",
    "        else:\n",
    "            print(f\"  {year}: 0 articles\")\n",
    "    \n",
    "    return combined_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state_data(state: str, mongo_client: MongoWrapper) -> pd.DataFrame:\n",
    "    print(f\"\\\\nProcessing {state}...\")\n",
    "    \n",
    "    # get data from MongoDB collections\n",
    "    # data from Google News\n",
    "    try:\n",
    "        gnews_data = mongo_client.get_collection_entries(f'gnews_{state}')\n",
    "        gnews_df = pd.DataFrame(gnews_data)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error getting Google News data for {state}: {e}\")\n",
    "        gnews_df = pd.DataFrame()\n",
    "    \n",
    "    # data from MediaCloud\n",
    "    try:\n",
    "        mcloud_data = mongo_client.get_collection_entries(f'mediacloud_{state}')\n",
    "        mcloud_df = pd.DataFrame(mcloud_data)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error getting MediaCloud data for {state}: {e}\")\n",
    "        mcloud_df = pd.DataFrame()\n",
    "    \n",
    "    # handle the date columns \n",
    "    # handle Google News dates\n",
    "    try:\n",
    "        gnews_df[\"date\"] = pd.to_datetime(gnews_df[\"published date\"], format='%a, %d %b %Y %H:%M:%S GMT')\n",
    "        gnews_df = gnews_df.drop(columns=['published date'])\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing Google News dates for {state}: {e}\")\n",
    "        gnews_df = pd.DataFrame()\n",
    "    \n",
    "    # handle MediaCloud dates\n",
    "    try:\n",
    "        mcloud_df['date'] = pd.to_datetime(mcloud_df['publish_date'])\n",
    "        mcloud_df['date_str'] = mcloud_df['date'].dt.strftime('%a, %d %b %Y 08:00:00 GMT')\n",
    "        mcloud_df['date'] = pd.to_datetime(mcloud_df['date_str'], format='%a, %d %b %Y %H:%M:%S GMT')\n",
    "        mcloud_df = mcloud_df.drop(columns=['publish_date', 'date_str'])\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing MediaCloud dates for {state}: {e}\")\n",
    "        mcloud_df = pd.DataFrame()\n",
    "    \n",
    "    # combine the dataframes\n",
    "    combined = combine(mcloud_df, gnews_df, state)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the contents \n",
    "lemm = spacy.load('es_core_news_md')\n",
    "\n",
    "# define custom stopwords to add to default ones \n",
    "custom_stopwords = {\n",
    "    'él', 'ella', 'ellos', 'ellas', 'le', 'la', 'lo', 'les', 'las', 'los', 'sino', 'querétaro', 'municipio',\n",
    "    'año', 'mes', 'día', 'tiempo', 'momento', 'semana', '2020', '2021', '2022', '2023', '2024', '2025', 'méxico',\n",
    "    'mil', 'ciento', 'millón', 'dólares', 'tener', 'hacer', 'ver', 'ir', 'llegar', 'mexicano', 'secretaría',\n",
    "    'salir', 'seguir', 'entre', 'sobre', 'tras', 'mediante', 'durante', 'aunque', 'ciudad', 'lugar', 'grupo', \n",
    "    'nacional', 'persona', 'caso', 'entidad', 'the', 'país', 'público', 'general', 'lópez', 'dejar', 'decidir',\n",
    "    'gobierno', 'presidente', 'chihuahua', 'california', 'león', 'durango', 'coahuila', 'puebla', 'pasar', 'región'\n",
    "    'luis', 'san', 'roo', 'veracruz', 'aguascalientes', 'baja', 'sur', 'potosí', 'sinaloa', 'municipal', 'norte',\n",
    "    'campeche', 'chiapas', 'colima', 'estado', 'guanajuato', 'ciudad', 'estado', 'quintana', 'contar', 'querer',\n",
    "    'guerrero', 'hidalgo', 'jalisco', 'michoacán', 'morelos', 'nayarit', 'nuevo', 'zona', 'centro', 'deber', 'recibir',\n",
    "    'sonora', 'tabasco', 'tamaulipa', 'tlaxcala', 'yucatán', 'zacateca', 'oaxaca', 'encontrar', 'ofrecer', 'zacatecas'}\n",
    "\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(custom_stopwords)\n",
    "\n",
    "# function to preprocess \n",
    "def preprocess(text):\n",
    "    text = text.lower() # lowercase \n",
    "    doc = lemm(text)\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.lemma_ not in ALL_STOP_WORDS # remove stopwords (default and custom)\n",
    "        and not token.is_punct # remove punctuation\n",
    "        and not token.is_digit # remove numbers \n",
    "        and not any(char.isdigit() for char in token.text) # remove tokens with digits\n",
    "        and len(token.lemma_.strip()) > 2 # remove too short tokens\n",
    "        and token.lemma_.strip() != ''] # remove empty tokens\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def clean_and_preprocess_data(state_dfs: dict) -> dict:\n",
    "    state_dfs_cleaned = {}\n",
    "    \n",
    "    for state, df in state_dfs.items():\n",
    "        try:\n",
    "            df_cleaned = df.copy()\n",
    "            # Preprocess text\n",
    "            df_cleaned['text_cleaned'] = df_cleaned['content'].astype(str).apply(preprocess)\n",
    "            # Count words\n",
    "            df_cleaned['word_count'] = df_cleaned['text_cleaned'].apply(lambda x: len(x.split()))\n",
    "            # Remove short articles\n",
    "            df_cleaned = df_cleaned[df_cleaned['word_count'] >= 10]\n",
    "            \n",
    "            state_dfs_cleaned[state] = df_cleaned\n",
    "            print(f\"Cleaned {state}: {len(df_cleaned)} articles remaining\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning {state}: {e}\")\n",
    "    \n",
    "    return state_dfs_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_data(state_dfs_cleaned: dict, target_years: list = [2020, 2022]) -> dict:\n",
    "    combined_state_data = {}\n",
    "    \n",
    "    # extract unique states from keys like \"State_2020\", \"State_2022\"\n",
    "    states = set([key.rsplit('_', 1)[0] for key in state_dfs_cleaned.keys()])\n",
    "    \n",
    "    for state in states:\n",
    "        combined_texts = []\n",
    "        year_labels = []\n",
    "        \n",
    "        for year in target_years:\n",
    "            key = f\"{state}_{year}\"\n",
    "            if key in state_dfs_cleaned:\n",
    "                df = state_dfs_cleaned[key]\n",
    "                texts = df['text_cleaned'].tolist()\n",
    "                combined_texts.extend(texts)\n",
    "                year_labels.extend([year] * len(texts))\n",
    "        \n",
    "        if combined_texts:\n",
    "            combined_state_data[state] = {\n",
    "                'texts': combined_texts, \n",
    "                'years': year_labels}\n",
    "    \n",
    "    return combined_state_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run LDA on the combined data (2020 and 2022 together - so one model per state)\n",
    "def run_lda(combined_state_data: dict) -> dict:\n",
    "    lda_results = {}\n",
    "    \n",
    "    for state, data in combined_state_data.items():\n",
    "        texts = data['texts']\n",
    "        years = data['years']\n",
    "            \n",
    "        # CountVectorize texts \n",
    "        cv = CountVectorizer(ngram_range=(1, 3), lowercase=False, min_df=0.001, max_df=0.6, \n",
    "                             stop_words=list(ALL_STOP_WORDS), token_pattern=r'\\b[a-záéíóúñü]{3,}\\b')\n",
    "            \n",
    "        vectorized_text = cv.fit_transform(texts)\n",
    "        dtm_sparse = csr_matrix(vectorized_text)\n",
    "        corpus = Sparse2Corpus(dtm_sparse, documents_columns=False)\n",
    "            \n",
    "        # gensim dictionary   \n",
    "        vocabulary_gensim = {val: key for key, val in cv.vocabulary_.items()}\n",
    "        dictionary = corpora.Dictionary()\n",
    "        dictionary.id2token = vocabulary_gensim\n",
    "        dictionary.token2id = cv.vocabulary_\n",
    "            \n",
    "        # define LDA model\n",
    "        lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=8, \n",
    "                                    passes=20, random_state=42, eta=0.001, alpha=0.001)\n",
    "            \n",
    "        # save the model \n",
    "        lda_results[state] = { \"model\": lda_model, \"corpus\": corpus, \"dictionary\": dictionary, \"years\": years, \"texts\": texts}\n",
    "        \n",
    "        print(f\"LDA completed for {state}\")\n",
    "        \n",
    "    return lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topic_shares(lda_results: dict, target_years: list = [2020, 2022]) -> dict:\n",
    "    year_topic_shares = {}\n",
    "    \n",
    "    for year in target_years:\n",
    "        year_topic_shares[year] = {}\n",
    "        \n",
    "        for state, data in lda_results.items():\n",
    "            model = data[\"model\"]\n",
    "            corpus = data[\"corpus\"]\n",
    "            years = data[\"years\"]\n",
    "            \n",
    "            # filter documents for the specific year\n",
    "            year_indices = [i for i, y in enumerate(years) if y == year]\n",
    "            \n",
    "            if year_indices:\n",
    "                topic_dist = np.zeros(model.num_topics)\n",
    "                doc_count = 0\n",
    "                \n",
    "                for idx in year_indices:\n",
    "                    doc = corpus[idx]\n",
    "                    doc_topics = model[doc]\n",
    "                    doc_count += 1\n",
    "                    \n",
    "                    for topic_id, prob in doc_topics:\n",
    "                        topic_dist[topic_id] += prob\n",
    "                \n",
    "                if doc_count > 0:\n",
    "                    topic_dist = topic_dist / doc_count\n",
    "                \n",
    "                year_topic_shares[year][state] = topic_dist\n",
    "                print(f\"Topic shares calculated for {state} - {year}\")\n",
    "    \n",
    "    return year_topic_shares\n",
    "\n",
    "def save_results(lda_results: dict, year_topic_shares: dict, target_years: list = [2020, 2022]):\n",
    "    # save distributiona of topics - one per year\n",
    "    for year in target_years:\n",
    "        if year in year_topic_shares and year_topic_shares[year]:\n",
    "            df_shares = pd.DataFrame.from_dict(year_topic_shares[year], orient='index')\n",
    "            df_shares.columns = [f'Topic_{i}' for i in range(df_shares.shape[1])]\n",
    "            df_shares.to_csv(f'shares_{year}_improved2.csv')\n",
    "    \n",
    "    # save keywords for each state - one csv only since is one LDA model per state but per both years\n",
    "    all_keywords = {}\n",
    "    \n",
    "    for state, data in lda_results.items():\n",
    "        model = data[\"model\"]\n",
    "        state_topics = {}\n",
    "\n",
    "        for topic_id in range(model.num_topics):\n",
    "            top_words = model.show_topic(topic_id, topn=50)\n",
    "            words = [word for word, _ in top_words]\n",
    "            state_topics[f'Topic_{topic_id}'] = ', '.join(words)\n",
    "        \n",
    "        all_keywords[state] = state_topics\n",
    "\n",
    "    if all_keywords:\n",
    "        df_keywords = pd.DataFrame.from_dict(all_keywords, orient='index')\n",
    "        df_keywords.to_csv('keywords_all_states_improved.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    # connect to MongoDB\n",
    "    mongo_client = MongoWrapper(\n",
    "        db=\"news_outlets\",\n",
    "        user=os.getenv(\"MONGO_USERNAME\"),\n",
    "        password=os.getenv(\"MONGO_PASSWORD\"),\n",
    "        ip=os.getenv(\"MONGO_IP\"),\n",
    "        port=os.getenv(\"MONGO_PORT\"))\n",
    "    \n",
    "    # Mexican states\n",
    "    states = [\n",
    "        \"Aguascalientes\", \"Baja California\", \"Baja California Sur\", \"Campeche\", \"Chiapas\", \"Chihuahua\",\n",
    "        \"Coahuila\", \"Colima\", \"Durango\", \"Guanajuato\", \"Guerrero\", \"Hidalgo\", \"Jalisco\", \"Mexico\",\n",
    "        \"Michoacan\", \"Morelos\", \"Nayarit\", \"Nuevo Leon\", \"Oaxaca\", \"Puebla\", \"Queretaro\", \"Quintana Roo\",\n",
    "        \"San Luis Potosi\", \"Sinaloa\", \"Sonora\", \"Tabasco\", \"Tamaulipas\", \"Tlaxcala\", \"Veracruz\",\n",
    "        \"Yucatan\", \"Zacatecas\", \"Ciudad de México\"]\n",
    "\n",
    "    \n",
    "    # years to analyze\n",
    "    target_years = [2020, 2022]\n",
    "\n",
    "    # retrieve data for each state\n",
    "    state_raw_dfs = {}\n",
    "    \n",
    "    for state in states:\n",
    "        combined_df = process_state_data(state, mongo_client)\n",
    "        \n",
    "        if not combined_df.empty:\n",
    "            # split by years\n",
    "            for year in target_years:\n",
    "                year_data = combined_df[combined_df['year'] == year].copy()\n",
    "                if not year_data.empty:\n",
    "                    state_year_key = f\"{state}_{year}\"\n",
    "                    state_raw_dfs[state_year_key] = year_data\n",
    "    \n",
    "    print(f\"\\\\nTotal state-year combinations: {len(state_raw_dfs)}\")\n",
    "    \n",
    "    # preprocessing\n",
    "    state_dfs_cleaned = clean_and_preprocess_data(state_raw_dfs)\n",
    "    \n",
    "    # combine data for LDA\n",
    "    combined_state_data = combined_data(state_dfs_cleaned, target_years)\n",
    "    \n",
    "    # run LDA\n",
    "    lda_results = run_lda(combined_state_data)\n",
    "    \n",
    "    # calculate topic shares and save results \n",
    "    year_topic_shares = calculate_topic_shares(lda_results, target_years)\n",
    "    save_results(lda_results, year_topic_shares, target_years)\n",
    "    \n",
    "    return lda_results, year_topic_shares\n",
    "\n",
    "lda_results, year_topic_shares = pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('news_data/keywords_all_states_improved.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cols_2020 = [col for col in df.columns if col.startswith(\"Topic_\")]\n",
    "for i in range(0, len(topic_cols_2020), 2):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    for j in range(2):\n",
    "        if i + j < len(topic_cols_2020):\n",
    "            col = topic_cols_2020[i + j]\n",
    "            text = ' '.join(df[col].dropna().tolist())\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n",
    "            axes[j].imshow(wordcloud, interpolation='bilinear')\n",
    "            axes[j].axis('off')\n",
    "            axes[j].set_title(col, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
