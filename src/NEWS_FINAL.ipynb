{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Outlets LDA Analysis \n",
    "\n",
    "This implementation processes news articles from multiple sources to extract latent topics across Mexican states. The analysis uses Latent Dirichlet Allocation (LDA) to identify thematic patterns in news coverage that can serve as indicators of socioeconomic conditions.\n",
    "\n",
    "## Data Sources and Overview\n",
    "\n",
    "- **Google News**: Real-time news aggregation with broad media coverage\n",
    "- **MediaCloud**: Academic news database with extensive historical archives\n",
    "**Geographic Scope**: All 32 Mexican states with state-specific news collections\n",
    "**Temporal Coverage**: 2020 and 2022 comparative analysis periods\n",
    "\n",
    "### Data Retrieval \n",
    "\n",
    "**Collection Strategy:**\n",
    "```python\n",
    "def process_state_data(state: str, mongo_client: MongoWrapper):\n",
    "    gnews_data = mongo_client.get_collection_entries(f'gnews_{state}')\n",
    "    mcloud_data = mongo_client.get_collection_entries(f'mediacloud_{state}')\n",
    "```\n",
    "\n",
    "## Text Preprocessing Pipeline\n",
    "\n",
    "### Linguistic Processing Framework\n",
    "\n",
    "**Language Model**: `es_core_news_md` (Spanish language model)\n",
    "\n",
    "### Stopword Removal\n",
    "\n",
    "We customized our stopwords, as we realized that just using spaCy's built-in STOP_WORDS were not enough, since we ended up with word clouds full of meaningless words that did not lead to any topic identification. \n",
    "```python\n",
    "custom_stopwords = {\n",
    "    'él', 'ella', 'ellos', 'ellas',          # Personal pronouns\n",
    "    'año', 'mes', 'día', 'tiempo',           # Temporal expressions\n",
    "    '2020', '2021', '2022', '2023',          # Year exclusions\n",
    "    'chihuahua', 'california', 'león',       # State name removal\n",
    "    'gobierno', 'presidente', 'secretaría'   # Institutional terms\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "### Text Cleaning \n",
    "\n",
    "1. **Case normalization**: Convert to lowercase for consistency\n",
    "2. **Lemmatization**: via spaCy, to obtain the root of the word \n",
    "3. **Token filtering**: Remove punctuation, digits, short tokens\n",
    "4. **Length validation**: Exclude tokens shorter than 3 characters\n",
    "5. **Content filtering**: Remove articles with fewer than 10 words\n",
    "\n",
    "## LDA Model Architecture\n",
    "\n",
    "### Topic Modeling Framework\n",
    "\n",
    "```python\n",
    "lda_model = models.LdaModel(\n",
    "    corpus=corpus, \n",
    "    id2word=dictionary, \n",
    "    num_topics=8,       \n",
    "    passes=20,              \n",
    "    random_state=42,  \n",
    "    eta=0.001,           \n",
    "    alpha=0.001        \n",
    ")\n",
    "```\n",
    "\n",
    "### Hyperparameter Justification\n",
    "\n",
    "**Number of Topics (8):**\n",
    "After many trials, we found 8 to be the optimal number of topics to use, as it leads to identifiable and coherent topics. Indeed, while still experimenting we used more topics, and this led to great overleap, which is counterproductive as basically all topics look the same. \n",
    "\n",
    "**Passes (20):**\n",
    "This parameter indicates how many times the model passes through the whole corpus, and we believe this value is adapt and ensures convergence (i.e., stable topic distributions) while keeping computational costs reasonable. \n",
    "\n",
    "**Alpha (0.001) - Document-Topic Distribution:**\n",
    "We set this parameter at such a low value to reduce overlap between topics, as this value forces the model to identify fewer topics in each document. In this way, there is a clearer thematic assignments per document that should contribute to reduce overlap. \n",
    "\n",
    "**Eta (0.001) - Topic-Word Distribution:**\n",
    "Similarly as above, we set a low value for this parameter to control for overlap between topics. Indeed, this parameter forces the model to define topics based on fewer, more distinctive words. \n",
    "\n",
    "### Vectorization Strategy\n",
    "\n",
    "**CountVectorizer Configuration:**\n",
    "```python\n",
    "CountVectorizer(\n",
    "    ngram_range=(1, 3),             \n",
    "    min_df=0.001,                   \n",
    "    max_df=0.6,                            \n",
    "    token_pattern=r'\\b[a-záéíóúñü]{3,}\\b'  \n",
    ")\n",
    "```\n",
    "\n",
    "**N-gram Selection (1,3):**\n",
    "\n",
    "We selected individual words, bigrams and trigrams to captures more complex and potentially informative expressions. \n",
    "\n",
    "**Frequency Thresholds:**\n",
    "- **min_df=0.001**: Exclude extremely rare terms (noise reduction)\n",
    "- **max_df=0.6**: Remove overly common terms (stop-word like behavior)\n",
    "\n",
    "## Topic Share Calculation and Analysis\n",
    "\n",
    "To train our LDA, we combined 2020 and 2022 to have the exact same topics across both years and to have a coherent comparison of topic shares. \n",
    "\n",
    "### Output\n",
    "\n",
    "**Topic Share:**\n",
    "- **Topics probability distributions** per State \n",
    "- Separate CSV files for 2020 and 2022\n",
    "\n",
    "**Keywords:**\n",
    "- **Top-50 words per topic**\n",
    "- **State-specific keywords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "from mongo_wrapper.mongo_wrapper import MongoWrapper\n",
    "import os\n",
    "import spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim import corpora, models\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "from collections import Counter, defaultdict\n",
    "import networkx as nx\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data from different collections in MongoDB: articles from Google News and articles from MediaCloud\n",
    "def combine(mcloud: pd.DataFrame, gnews: pd.DataFrame, state: str) -> pd.DataFrame:\n",
    "    gnews['source'] = 'gnews'\n",
    "    mcloud['source'] = 'mcloud'\n",
    "    \n",
    "    # ensure same columns exist in both dataframes\n",
    "    all_columns = set(gnews.columns) | set(mcloud.columns)\n",
    "    for col in all_columns:\n",
    "        if col not in gnews.columns:\n",
    "            gnews[col] = None\n",
    "        if col not in mcloud.columns:\n",
    "            mcloud[col] = None\n",
    "    \n",
    "    # combine the dataframes\n",
    "    combined = pd.concat([gnews, mcloud], ignore_index=True)\n",
    "    combined['year'] = combined['date'].dt.year\n",
    "    \n",
    "    # keep only articles from 2020 and 2022\n",
    "    target_years = [2020, 2022]\n",
    "    combined_filtered = combined[combined['year'].isin(target_years)]\n",
    "    \n",
    "    # check how many articles are in each year-state combination\n",
    "    print(f\"\\\\n{state} - summary for 2020 and 2022:\")\n",
    "    \n",
    "    for year in target_years:\n",
    "        year_data = combined_filtered[combined_filtered['year'] == year]\n",
    "        \n",
    "        if not year_data.empty:\n",
    "            mcloud_count = len(year_data[year_data['source'] == 'mcloud'])\n",
    "            gnews_count = len(year_data[year_data['source'] == 'gnews'])\n",
    "            total_count = len(year_data)\n",
    "            \n",
    "            print(f\"  {year}:\")\n",
    "            print(f\"    MediaCloud: {mcloud_count} articles\")\n",
    "            print(f\"    Google News: {gnews_count} articles\")\n",
    "            print(f\"    Total: {total_count} articles\")\n",
    "        else:\n",
    "            print(f\"  {year}: 0 articles\")\n",
    "    \n",
    "    return combined_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_state_data(state: str, mongo_client: MongoWrapper) -> pd.DataFrame:\n",
    "    print(f\"\\\\nProcessing {state}...\")\n",
    "    \n",
    "    # get data from MongoDB collections\n",
    "    # data from Google News\n",
    "    try:\n",
    "        gnews_data = mongo_client.get_collection_entries(f'gnews_{state}')\n",
    "        gnews_df = pd.DataFrame(gnews_data)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error getting Google News data for {state}: {e}\")\n",
    "        gnews_df = pd.DataFrame()\n",
    "    \n",
    "    # data from MediaCloud\n",
    "    try:\n",
    "        mcloud_data = mongo_client.get_collection_entries(f'mediacloud_{state}')\n",
    "        mcloud_df = pd.DataFrame(mcloud_data)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error getting MediaCloud data for {state}: {e}\")\n",
    "        mcloud_df = pd.DataFrame()\n",
    "    \n",
    "    # handle the date columns \n",
    "    # handle Google News dates\n",
    "    try:\n",
    "        gnews_df[\"date\"] = pd.to_datetime(gnews_df[\"published date\"], format='%a, %d %b %Y %H:%M:%S GMT')\n",
    "        gnews_df = gnews_df.drop(columns=['published date'])\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing Google News dates for {state}: {e}\")\n",
    "        gnews_df = pd.DataFrame()\n",
    "    \n",
    "    # handle MediaCloud dates\n",
    "    try:\n",
    "        mcloud_df['date'] = pd.to_datetime(mcloud_df['publish_date'])\n",
    "        mcloud_df['date_str'] = mcloud_df['date'].dt.strftime('%a, %d %b %Y 08:00:00 GMT')\n",
    "        mcloud_df['date'] = pd.to_datetime(mcloud_df['date_str'], format='%a, %d %b %Y %H:%M:%S GMT')\n",
    "        mcloud_df = mcloud_df.drop(columns=['publish_date', 'date_str'])\n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing MediaCloud dates for {state}: {e}\")\n",
    "        mcloud_df = pd.DataFrame()\n",
    "    \n",
    "    # combine the dataframes\n",
    "    combined = combine(mcloud_df, gnews_df, state)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the contents \n",
    "lemm = spacy.load('es_core_news_md')\n",
    "\n",
    "# define custom stopwords to add to default ones \n",
    "custom_stopwords = {\n",
    "    'él', 'ella', 'ellos', 'ellas', 'le', 'la', 'lo', 'les', 'las', 'los', 'sino', 'querétaro', 'municipio',\n",
    "    'año', 'mes', 'día', 'tiempo', 'momento', 'semana', '2020', '2021', '2022', '2023', '2024', '2025', 'méxico',\n",
    "    'mil', 'ciento', 'millón', 'dólares', 'tener', 'hacer', 'ver', 'ir', 'llegar', 'mexicano', 'secretaría',\n",
    "    'salir', 'seguir', 'entre', 'sobre', 'tras', 'mediante', 'durante', 'aunque', 'ciudad', 'lugar', 'grupo', \n",
    "    'nacional', 'persona', 'caso', 'entidad', 'the', 'país', 'público', 'general', 'lópez', 'dejar', 'decidir',\n",
    "    'gobierno', 'presidente', 'chihuahua', 'california', 'león', 'durango', 'coahuila', 'puebla', 'pasar', 'región'\n",
    "    'luis', 'san', 'roo', 'veracruz', 'aguascalientes', 'baja', 'sur', 'potosí', 'sinaloa', 'municipal', 'norte',\n",
    "    'campeche', 'chiapas', 'colima', 'estado', 'guanajuato', 'ciudad', 'estado', 'quintana', 'contar', 'querer',\n",
    "    'guerrero', 'hidalgo', 'jalisco', 'michoacán', 'morelos', 'nayarit', 'nuevo', 'zona', 'centro', 'deber', 'recibir',\n",
    "    'sonora', 'tabasco', 'tamaulipa', 'tlaxcala', 'yucatán', 'zacateca', 'oaxaca', 'encontrar', 'ofrecer', 'zacatecas'}\n",
    "\n",
    "ALL_STOP_WORDS = STOP_WORDS.union(custom_stopwords)\n",
    "\n",
    "# function to preprocess \n",
    "def preprocess(text):\n",
    "    text = text.lower() # lowercase \n",
    "    doc = lemm(text)\n",
    "    tokens = [\n",
    "        token.lemma_ for token in doc \n",
    "        if token.lemma_ not in ALL_STOP_WORDS # remove stopwords (default and custom)\n",
    "        and not token.is_punct # remove punctuation\n",
    "        and not token.is_digit # remove numbers \n",
    "        and not any(char.isdigit() for char in token.text) # remove tokens with digits\n",
    "        and len(token.lemma_.strip()) > 2 # remove too short tokens\n",
    "        and token.lemma_.strip() != ''] # remove empty tokens\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def clean_and_preprocess_data(state_dfs: dict) -> dict:\n",
    "    state_dfs_cleaned = {}\n",
    "    \n",
    "    for state, df in state_dfs.items():\n",
    "        try:\n",
    "            df_cleaned = df.copy()\n",
    "            # Preprocess text\n",
    "            df_cleaned['text_cleaned'] = df_cleaned['content'].astype(str).apply(preprocess)\n",
    "            # Count words\n",
    "            df_cleaned['word_count'] = df_cleaned['text_cleaned'].apply(lambda x: len(x.split()))\n",
    "            # Remove short articles\n",
    "            df_cleaned = df_cleaned[df_cleaned['word_count'] >= 10]\n",
    "            \n",
    "            state_dfs_cleaned[state] = df_cleaned\n",
    "            print(f\"Cleaned {state}: {len(df_cleaned)} articles remaining\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error cleaning {state}: {e}\")\n",
    "    \n",
    "    return state_dfs_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_data(state_dfs_cleaned: dict, target_years: list = [2020, 2022]) -> dict:\n",
    "    combined_state_data = {}\n",
    "    \n",
    "    # extract unique states from keys like \"State_2020\", \"State_2022\"\n",
    "    states = set([key.rsplit('_', 1)[0] for key in state_dfs_cleaned.keys()])\n",
    "    \n",
    "    for state in states:\n",
    "        combined_texts = []\n",
    "        year_labels = []\n",
    "        \n",
    "        for year in target_years:\n",
    "            key = f\"{state}_{year}\"\n",
    "            if key in state_dfs_cleaned:\n",
    "                df = state_dfs_cleaned[key]\n",
    "                texts = df['text_cleaned'].tolist()\n",
    "                combined_texts.extend(texts)\n",
    "                year_labels.extend([year] * len(texts))\n",
    "        \n",
    "        if combined_texts:\n",
    "            combined_state_data[state] = {\n",
    "                'texts': combined_texts, \n",
    "                'years': year_labels}\n",
    "    \n",
    "    return combined_state_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run LDA on the combined data (2020 and 2022 together - so one model per state)\n",
    "def run_lda(combined_state_data: dict) -> dict:\n",
    "    lda_results = {}\n",
    "    \n",
    "    for state, data in combined_state_data.items():\n",
    "        texts = data['texts']\n",
    "        years = data['years']\n",
    "            \n",
    "        # CountVectorize texts \n",
    "        cv = CountVectorizer(ngram_range=(1, 3), lowercase=False, min_df=0.001, max_df=0.6, \n",
    "                             stop_words=list(ALL_STOP_WORDS), token_pattern=r'\\b[a-záéíóúñü]{3,}\\b')\n",
    "            \n",
    "        vectorized_text = cv.fit_transform(texts)\n",
    "        dtm_sparse = csr_matrix(vectorized_text)\n",
    "        corpus = Sparse2Corpus(dtm_sparse, documents_columns=False)\n",
    "            \n",
    "        # gensim dictionary   \n",
    "        vocabulary_gensim = {val: key for key, val in cv.vocabulary_.items()}\n",
    "        dictionary = corpora.Dictionary()\n",
    "        dictionary.id2token = vocabulary_gensim\n",
    "        dictionary.token2id = cv.vocabulary_\n",
    "            \n",
    "        # define LDA model\n",
    "        lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=8, \n",
    "                                    passes=20, random_state=42, eta=0.001, alpha=0.001)\n",
    "            \n",
    "        # save the model \n",
    "        lda_results[state] = { \"model\": lda_model, \"corpus\": corpus, \"dictionary\": dictionary, \"years\": years, \"texts\": texts}\n",
    "        \n",
    "        print(f\"LDA completed for {state}\")\n",
    "        \n",
    "    return lda_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_topic_shares(lda_results: dict, target_years: list = [2020, 2022]) -> dict:\n",
    "    year_topic_shares = {}\n",
    "    \n",
    "    for year in target_years:\n",
    "        year_topic_shares[year] = {}\n",
    "        \n",
    "        for state, data in lda_results.items():\n",
    "            model = data[\"model\"]\n",
    "            corpus = data[\"corpus\"]\n",
    "            years = data[\"years\"]\n",
    "            \n",
    "            # filter documents for the specific year\n",
    "            year_indices = [i for i, y in enumerate(years) if y == year]\n",
    "            \n",
    "            if year_indices:\n",
    "                topic_dist = np.zeros(model.num_topics)\n",
    "                doc_count = 0\n",
    "                \n",
    "                for idx in year_indices:\n",
    "                    doc = corpus[idx]\n",
    "                    doc_topics = model[doc]\n",
    "                    doc_count += 1\n",
    "                    \n",
    "                    for topic_id, prob in doc_topics:\n",
    "                        topic_dist[topic_id] += prob\n",
    "                \n",
    "                if doc_count > 0:\n",
    "                    topic_dist = topic_dist / doc_count\n",
    "                \n",
    "                year_topic_shares[year][state] = topic_dist\n",
    "                print(f\"Topic shares calculated for {state} - {year}\")\n",
    "    \n",
    "    return year_topic_shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(lda_results: dict, year_topic_shares: dict, target_years: list = [2020, 2022]):\n",
    "    # save distributiona of topics - one per year\n",
    "    for year in target_years:\n",
    "        if year in year_topic_shares and year_topic_shares[year]:\n",
    "            df_shares = pd.DataFrame.from_dict(year_topic_shares[year], orient='index')\n",
    "            df_shares.columns = [f'Topic_{i}' for i in range(df_shares.shape[1])]\n",
    "            df_shares.to_csv(f'shares_{year}.csv')\n",
    "    \n",
    "    # save keywords for each state - one csv only since is one LDA model per state but per both years\n",
    "    all_keywords = {}\n",
    "    \n",
    "    for state, data in lda_results.items():\n",
    "        model = data[\"model\"]\n",
    "        state_topics = {}\n",
    "\n",
    "        for topic_id in range(model.num_topics):\n",
    "            top_words = model.show_topic(topic_id, topn=50)\n",
    "            words = [word for word, _ in top_words]\n",
    "            state_topics[f'Topic_{topic_id}'] = ', '.join(words)\n",
    "        \n",
    "        all_keywords[state] = state_topics\n",
    "\n",
    "    if all_keywords:\n",
    "        df_keywords = pd.DataFrame.from_dict(all_keywords, orient='index')\n",
    "        df_keywords.to_csv('keywords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    # connect to MongoDB\n",
    "    mongo_client = MongoWrapper(\n",
    "        db=\"news_outlets\",\n",
    "        user=os.getenv(\"MONGO_USERNAME\"),\n",
    "        password=os.getenv(\"MONGO_PASSWORD\"),\n",
    "        ip=os.getenv(\"MONGO_IP\"),\n",
    "        port=os.getenv(\"MONGO_PORT\"))\n",
    "    \n",
    "    # Mexican states\n",
    "    states = [\n",
    "        \"Aguascalientes\", \"Baja California\", \"Baja California Sur\", \"Campeche\", \"Chiapas\", \"Chihuahua\",\n",
    "        \"Coahuila\", \"Colima\", \"Durango\", \"Guanajuato\", \"Guerrero\", \"Hidalgo\", \"Jalisco\", \"Mexico\",\n",
    "        \"Michoacan\", \"Morelos\", \"Nayarit\", \"Nuevo Leon\", \"Oaxaca\", \"Puebla\", \"Queretaro\", \"Quintana Roo\",\n",
    "        \"San Luis Potosi\", \"Sinaloa\", \"Sonora\", \"Tabasco\", \"Tamaulipas\", \"Tlaxcala\", \"Veracruz\",\n",
    "        \"Yucatan\", \"Zacatecas\", \"Ciudad de México\"]\n",
    "\n",
    "    \n",
    "    # years to analyze\n",
    "    target_years = [2020, 2022]\n",
    "\n",
    "    # retrieve data for each state\n",
    "    state_raw_dfs = {}\n",
    "    \n",
    "    for state in states:\n",
    "        combined_df = process_state_data(state, mongo_client)\n",
    "        \n",
    "        if not combined_df.empty:\n",
    "            # split by years\n",
    "            for year in target_years:\n",
    "                year_data = combined_df[combined_df['year'] == year].copy()\n",
    "                if not year_data.empty:\n",
    "                    state_year_key = f\"{state}_{year}\"\n",
    "                    state_raw_dfs[state_year_key] = year_data\n",
    "    \n",
    "    print(f\"\\\\nTotal state-year combinations: {len(state_raw_dfs)}\")\n",
    "    \n",
    "    # preprocessing\n",
    "    state_dfs_cleaned = clean_and_preprocess_data(state_raw_dfs)\n",
    "    \n",
    "    # combine data for LDA\n",
    "    combined_state_data = combined_data(state_dfs_cleaned, target_years)\n",
    "    \n",
    "    # run LDA\n",
    "    lda_results = run_lda(combined_state_data)\n",
    "    \n",
    "    # calculate topic shares and save results \n",
    "    year_topic_shares = calculate_topic_shares(lda_results, target_years)\n",
    "    save_results(lda_results, year_topic_shares, target_years)\n",
    "    \n",
    "    return lda_results, year_topic_shares\n",
    "\n",
    "lda_results, year_topic_shares = pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('news_data/keywords_all_states_improved.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_cols_2020 = [col for col in df.columns if col.startswith(\"Topic_\")]\n",
    "for i in range(0, len(topic_cols_2020), 2):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    for j in range(2):\n",
    "        if i + j < len(topic_cols_2020):\n",
    "            col = topic_cols_2020[i + j]\n",
    "            text = ' '.join(df[col].dropna().tolist())\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(text)\n",
    "            axes[j].imshow(wordcloud, interpolation='bilinear')\n",
    "            axes[j].axis('off')\n",
    "            axes[j].set_title(col, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
